[
  {
    "model": "Gemini 3 Pro",
    "provider": "Google",
    "type": "multimodal",
    "sections": [
      {
        "sectionId": "model-details",
        "sectionSnippets": [
          {
            "snippetId": "model-details-snippet-1",
            "url": "https://ai.google.dev/gemini-api/docs/models",
            "snippet": "OUR MOST INTELLIGENT MODEL\n\n## Gemini 3 Pro\n\nThe best model in the world for multimodal understanding, and our most powerful agentic and vibe-coding model yet, delivering richer visuals and deeper interactivity, all built on a foundation of state-of-the-art reasoning.\n\n### Expand to learn more\n\n#### Model details... ### Gemini 3 Pro Preview\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-3-pro-preview`|\n|Supported data types|Text, Image, Video, Audio, and PDF Text|\n|Token limits [*]|1,048,576 65,536|\n|Capabilities|Not supported Supported Supported Supported Supported Supported Not supported Not supported Not supported Supported Supported Supported Supported|\n|Versions||\n|Latest update|November 2025|\n|Knowledge cutoff|January 2025|... ### Gemini 3 Pro Image Preview\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-3-pro-image-preview`|\n|Supported data types|Image and Text Image and Text|\n|Token limits [*]|65,536 32,768|\n|Capabilities|Not supported Supported Not supported Not supported Not supported Not supported Not supported Supported Not supported Supported Supported Supported Not supported|\n|Versions||\n|Latest update|November 2025|\n|Knowledge cutoff|January 2025|\nOUR ADVANCED THINKING MODEL... ### Gemini 2.5 Pro\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.5-pro`|\n|Supported data types|Audio, images, video, text, and PDF Text|\n|Token limits [*]|1,048,576 65,536|\n|Capabilities|Not supported Supported Supported Supported Supported Supported Supported Not supported Not supported Supported Supported Supported Supported|\n|Versions||\n|Latest update|June 2025|\n|Knowledge cutoff|January 2025|... ### Gemini 2.5 Pro TTS\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.5-pro-preview-tts`|\n|Supported data types|Text Audio|\n|Token limits [*]|8,192 16,384|\n|Capabilities|Supported Not Supported Not supported Not supported Not Supported Not supported Not supported Not supported Not supported Not supported Not supported Not supported Not supported|\n|Versions||\n|Latest update|May 2025|\nFAST AND INTELLIGENT... ### Gemini 2.5 Flash Preview\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.5-flash-preview-09-2025`|\n|Supported data types|Text, images, video, audio Text|\n|Token limits [*]|1,048,576 65,536|\n|Capabilities|Not supported Supported Supported Supported Supported Supported Not supported Not supported Not supported Supported Supported Supported Supported|\n|Versions||\n|Latest update|September 2025|\n|Knowledge cutoff|January 2025|... |Versions|gemini-live-2.5-flash-preview will be deprecated on December 09, 2025|\n|Latest update|September 2025|\n|Knowledge cutoff|January 2025|... |Token limits [*]|1,048,576 8,192|\n|Capabilities|Supported Not supported Not supported Supported Not supported Supported Not supported Not supported Supported Supported Supported Not supported Supported|\n|Versions||\n|Latest update|April 2025|\n|Knowledge cutoff|August 2024|\nOUR SECOND GENERATION FAST MODEL... ## Model version name patterns\n\nGemini models are available in either\n\n*stable*, *preview*, *latest*, or\n\n*experimental* versions.\n\n### Stable\n\nPoints to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nFor example:\n\n`gemini-2.5-flash`.... ### Preview\n\nPoints to a preview model which may be used for production. Preview models will typically have billing enabled, might come with more restrictive rate limits and will be deprecated with at least 2 weeks notice.\n\nFor example:\n\n`gemini-2.5-flash-preview-09-2025`.... ### Latest\n\nPoints to the latest release for a specific model variation. This can be a\n\nstable, preview or experimental release. This alias will get hot-swapped with\n\nevery new release of a specific model variation. A\n\n**2-week notice** will\n\nbe provided through email before the version behind latest is changed.\n\nFor example:\n\n`gemini-flash-latest`.... ### Experimental\n\nPoints to an experimental model which will typically be not be suitable for production use and come with more restrictive rate limits. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nExperimental models are not stable and availability of model endpoints is subject to change.\n\n## Model deprecations\n\nFor information about model deprecations, visit the Gemini deprecations page."
          },
          {
            "snippetId": "model-details-snippet-2",
            "url": "https://www.youtube.com/watch?v=hf59-PhO3do",
            "snippet": "## AI Papers Slop\n\n##### Nov 18, 2025 (0:15:25)\nTitle: Gemini 3 Pro Model Card\nLink: https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf\nDate: November 2025... Chapters:\n00:00 - Gemini 3 Executive Summary\n01:37 - MoE Architecture Explained\n03:07 - 1 Million Token Capabilities\n03:49 - Synthetic Data and Training\n04:58 - Reasoning and Multimodal Benchmarks\n06:54 - Deep Think Ultra Tier\n08:01 - Vibe Coding Performance\n09:09 - Anti-Gravity Agent Platform\n10:35 - Safety Metrics and Regressions\n11:56 - Frontier Risk Analysis\n13:39 - Availability and Reliability Challenges... ### Transcript\n\n{ts:0} We are jumping straight into it today, analyzing the brand new Gemini 3 Pro\n{ts:4} model card. If you're an engineer, researcher, this is for you. We're going to pull out the high impact essential\n{ts:10} facts you need to know about Google's new flagship model.... And the headline here really isn't just\n{ts:15} an incremental update. This is a a pretty decisive architectural leap, I think, toward what you could call\n{ts:22} egentic AGI. The model card confirms it's a sparse mixture of experts or MOE transformer,\n{ts:28} which is so critical.... It is because it decouples that huge model capacity from the raw computational cost,\n{ts:34} right? And that capacity means real scale you can actually use. We're talking native multimodal text, audio,\n{ts:40} image, video, and a just an enormous 1 million token context window, plus a 64k token output that changes things for... Yes. And they really stress that this is not a fine-tune of 2.5 Pro or any prior\n{ts:105} model. It's a completely new build from the ground up and that new foundation is all about\n{ts:110} this sparse mixture of experts architecture the MOI model right and you know the core idea has... I mean, we're talking about feeding it entire data sets, huge\n{ts:200} complex documents, long audio files, video, and this is the critical part for developers, entire code repository,\n{ts:208} a whole repo, the whole thing. You can give it the full documentation for a complex library... {ts:213} and just ask it to write a patch or explain a dependency. That's a huge shift. that ability to just ingest an\n{ts:220} entire repo without any pre-training or fine-tuning. That's a massive deal for enterprise use. So, okay, how do they\n{ts:227} feed this thing?... Let's talk training data. The card mentions a large scale diverse\n{ts:232} and uh natively multimodal collection. So, you have the usual suspects, public web data, licensed data, and some user\n{ts:240} data with, you know, the standard controls. But there was one thing that really... And the post training using reinforcement learning on stuff like theorem proving data just\n{ts:276} doubles down on that goal. It's all pointing towards agency. And this was all done on Google's own\n{ts:281} hardware. Their TPUs, the tensor processing units, all orchestrated with JX and ML pathways.... The entire platform is designed to let the agent plan and execute complex end to-end\n{ts:566} tasks on its own and critically validate its own work. Critically, it can run tests, check the\n{ts:572} output, see if the UI looks right in the browser. It's a closed loop.... What about the frontier risks? How is Google\n{ts:640} handling safety? They say it's their most secure model yet. They detail all the mitigations,\n{ts:644} data set filtering, RLHF, critic feedback, all the standard stuff. But we need to look closely at the internal\n{ts:650} safety metrics they shared.... {ts:809} challenges. So, it has some foundational planning abilities, but it's not demonstrating dangerous levels of\n{ts:814} self-motivated autonomy in these tests. It's an agent, but it's not a rogue agent.\n{ts:818} So, what does this all mean for developers today?... The capabilities are obviously huge, and the safety analysis,\n{ts:825} while needing a critical eye, seems to clear it for general release. It's rolling out now. Developers can get\n{ts:830} it in AI Studio, Vert.Ex AI, the CLI, and through that new anti-gravity platform."
          },
          {
            "snippetId": "model-details-snippet-3",
            "url": "https://web.archive.org/web/20251118111103if_/https:/storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf",
            "snippet": "Gemini 3 Pro \nModel Card\nGemini 3 Pro - Model Card \n \n \n \nModel Cards are intended to provide essential information on Gemini models, including known \nlimitations, mitigation approaches, and safety performance. Model cards may be updated from \ntime-to-time; for example, to include updated evaluations as the model is improved or revised. See the \nGoogle DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous \nmodel cards did. We hope more information about the training dataset, distribution, and intended uses \nwill empower developers with deeper insights and help build more robust and responsible downstream \napplications. \n \nPublished / Model Release: November 2025... Model Information \n \nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of \nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced \nmodel for complex tasks, and can comprehend vast datasets, challenging problems from different \ninformation sources, including text, audio, images, video, and entire code repositories.... data with paired instructions and responses in addition to human preference and tool-use data. Gemini \n3 Pro is trained using reinforcement learning techniques that can leverage multi-step reasoning, \nproblem-solving and theorem-proving data.  \n \nThe training dataset also includes: publicly available datasets that are readily downloadable; data \nobtained by crawlers; licensed data obtained via commercial licensing agreements; user data (i.e., data... collected from users of Google products and services to train AI models, along with user interactions \nwith the model) in accordance with Google’s relevant terms of service, privacy policy, service-specific \npolicies, and pursuant to user controls, where appropriate; other datasets that Google acquires or \ngenerates in the course of its business operations, or directly from its workforce; and AI-generated... growing complexity of large foundation models. Training can be distributed across multiple TPU devices \nfor faster and more efficient processing. \n \nThe efficiencies gained through the use of TPUs are aligned with Google's commitment to operate \nsustainably. \n \nSoftware: Training was done using JAX and ML Pathways. \n \n \n2... Distribution \n \nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; \nrespective documentation shared in line: \n \n●​\nGemini App \n●​\nGoogle Cloud / Vertex AI \n●​\nGoogle AI Studio \n●​\nGemini API \n●​\nGoogle AI Mode \n●​... Google Antigravity \n \nOur models are available to downstream providers via an application program interface (API) and \nsubject to relevant terms of use. There is no required hardware or software to use the model. For \nAI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google \nCloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini \nAPI in Vertex AI quickstart.  \n \n \n \n3... applications that require:  agentic performance, advanced coding, long context and/or multimodal \nunderstanding, and/or algorithmic development. \n \nKnown Limitations: Gemini 3 Pro may exhibit some of the general limitations of foundation models, \nsuch as hallucinations. There may also be occasional slowness or timeout issues. The knowledge cutoff... date for Gemini 3 Pro was January 2025.  \n \nAcceptable Usage: Google’s Generative AI Prohibited Use Policy applies to uses of the model in \naccordance with the applicable terms of service. Additionally, the model should not be integrated into \ncertain systems (also found in Google’s Generative AI Prohibited Use Policy), including those that: (1)... Evaluation Approach: Gemini 3 Pro was developed in partnership with internal safety, security, and \nresponsibility teams. A range of evaluations and red teaming activities were conducted to help improve \nthe model and inform decision-making. These evaluations and activities align with Google's AI Principles \nand responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use... Policy and the Gemini API Additional Terms of Service).    \n \nEvaluation types included but were not limited to:  \n \n●​\nTraining/Development Evaluations including automated and human evaluations carried out \ncontinuously throughout and after the model’s training, to monitor its progress and \nperformance; \n●​\nHuman Red Teaming conducted by specialist teams who sit outside of the model development... Training and Development Evaluation Results:  Results for some of the internal safety evaluations \nconducted during the development phase are listed below. The evaluation results are for automated \nevaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage \nincrease or decrease in performance compared to the indicated model, as described below. Overall,... Instrumental \nReasoning​\nLevels 1 + 2 \nCCL not \nreached \n \nMore details can be found in the Gemini 3 Pro Frontier Safety Framework Report. \n \n \n \n8"
          },
          {
            "snippetId": "model-details-snippet-4",
            "url": "https://huggingface.co/datasets/multimodalart/google-gemini-3-pro-pre-release-model-card",
            "snippet": "# Gemini 3 Pro Model Card\n\n### ⚠️ This is a mirror to the pre-release model card taht was officially published by Google @ https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf and subsequentially yanked. It will get outdated by the actual release version in a few hours (date of publication Nov 18, 13:16 CET)\n\n*Model card published: November, 2025*... # Gemini 3 Pro - Model Card\n\n*Model Cards* *are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from*\n\n*time-to-time; for example, to include updated evaluations as the model is improved or revised. See the* *Google DeepMind site* *for a comprehensive list of model cards.*... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\n*Published / Model Release: November 2025*... ## Model Information\n\n**Description**: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of\n\nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google's most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from diﬀerent information sources, including text, audio, images, video, and entire code repositories.... **Model dependencies:** This model is not a modiﬁcation or a ﬁne-tune of a prior model.\n\n**Inputs:** Text strings (e.g., a question, a prompt, document(s) to be summarized), images, audio, and video ﬁles, with a token context window of up to 1M.... ## Model Data\n\n**Training Dataset:** The pre-training dataset was a large-scale, diverse collection of data encompassing a wide range of domains and modalities, which included publicly-available web-documents, text, code (various programming languages), images, audio (including speech and other audio types) and video.\n\nThe post-training dataset consisted of vetted instruction tuning data and was a collection of multimodal data with paired instructions and responses in addition to human preference and tool-use data. Gemini 3 Pro is trained using reinforcement learning techniques that can leverage multi-step reasoning, problem-solving and theorem-proving data.... The training dataset also includes: publicly available datasets that are readily downloadable; data obtained by crawlers; licensed data obtained via commercial licensing agreements; user data (i.e., data collected from users of Google products and services to train AI models, along with user interactions with the model) in accordance with Google's relevant terms of service, privacy policy, service-speciﬁc policies, and pursuant to user controls, where appropriate; other datasets that Google acquires or generates in the course of its business operations, or directly from its workforce; and AI-generated synthetic data.... The eﬃciencies gained through the use of TPUs are aligned with Google's commitment to operate sustainably.\n\n**Software:** Training was done using JAX and ML Pathways.... ## Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\nOur models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... ## Intended Usage and Limitations\n\n**Beneﬁt and Intended Usage:** Gemini 3 Pro is our most intelligent and adaptive model yet, capable of helping with real-world complexity, solving problems that require enhanced reasoning and intelligence, creativity, strategic planning and making improvements step-by-step. It is particularly well-suited for applications that require: agentic performance, advanced coding, long context and/or multimodal understanding, and/or algorithmic development.... **Known Limitations:** Gemini 3 Pro may exhibit some of the general limitations of foundation models, such as hallucinations. There may also be occasional slowness or timeout issues. The knowledge cutoﬀ date for Gemini 3 Pro was January 2025.... ## Ethics and Content Safety\n\n**Evaluation Approach:** Gemini 3 Pro was developed in partnership with internal safety, security, and responsibility teams. A range of evaluations and red teaming activities were conducted to help improve the model and inform decision-making. These evaluations and activities align with Google's AI Principles and responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use Policy and the Gemini API Additional Terms of Service)."
          },
          {
            "snippetId": "model-details-snippet-5",
            "url": "https://www.studocu.com/en-gb/document/university-of-east-london/financial-accounting/gemini-3-pro-model-card-insights-safety-evaluations-nov-2025/146206142",
            "snippet": "Key Concepts\n\n- Multimodal Capabilities: Gemini 3 Pro processes text, images, audio, and video, enhancing its versatility in handling diverse data types.\n\n- Sparse MoE Architecture: This model employs a mixture of experts approach, optimizing computational efficiency by activating only relevant parameters.\n\n- Training Dataset: A comprehensive dataset includes various modalities and is processed to ensure quality and safety.... - Evaluation Methods: The model undergoes rigorous testing across benchmarks to assess its reasoning and multimodal performance.\n\n- Safety Policies: Strict guidelines are in place to prevent harmful content generation, ensuring responsible AI use.\n\n- Intended Applications: Designed for complex problem-solving, the model excels in areas requiring advanced reasoning and creativity.\n\n## Preview text\n\nModel card published: November, 2025... ## Gemini 3 Pro\n\n## Model Card\n\n### Gemini 3 Pro - Model Card\n\nModel Cards are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from time-to-time; for example, to include updated evaluations as the model is improved or revised. See the Google DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\nPublished / Model Release: November 2025... #### Model Information\n\nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of highly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from different information sources, including text, audio, images, video, and entire code repositories.... Model dependencies: This model is not a modification or a fine-tune of a prior model.\n\nInputs: Text strings (e., a question, a prompt, document(s) to be summarized), images, audio, and video files, with a token context window of up to 1M.\n\nOutputs: Text, with a 64K token output.... Architecture: Gemini 3 Pro is a sparse mixture-of-experts (MoE) (Clark et al., 2022; Du et al., 2021; Fedus et al., 2021; Jiang et al., 2024, Lepikhin et al., 2020; Riquelme et al., 2021; Roller et al., 2021; Shazeer et al., 2017 ) transformer-based model (Vaswani et al., 2017) with native multimodal support for text, vision, and audio inputs.... #### Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\n● Gemini App ● Google Cloud / Vertex AI ● Google AI Studio ● Gemini API ● Google AI Mode ● Google Antigravity... Our models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... #### Evaluation\n\nApproach: Gemini 3 Pro was evaluated across a range of benchmarks, including reasoning, multimodal capabilities, agentic tool use, multi-lingual performance, and long-context. Additional benchmarks and details on approach, results and their methodologies can be found at: deepmind/models/evals/gemini-3-pro.... Evaluation 1 Description Gemini 3 Pro vs. Gemini 2 Pro\n\nText to Text Safety Automated content safety evaluation measuring safety policies -10%\n\nMultilingual Safety Automated safety policy evaluation across multiple languages +0% (non-egregious)\n\nImage to Text Safety Automated content safety evaluation measuring safety policies +3% (non-egregious)... Human Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level findings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams to protect children online and meet Google’s commitments to child safety across our models and Google products.... ● dataset filtering; ● conditional pre-training; ● supervised fine-tuning; ● reinforcement learning from human and critic feedback; ● safety policies and desiderata; ● product-level mitigations such as safety filtering.\n\nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2 Pro but still an open research problem), and b) possible degradation in multi-turn conversations."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Model overview",
            "score": 1,
            "explanation": "Gemini 3 Pro is now Google’s most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from different information sources."
          },
          {
            "name": "Organization developing the model",
            "score": 1,
            "explanation": "Gemini 3 Pro was developed in partnership with internal safety, security, and responsibility teams."
          },
          {
            "name": "Model Version",
            "score": 1,
            "explanation": "Model card published: November, 2025"
          },
          {
            "name": "Model Release Date",
            "score": 1,
            "explanation": "Published / Model Release: November 2025"
          },
          {
            "name": "Model Version Progression",
            "score": 1,
            "explanation": "This model is not a modification or a fine-tune of a prior model."
          },
          {
            "name": "Model Architecture",
            "score": 1,
            "explanation": "Gemini 3 Pro is a sparse mixture-of-experts (MoE) transformer-based model."
          },
          {
            "name": "Model Dependencies",
            "score": 1,
            "explanation": "This model is not a modification or a fine-tune of a prior model."
          },
          {
            "name": "Paper and relevant links",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Model Distribution Forms",
            "score": 1,
            "explanation": "The Gemini family of models, including Gemini 3 Pro, are distributed in the following channels."
          }
        ]
      },
      {
        "sectionId": "model-inputs-outputs",
        "sectionSnippets": [
          {
            "snippetId": "model-inputs-outputs-snippet-1",
            "url": "https://web.archive.org/web/20251118111103if_/https:/storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf",
            "snippet": "Gemini 3 Pro \nModel Card\nGemini 3 Pro - Model Card \n \n \n \nModel Cards are intended to provide essential information on Gemini models, including known \nlimitations, mitigation approaches, and safety performance. Model cards may be updated from \ntime-to-time; for example, to include updated evaluations as the model is improved or revised. See the \nGoogle DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous \nmodel cards did. We hope more information about the training dataset, distribution, and intended uses \nwill empower developers with deeper insights and help build more robust and responsible downstream \napplications. \n \nPublished / Model Release: November 2025... Model Information \n \nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of \nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced \nmodel for complex tasks, and can comprehend vast datasets, challenging problems from different \ninformation sources, including text, audio, images, video, and entire code repositories.... Model dependencies: This model is not a modification or a fine-tune of a prior model.   \n \nInputs: Text strings (e.g., a question, a prompt, document(s) to be summarized), images, audio, and \nvideo files, with a token context window of up to 1M.  \n \nOutputs:  Text, with a 64K token output.... data with paired instructions and responses in addition to human preference and tool-use data. Gemini \n3 Pro is trained using reinforcement learning techniques that can leverage multi-step reasoning, \nproblem-solving and theorem-proving data.  \n \nThe training dataset also includes: publicly available datasets that are readily downloadable; data \nobtained by crawlers; licensed data obtained via commercial licensing agreements; user data (i.e., data... specifically designed to handle the massive computations involved in training LLMs and can speed up \ntraining considerably compared to CPUs. TPUs often come with large amounts of high-bandwidth \nmemory, allowing for the handling of large models and batch sizes during training, which can lead to \nbetter model quality. TPU Pods (large clusters of TPUs) also provide a scalable solution for handling the... growing complexity of large foundation models. Training can be distributed across multiple TPU devices \nfor faster and more efficient processing. \n \nThe efficiencies gained through the use of TPUs are aligned with Google's commitment to operate \nsustainably. \n \nSoftware: Training was done using JAX and ML Pathways. \n \n \n2... Distribution \n \nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; \nrespective documentation shared in line: \n \n●​\nGemini App \n●​\nGoogle Cloud / Vertex AI \n●​\nGoogle AI Studio \n●​\nGemini API \n●​\nGoogle AI Mode \n●​... Google Antigravity \n \nOur models are available to downstream providers via an application program interface (API) and \nsubject to relevant terms of use. There is no required hardware or software to use the model. For \nAI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google \nCloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini \nAPI in Vertex AI quickstart.  \n \n \n \n3... applications that require:  agentic performance, advanced coding, long context and/or multimodal \nunderstanding, and/or algorithmic development. \n \nKnown Limitations: Gemini 3 Pro may exhibit some of the general limitations of foundation models, \nsuch as hallucinations. There may also be occasional slowness or timeout issues. The knowledge cutoff... Evaluation Approach: Gemini 3 Pro was developed in partnership with internal safety, security, and \nresponsibility teams. A range of evaluations and red teaming activities were conducted to help improve \nthe model and inform decision-making. These evaluations and activities align with Google's AI Principles \nand responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use... Policy and the Gemini API Additional Terms of Service).    \n \nEvaluation types included but were not limited to:  \n \n●​\nTraining/Development Evaluations including automated and human evaluations carried out \ncontinuously throughout and after the model’s training, to monitor its progress and \nperformance; \n●​\nHuman Red Teaming conducted by specialist teams who sit outside of the model development... concerns. \n \nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and \ndeployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations \ninclude, but are not limited to:  \n \n●​\ndataset filtering;  \n●​... Instrumental \nReasoning​\nLevels 1 + 2 \nCCL not \nreached \n \nMore details can be found in the Gemini 3 Pro Frontier Safety Framework Report. \n \n \n \n8"
          },
          {
            "snippetId": "model-inputs-outputs-snippet-2",
            "url": "https://ai.google.dev/gemini-api/docs/tokens",
            "snippet": "## Context windows\n\nThe models available through the Gemini API have context windows that are measured in tokens. The context window defines how much input you can provide and how much output the model can generate. You can determine the size of the context window by calling the getModels endpoint or by looking in the models documentation.\n\nIn the following example, you can see that the... `gemini-1.5-flash` model has an\n\ninput limit of about 1,000,000 tokens and an output limit of about 8,000 tokens,\n\nwhich means a context window is 1,000,000 tokens.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\nmodel_info = client.models.get(model=\"gemini-2.0-flash\")\n\nprint(f\"{model_info.input_token_limit=}\")\n\nprint(f\"{model_info.output_token_limit=}\")\n\n# ( e.g., input_token_limit=30720, output_token_limit=2048 )count_tokens.py\n\n```... ## Count tokens\n\nAll input to and output from the Gemini API is tokenized, including text, image files, and other non-text modalities.\n\nYou can count tokens in the following ways:... # ( e.g., total_tokens: 10 )\n\nresponse = client.models.generate_content(\n\nmodel=\"gemini-2.0-flash\", contents=prompt\n\n\n\n# The usage_metadata provides detailed token counts.\n\nprint(response.usage_metadata)\n\n# ( e.g., prompt_token_count: 11, candidates_token_count: 73, total_token_count: 84 )count_tokens.py\n\n```... ### Count multi-turn (chat) tokens\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\nchat = client.chats.create(\n\nmodel=\"gemini-2.0-flash\",\n\nhistory=[\n\ntypes.Content(\n\nrole=\"user\", parts=[types.Part(text=\"Hi my name is Bob\")]\n\n),\n\ntypes.Content(role=\"model\", parts=[types.Part(text=\"Hi Bob!\")]),... ],\n\n\n\n# Count tokens for the chat history.\n\nprint(\n\nclient.models.count_tokens(\n\nmodel=\"gemini-2.0-flash\", contents=chat.get_history()\n\n\n\n# ( e.g., total_tokens: 10 )\n\nresponse = chat.send_message(\n\nmessage=\"In one sentence, explain how a computer works to a young child.\"... ### Count multimodal tokens\n\nAll input to the Gemini API is tokenized, including text, image files, and other non-text modalities. Note the following high-level key points about tokenization of multimodal input during processing by the Gemini API:\n\nWith Gemini 2.0, image inputs with both dimensions <=384 pixels are counted as 258 tokens. Images larger in one or both dimensions are cropped and scaled as needed into tiles of 768x768 pixels, each counted as 258 tokens. Prior to Gemini 2.0, images used a fixed 258 tokens.... Video and audio files are converted to tokens at the following fixed rates: video at 263 tokens per second and audio at 32 tokens per second.\n\n#### Media resolutions\n\nGemini 3 Pro Preview introduces granular control over multimodal vision processing with the\n\n`media_resolution` parameter. The\n\n`media_resolution` parameter determines the\n\n**maximum number of tokens allocated per input image or video frame.**\n\nHigher resolutions improve the model's ability to\n\nread fine text or identify small details, but increase token usage and latency.\n\nFor more details about the parameter and how it can impact token calculations, see the media resolution guide.... response = client.models.generate_content(\n\nmodel=\"gemini-2.0-flash\", contents=[prompt, your_image_file]\n\n\n\nprint(response.usage_metadata)\n\n# ( e.g., prompt_token_count: 264, candidates_token_count: 80, total_token_count: 345 )count_tokens.py\n\n```... Example that provides the image as inline data:\n\n```\n\nfrom google import genai\n\nimport PIL.Image\n\nclient = genai.Client()\n\nprompt = \"Tell me about this image\"\n\nyour_image_file = PIL.Image.open(media / \"organ.jpg\")\n\n# Count tokens for combined text and inline image.\n\nprint(\n\nclient.models.count_tokens(\n\nmodel=\"gemini-2.0-flash\", contents=[prompt, your_image_file]... # ( e.g., total_tokens: 263 )\n\nresponse = client.models.generate_content(\n\nmodel=\"gemini-2.0-flash\", contents=[prompt, your_image_file]\n\n\n\nprint(response.usage_metadata)\n\n# ( e.g., prompt_token_count: 264, candidates_token_count: 80, total_token_count: 345 )count_tokens.py\n\n```... ### System instructions and tools\n\nSystem instructions and tools also count towards the total token count for the input.\n\nIf you use system instructions, the\n\n`total_tokens` count increases to\n\nreflect the addition of\n\n`system_instruction`.\n\nIf you use function calling, the\n\n`total_tokens` count increases to reflect the\n\naddition of\n\n`tools`."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-3",
            "url": "https://www.youtube.com/watch?v=-XTNBIVXRGA",
            "snippet": "## AI Papers Podcast Daily\n\n##### Nov 20, 2025 (0:13:04)\n\nThe **Gemini 3 Pro Model Card**, published by Google DeepMind in November 2025, introduces the latest generation of the Gemini series, describing it as Google’s most advanced, highly-capable, and natively multimodal reasoning model. Architecturally, Gemini 3 Pro is a sparse Mixture-of-Experts (MoE) transformer-based model, capable of understanding vast datasets and complex problems from various inputs, including text, images, audio, video, and code repositories, utilizing an extensive 1M token context window.... The model was trained using reinforcement learning techniques on a large, diverse dataset, and processed efficiently using Google’s specialized Tensor Processing Units (TPUs). Intended for applications requiring enhanced reasoning, intelligence, advanced coding, and agentic performance, Gemini 3 Pro significantly outperforms the previous Gemini 2.5 Pro model across a range of benchmarks. Development included continuous training evaluations, human red teaming by specialist teams, and adherence to safety policies designed to prevent the generation of harmful content, confirming that Gemini 3 Pro satisfied required launch thresholds for child safety and showed improved overall safety performance compared to its predecessor.... https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf... ### Transcript\n\n{ts:0} Welcome to the AI Papers podcast daily. We have a foundational piece of\n{ts:4} documentation that has just dropped and uh it's really going to set the pace for advanced models over the next year.\n{ts:11} We're going deep on the technical mandate for what Google DeepMind thinks is the new frontier.... {ts:16} That's absolutely right. Our focus today is the brand new Gemini 3 Pro model card just published in November 2025. And\n{ts:25} this is not a small update. This document signals a pretty massive architectural and capability shift.\n{ts:31} Google is calling it their most advanced model for complex tasks.... {ts:113} manage all that input which brings us to scale. I mean and the number that immediately jumps off the\n{ts:117} page is the context window. We're talking about an input of up to 1 million tokens. 1 million.\n{ts:122} Right. And for context a million tokens.... With a million tokens, the model can keep track of details buried anywhere in that\n{ts:146} entire stack of information. But there's an interesting choice here. The input is 1 million tokens, but the\n{ts:152} model card says the text output is limited to 64,000.... And this is where it gets really interesting where we go from just scale to efficiency. The architecture itself.\n{ts:190} The model card specifies Gemini 3 Pro uses a transformer-based sparse mixture of experts or MOI. This\n{ts:198} is a huge concept for anyone building with these models.... {ts:236} So if I ask a programming question, it wakes up the coding expert and maybe the logic expert, but the poetry expert gets\n{ts:243} to stay asleep. Precisely. And that's the core benefit they highlight in the model card. It\n{ts:248} lets Google decouple the model's total capacity, you know, how smart it is in theory, from the actual computation cost... It is. And that efficiency leads to the\n{ts:272} next question. How do you train something this complex? Let's get into the engine room. the training and the\n{ts:277} infrastructure, the pre-training data set is just immense. We're talking web documents, code, images, audio, video.... And you can't talk about training\n{ts:370} without the hardware. This all runs on Google's own silicon. Their tensor processing units, TPUs on scalable TPU\n{ts:378} pods. Yeah, the TPUs are just powerhouses for the kind of math these models need. And\n{ts:383} using their own custom hardware isn't just about speed.... {ts:571} development. If you need a model that can think several steps ahead, this is the tool.\n{ts:575} But even with this huge leap, we have to be realistic. What are the known limitations that the model card flags?\n{ts:582} Well, the usual foundation model issues are still there, primarily hallucinations.... It's not perfect.\n{ts:587} Developers have to build in safeguards. They also note occasional slowness or timeout issues, which probably reflects\n{ts:593} the complexity of the MOI routing. And a really critical point for anyone who needs up-to-date information.\n{ts:599} Yes, the card clearly states the model's knowledge cutoff was January 2025."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-4",
            "url": "https://huggingface.co/datasets/multimodalart/google-gemini-3-pro-pre-release-model-card",
            "snippet": "# Gemini 3 Pro Model Card\n\n### ⚠️ This is a mirror to the pre-release model card taht was officially published by Google @ https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf and subsequentially yanked. It will get outdated by the actual release version in a few hours (date of publication Nov 18, 13:16 CET)\n\n*Model card published: November, 2025*... # Gemini 3 Pro - Model Card\n\n*Model Cards* *are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from*\n\n*time-to-time; for example, to include updated evaluations as the model is improved or revised. See the* *Google DeepMind site* *for a comprehensive list of model cards.*... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\n*Published / Model Release: November 2025*... ## Model Information\n\n**Description**: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of\n\nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google's most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from diﬀerent information sources, including text, audio, images, video, and entire code repositories.... **Model dependencies:** This model is not a modiﬁcation or a ﬁne-tune of a prior model.\n\n**Inputs:** Text strings (e.g., a question, a prompt, document(s) to be summarized), images, audio, and video ﬁles, with a token context window of up to 1M.... **Outputs**: Text, with a 64K token output.\n\n**Architecture**: Gemini 3 Pro is a sparse mixture-of-experts (MoE) (Clark et al., 2022; Du et al., 2021; Fedus et al., 2021; Jiang et al., 2024, Lepikhin et al., 2020; Riquelme et al., 2021; Roller et al., 2021; Shazeer et al., 2017) transformer-based model (Vaswani et al., 2017) with native multimodal support for text, vision, and audio inputs.... ## Implementation and Sustainability\n\n**Hardware:** Gemini 3 Pro was trained using Google's Tensor Processing Units (TPUs). TPUs are speciﬁcally designed to handle the massive computations involved in training LLMs and can speed up training considerably compared to CPUs. TPUs often come with large amounts of high-bandwidth memory, allowing for the handling of large models and batch sizes during training, which can lead to better model quality. TPU Pods (large clusters of TPUs) also provide a scalable solution for handling the growing complexity of large foundation models. Training can be distributed across multiple TPU devices for faster and more eﬃcient processing.... The eﬃciencies gained through the use of TPUs are aligned with Google's commitment to operate sustainably.\n\n**Software:** Training was done using JAX and ML Pathways.... ## Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\nOur models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... ## Intended Usage and Limitations\n\n**Beneﬁt and Intended Usage:** Gemini 3 Pro is our most intelligent and adaptive model yet, capable of helping with real-world complexity, solving problems that require enhanced reasoning and intelligence, creativity, strategic planning and making improvements step-by-step. It is particularly well-suited for applications that require: agentic performance, advanced coding, long context and/or multimodal understanding, and/or algorithmic development.... **Training and Development Evaluation Results:** Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustiﬁed refusals low. We mark improvements in green and regressions in red."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-5",
            "url": "https://ai.google.dev/gemini-api/docs/gemini-3",
            "snippet": "Gemini 3 is our most intelligent model family to date, built on a foundation of state-of-the-art reasoning. It is designed to bring any idea to life by mastering agentic workflows, autonomous coding, and complex multimodal tasks. This guide covers key features of the Gemini 3 model family and how to get the most out of it.\n\nGemini 3 Pro uses dynamic thinking by default to reason through prompts. For faster, lower-latency responses when complex reasoning isn't required, you can constrain the model's thinking level to\n\n`low`.... ## Explore\n\nExplore our collection of Gemini 3 apps to see how the model handles advanced reasoning, autonomous coding, and complex multimodal tasks.... ## Meet Gemini 3\n\nGemini 3 Pro is the first model in the new series.\n\n`gemini-3-pro-preview` is best for your complex tasks that require broad world knowledge and advanced reasoning across modalities.... |Model ID|Context Window (In / Out)|Knowledge Cutoff|Pricing (Input / Output)*|\n|--|--|--|--|\n|gemini-3-pro-preview|1M / 64k|Jan 2025|$2 / $12 (<200k tokens) $4 / $18 (>200k tokens)|... |gemini-3-pro-image-preview|65k / 32k|Jan 2025|$2 (Text Input) / $0.134 (Image Output)**|\n** Pricing is per 1 million tokens unless otherwise noted.*\n\n*** Image pricing varies by resolution. See the pricing page for details.*\n\nFor detailed rate limits, batch pricing, and additional information, see the models page.... ## New API features in Gemini 3\n\nGemini 3 introduces new parameters designed to give developers more control over latency, cost, and multimodal fidelity.... ### Media resolution\n\nGemini 3 introduces granular control over multimodal vision processing via the\n\n`media_resolution` parameter. Higher resolutions improve the model's ability to read fine text or identify small details, but increase token usage and latency. The\n\n`media_resolution` parameter determines the\n\n**maximum number of tokens allocated per input image or video frame.**... #### Image Generation & Editing\n\nFor image generation, signatures are strictly validated. They appear on the\n\n**first part** (text or image) and **all subsequent image parts**. All must be returned in the next turn.... ### Structured Outputs with tools\n\nGemini 3 allows you to combine Structured Outputs with built-in tools, including Grounding with Google Search, URL Context, and Code Execution.... \"response_json_schema\": MatchResult.model_json_schema(),\n\n},\n\n\n\nresult = MatchResult.model_validate_json(response.text)\n\nprint(result)\n\n```... ### Image generation\n\nGemini 3 Pro Image lets you generate and edit images from text prompts. It uses reasoning to \"think\" through a prompt and can retrieve real-time data—such as weather forecasts or stock charts—before using Google Search grounding before generating high-fidelity images.\n\n**New & improved capabilities:** **Native 4K & text rendering:**Generate sharp, legible text and diagrams with native upscaling to 2K and 4K resolutions. **Grounded generation:**Use the... `media_resolution_high`setting to ensure continued accuracy.\n\n**Token consumption:**Migrating to Gemini 3 Pro defaults may **increase**token usage for PDFs but **decrease**token usage for video. If requests now exceed the context window due to higher default resolutions, we recommend explicitly reducing the media resolution. **Image segmentation:**Image segmentation capabilities (returning pixel-level masks for objects) are not supported in Gemini 3 Pro. For workloads requiring native image segmentation, we recommend continuing to utilize Gemini 2.5 Flash with thinking turned off or Gemini Robotics-ER 1.5.... ## FAQ\n\n**What is the knowledge cutoff for Gemini 3 Pro?**Gemini 3 has a knowledge cutoff of January 2025. For more recent information, use the Search Grounding tool. **What are the context window limits?**Gemini 3 Pro supports a 1 million token input context window and up to 64k tokens of output. **Is there a free tier for Gemini 3 Pro?**You can try the model for free in Google AI Studio, but currently, there is no free tier available for... **Does Gemini 3 support the Batch API?**Yes, Gemini 3 supports the Batch API. **Is Context Caching supported?**Yes, Context Caching is supported for Gemini 3. The minimum token count required to initiate caching is 2,048 tokens. **Which tools are supported in Gemini 3?**Gemini 3 supports Google Search, File Search, Code Execution, and URL Context. It also supports standard Function Calling for your own custom tools. Please note that Google Maps and Computer Use are currently not supported.... ## Next steps\n\n- Get started with the Gemini 3 Cookbook\n\n- Check the dedicated Cookbook guide on thinking levels and how to migrate from thinking budget to thinking levels."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Inputs",
            "score": 1,
            "explanation": "Text strings (e.g., a question, a prompt, document(s) to be summarized), images, audio, and video files, with a token context window of up to 1M."
          },
          {
            "name": "Outputs",
            "score": 1,
            "explanation": "Text, with a 64K token output."
          },
          {
            "name": "Token Count",
            "score": 1,
            "explanation": "Gemini 3 Pro supports a 1 million token input context window and up to 64k tokens of output."
          }
        ]
      },
      {
        "sectionId": "model-data",
        "sectionSnippets": [
          {
            "snippetId": "model-data-snippet-1",
            "url": "https://web.archive.org/web/20251118111103if_/https:/storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf",
            "snippet": "Gemini 3 Pro \nModel Card\nGemini 3 Pro - Model Card \n \n \n \nModel Cards are intended to provide essential information on Gemini models, including known \nlimitations, mitigation approaches, and safety performance. Model cards may be updated from \ntime-to-time; for example, to include updated evaluations as the model is improved or revised. See the \nGoogle DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous \nmodel cards did. We hope more information about the training dataset, distribution, and intended uses \nwill empower developers with deeper insights and help build more robust and responsible downstream \napplications. \n \nPublished / Model Release: November 2025... Model Information \n \nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of \nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced \nmodel for complex tasks, and can comprehend vast datasets, challenging problems from different \ninformation sources, including text, audio, images, video, and entire code repositories.... Model Data \n \nTraining Dataset: The pre-training dataset was a large-scale, diverse collection of data encompassing a \nwide range of domains and modalities, which included publicly-available web-documents, text, code \n(various programming languages), images, audio (including speech and other audio types) and video. \nThe post-training dataset consisted of vetted instruction tuning data and was a collection of multimodal... data with paired instructions and responses in addition to human preference and tool-use data. Gemini \n3 Pro is trained using reinforcement learning techniques that can leverage multi-step reasoning, \nproblem-solving and theorem-proving data.  \n \nThe training dataset also includes: publicly available datasets that are readily downloadable; data \nobtained by crawlers; licensed data obtained via commercial licensing agreements; user data (i.e., data... collected from users of Google products and services to train AI models, along with user interactions \nwith the model) in accordance with Google’s relevant terms of service, privacy policy, service-specific \npolicies, and pursuant to user controls, where appropriate; other datasets that Google acquires or \ngenerates in the course of its business operations, or directly from its workforce; and AI-generated... synthetic data.   \n \nTraining Data Processing: Data filtering and preprocessing included techniques such as deduplication, \nhonoring robots.txt, safety filtering in-line with Google's commitment to advancing AI safely and \nresponsibly, and quality filtering to mitigate risks and improve training data reliability. Once data is \ncollected, it is cleaned and preprocessed to make it suitable for training. This process involves, on a... growing complexity of large foundation models. Training can be distributed across multiple TPU devices \nfor faster and more efficient processing. \n \nThe efficiencies gained through the use of TPUs are aligned with Google's commitment to operate \nsustainably. \n \nSoftware: Training was done using JAX and ML Pathways. \n \n \n2... Distribution \n \nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; \nrespective documentation shared in line: \n \n●​\nGemini App \n●​\nGoogle Cloud / Vertex AI \n●​\nGoogle AI Studio \n●​\nGemini API \n●​\nGoogle AI Mode \n●​... Intended Usage and Limitations \n \nBenefit and Intended Usage: Gemini 3 Pro is our most intelligent and adaptive model yet, capable of \nhelping with real-world complexity, solving problems that require enhanced reasoning and intelligence, \ncreativity, strategic planning and making improvements step-by-step. It is particularly well-suited for... Evaluation Approach: Gemini 3 Pro was developed in partnership with internal safety, security, and \nresponsibility teams. A range of evaluations and red teaming activities were conducted to help improve \nthe model and inform decision-making. These evaluations and activities align with Google's AI Principles \nand responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use... Policy and the Gemini API Additional Terms of Service).    \n \nEvaluation types included but were not limited to:  \n \n●​\nTraining/Development Evaluations including automated and human evaluations carried out \ncontinuously throughout and after the model’s training, to monitor its progress and \nperformance; \n●​\nHuman Red Teaming conducted by specialist teams who sit outside of the model development... Training and Development Evaluation Results:  Results for some of the internal safety evaluations \nconducted during the development phase are listed below. The evaluation results are for automated \nevaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage \nincrease or decrease in performance compared to the indicated model, as described below. Overall,... concerns. \n \nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and \ndeployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations \ninclude, but are not limited to:  \n \n●​\ndataset filtering;  \n●​"
          },
          {
            "snippetId": "model-data-snippet-2",
            "url": "https://huggingface.co/datasets/multimodalart/google-gemini-3-pro-pre-release-model-card",
            "snippet": "# Gemini 3 Pro - Model Card\n\n*Model Cards* *are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from*\n\n*time-to-time; for example, to include updated evaluations as the model is improved or revised. See the* *Google DeepMind site* *for a comprehensive list of model cards.*... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\n*Published / Model Release: November 2025*... ## Model Information\n\n**Description**: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of\n\nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google's most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from diﬀerent information sources, including text, audio, images, video, and entire code repositories.... **Model dependencies:** This model is not a modiﬁcation or a ﬁne-tune of a prior model.\n\n**Inputs:** Text strings (e.g., a question, a prompt, document(s) to be summarized), images, audio, and video ﬁles, with a token context window of up to 1M.... ## Model Data\n\n**Training Dataset:** The pre-training dataset was a large-scale, diverse collection of data encompassing a wide range of domains and modalities, which included publicly-available web-documents, text, code (various programming languages), images, audio (including speech and other audio types) and video.\n\nThe post-training dataset consisted of vetted instruction tuning data and was a collection of multimodal data with paired instructions and responses in addition to human preference and tool-use data. Gemini 3 Pro is trained using reinforcement learning techniques that can leverage multi-step reasoning, problem-solving and theorem-proving data.... The training dataset also includes: publicly available datasets that are readily downloadable; data obtained by crawlers; licensed data obtained via commercial licensing agreements; user data (i.e., data collected from users of Google products and services to train AI models, along with user interactions with the model) in accordance with Google's relevant terms of service, privacy policy, service-speciﬁc policies, and pursuant to user controls, where appropriate; other datasets that Google acquires or generates in the course of its business operations, or directly from its workforce; and AI-generated synthetic data.... **Training Data Processing:** Data ﬁltering and preprocessing included techniques such as deduplication, honoring robots.txt, safety ﬁltering in-line with Google's commitment to advancing AI safely and responsibly, and quality ﬁltering to mitigate risks and improve training data reliability. Once data is collected, it is cleaned and preprocessed to make it suitable for training. This process involves, on a\n\ncase-by-case basis, ﬁltering irrelevant or harmful content, text, and other modalities, including ﬁltering content that is pornographic, violent, or violative of child sexual abuse material (CSAM) laws.... ## Implementation and Sustainability\n\n**Hardware:** Gemini 3 Pro was trained using Google's Tensor Processing Units (TPUs). TPUs are speciﬁcally designed to handle the massive computations involved in training LLMs and can speed up training considerably compared to CPUs. TPUs often come with large amounts of high-bandwidth memory, allowing for the handling of large models and batch sizes during training, which can lead to better model quality. TPU Pods (large clusters of TPUs) also provide a scalable solution for handling the growing complexity of large foundation models. Training can be distributed across multiple TPU devices for faster and more eﬃcient processing.... The eﬃciencies gained through the use of TPUs are aligned with Google's commitment to operate sustainably.\n\n**Software:** Training was done using JAX and ML Pathways.... ## Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\nOur models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... ## Intended Usage and Limitations\n\n**Beneﬁt and Intended Usage:** Gemini 3 Pro is our most intelligent and adaptive model yet, capable of helping with real-world complexity, solving problems that require enhanced reasoning and intelligence, creativity, strategic planning and making improvements step-by-step. It is particularly well-suited for applications that require: agentic performance, advanced coding, long context and/or multimodal understanding, and/or algorithmic development."
          },
          {
            "snippetId": "model-data-snippet-3",
            "url": "https://www.studocu.com/en-gb/document/university-of-east-london/financial-accounting/gemini-3-pro-model-card-insights-safety-evaluations-nov-2025/146206142",
            "snippet": "Key Concepts\n\n- Multimodal Capabilities: Gemini 3 Pro processes text, images, audio, and video, enhancing its versatility in handling diverse data types.\n\n- Sparse MoE Architecture: This model employs a mixture of experts approach, optimizing computational efficiency by activating only relevant parameters.\n\n- Training Dataset: A comprehensive dataset includes various modalities and is processed to ensure quality and safety.... - Evaluation Methods: The model undergoes rigorous testing across benchmarks to assess its reasoning and multimodal performance.\n\n- Safety Policies: Strict guidelines are in place to prevent harmful content generation, ensuring responsible AI use.\n\n- Intended Applications: Designed for complex problem-solving, the model excels in areas requiring advanced reasoning and creativity.\n\n## Preview text\n\nModel card published: November, 2025... ## Gemini 3 Pro\n\n## Model Card\n\n### Gemini 3 Pro - Model Card\n\nModel Cards are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from time-to-time; for example, to include updated evaluations as the model is improved or revised. See the Google DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\nPublished / Model Release: November 2025... #### Model Information\n\nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of highly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from different information sources, including text, audio, images, video, and entire code repositories.... Model dependencies: This model is not a modification or a fine-tune of a prior model.\n\nInputs: Text strings (e., a question, a prompt, document(s) to be summarized), images, audio, and video files, with a token context window of up to 1M.\n\nOutputs: Text, with a 64K token output.... Architecture: Gemini 3 Pro is a sparse mixture-of-experts (MoE) (Clark et al., 2022; Du et al., 2021; Fedus et al., 2021; Jiang et al., 2024, Lepikhin et al., 2020; Riquelme et al., 2021; Roller et al., 2021; Shazeer et al., 2017 ) transformer-based model (Vaswani et al., 2017) with native multimodal support for text, vision, and audio inputs.... #### Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\n● Gemini App ● Google Cloud / Vertex AI ● Google AI Studio ● Gemini API ● Google AI Mode ● Google Antigravity... Our models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... #### Evaluation\n\nApproach: Gemini 3 Pro was evaluated across a range of benchmarks, including reasoning, multimodal capabilities, agentic tool use, multi-lingual performance, and long-context. Additional benchmarks and details on approach, results and their methodologies can be found at: deepmind/models/evals/gemini-3-pro.... Training and Development Evaluation Results: Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2 Pro across both safety and tone, while keeping unjustified refusals low. We mark improvements in green and regressions in red.... Human Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level findings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams to protect children online and meet Google’s commitments to child safety across our models and Google products.... ● dataset filtering; ● conditional pre-training; ● supervised fine-tuning; ● reinforcement learning from human and critic feedback; ● safety policies and desiderata; ● product-level mitigations such as safety filtering.\n\nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2 Pro but still an open research problem), and b) possible degradation in multi-turn conversations."
          },
          {
            "snippetId": "model-data-snippet-4",
            "url": "https://ai.google.dev/gemini-api/docs/changelog",
            "snippet": "This page documents updates to the Gemini API.\n\n## November 20, 2025\n\n- Released Gemini 3 Pro Image Preview,\n\n`gemini-3-pro-image-preview`, the next iteration to the Nano Banana model. Read the Image generation page for more details.... ## November 18, 2025\n\nLaunched the first Gemini 3 series model,\n\n`gemini-3-pro-preview`, our state-of-the-art reasoning and multimodal understanding model with powerful agentic and coding capabilities.\n\nIn addition to improvements in intelligence and performance, Gemini 3 Pro Preview introduces new behavior around:\n\nRead the Gemini 3 Developer Guide for migration, new features, and specs.... ## November 10, 2025\n\nThe following model is deprecated:\n\n`imagen-3.0-generate-002`\n\nUse Imagen 4 instead. Refer to the Gemini deprecations table for more details.\n\n\n\n## November 6, 2025\n\n- Launched the File Search API to public preview, enabling developers to ground responses in their own data. Read the new File Search page for more info.... ## October 29, 2025\n\n- Launched the new logging and datasets tool for the Gemini API.... ## September 25, 2025\n\nReleased Gemini Robotics-ER 1.5 model in preview. See the Robotics overview to learn about how to use the model for your robotics application.\n\nLaunched following preview models:\n\n`gemini-2.5-flash-preview-09-2025`\n\n`gemini-2.5-flash-lite-preview-09-2025`\n\nSee the Models page for details.... ## June 24, 2025\n\n- Released Imagen 4 Ultra and Standard Preview models. To learn more, see the Image generation page.... ## June 17, 2025\n\n- Released\n\n`gemini-2.5-pro`, the stable version of our most powerful model, now with adaptive thinking. To learn more, see Gemini 2.5 Pro and Thinking.\n\n`gemini-2.5-pro-preview-05-06`will be redirected to... ## June 05, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-06-05`, a new version of our most powerful model, now with adaptive thinking. To learn more, see Gemini 2.5 Pro Preview and Thinking.\n\n`gemini-2.5-pro-preview-05-06`will be redirected to\n\n`gemini-2.5-pro`on June 26, 2025.... **Model updates:**\n\n- Released\n\n`gemini-2.5-flash-preview-05-20`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n- Released the\n\n`gemini-2.5-pro-preview-tts`and... ## May 7, 2025\n\n- Released\n\n`gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see Image generation and Gemini 2.0 Flash Preview Image Generation.\n\n## May 6, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling.\n\n`gemini-2.5-pro-preview-03-25`will automatically point to the new version of the model.... ## February 11, 2025\n\n**API updates:**\n\n- Updates on the OpenAI libraries compatibility.\n\n## February 6, 2025\n\n**Model updates:**\n\n- Released\n\n`imagen-3.0-generate-002`, a generally available (GA) version of Imagen 3 in the Gemini API.\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for Java for public preview.... ## March 19, 2024\n\n**Model updates:**\n\n- Added support for tuning Gemini 1.0 Pro in Google AI Studio or with the Gemini API.... ## December 13 2023\n\n**Model updates:**\n\n- gemini-pro: New text model for a wide variety of tasks. Balances capability and efficiency.\n\n- gemini-pro-vision: New multimodal model for a wide variety of tasks. Balances capability and efficiency.\n\n- embedding-001: New embeddings model.\n\n- aqa: A new specially tuned model that is trained to answer questions using text passages for grounding generated answers.... - Streaming available through the\n\n`StreamGenerateContent`method.\n\n- Multimodal capability: Image is a new supported modality\n\n- New beta features:\n\n- Function Calling\n\n- Semantic Retriever\n\n- Attributed Question Answering (AQA)\n\n- Updated candidate count: Gemini models only return 1 candidate.\n\n- Different Safety Settings and SafetyRating categories. See safety settings for more details.\n\n- Tuning models is not yet supported for Gemini models (Work in progress)."
          },
          {
            "snippetId": "model-data-snippet-5",
            "url": "https://en.wikipedia.org/wiki/Gemini_(language_model)",
            "snippet": "**Gemini** is a family of multimodal large language models (LLMs) developed by Google DeepMind, and the successor to LaMDA and PaLM 2. Comprising Gemini Pro, Gemini Flash, and Gemini Lite, it was announced on December 6, 2023. It powers the chatbot of the same name.... At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2.... It was made available only in English. Touted as Google's \"largest and most capable AI model\" and designed to emulate human behavior, the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs), and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.... Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test, obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13, while Gemini Nano will be made available to Android developers as well.... It also introduces improved agentic capabilities, a new Google Gen AI SDK, and \"Jules,\" an experimental AI coding agent for GitHub. Additionally, Google Colab is integrating Gemini 2.0 to generate data science notebooks from natural language. Gemini 2.0 was available through the Gemini chat interface for all users as \"Gemini 2.0 Flash experimental\".... On January 30, 2025, Google released Gemini 2.0 Flash as the new default model, with Gemini 1.5 Flash still available for usage. This was followed by the release of Gemini 2.0 Pro on February 5, 2025. Additionally, Google released Gemini 2.0 Flash Thinking Experimental, which details the language model's thinking process when responding to prompts.... On March 12, 2025, Google also announced Gemini Robotics, a vision-language-action model based on the Gemini 2.0 family of models.\n\nThe next day, Google announced that Gemini in Android Studio would be able to understand simple UI mockups and transform them into working Jetpack Compose code.... Gemini 2.5 Pro Experimental was released on March 25, 2025, described by Google as its most intelligent AI model yet, featuring enhanced reasoning and coding capabilities, and a \"thinking model\" capable of reasoning through steps before responding, using techniques like chain-of-thought prompting, whilst maintaining native multimodality and launching with a 1 million token context window.... At Google I/O 2025, Google announced significant updates to its Gemini core models. Gemini 2.5 Flash became the default model, delivering faster responses. Gemini 2.5 Pro was introduced as the most advanced Gemini model, featuring reasoning, coding capabilities, and the new Deep Think mode for complex tasks. Both 2.5 Pro and Flash support native audio output and improved security.... On June 17, 2025, Google announced general availability for 2.5 Pro and Flash. They also introduced Gemini 2.5 Flash-Lite that same day, a model optimized for speed and cost-efficiency.... On November 18th, 2025, Google announced the release of 3.0 Pro and 3.0 Deep Think. These new models replace 2.5 Pro and Flash, and are the most powerful models available as of November 2025. On release 3.0 Pro outperformed major AI models in 19 out of 20 benchmarks tested, including surpassing OpenAI's GPT-5 Pro in Humanity's Last Exam, with an accuracy of 41% compared to OpenAI's 31.64%.... |3.0 Pro Image (Nano Banana Pro)|20 November 2025|Active|An improved version of Nano Banana which includes better text rendering and better real world knowledge.|... Gemini and Gemma models are decoder-only transformers, with modifications to allow efficient training and inference on TPUs. The 1.0 generation uses multi-query attention.\n\n**Technical specifications of Gemini models**... ## External links\n- Official website\n- Press release via *The Keyword*\n- White paper for 1.0 and 1.5\n- White paper for Gemini 3.0 Pro [1]\n\nCategories: - 2023 software\n- Chatbots\n- Google DeepMind\n- Google software\n- Large language models\n- Multimodal interaction\n- 2023 in artificial intelligence\n- Generative pre-trained transformers"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Training Dataset",
            "score": 1,
            "explanation": "The pre-training dataset was a large-scale, diverse collection of data encompassing a wide range of domains and modalities."
          },
          {
            "name": "Training Data Processing",
            "score": 1,
            "explanation": "Data filtering and preprocessing included techniques such as deduplication, honoring robots.txt, safety filtering in-line with Google's commitment to advancing AI safely and responsibly."
          },
          {
            "name": "Knowledge Count",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      },
      {
        "sectionId": "model-implementation-sustainability",
        "sectionSnippets": [
          {
            "snippetId": "model-implementation-sustainability-snippet-1",
            "url": "https://web.archive.org/web/20251118111103if_/https:/storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf",
            "snippet": "Gemini 3 Pro \nModel Card\nGemini 3 Pro - Model Card \n \n \n \nModel Cards are intended to provide essential information on Gemini models, including known \nlimitations, mitigation approaches, and safety performance. Model cards may be updated from \ntime-to-time; for example, to include updated evaluations as the model is improved or revised. See the \nGoogle DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous \nmodel cards did. We hope more information about the training dataset, distribution, and intended uses \nwill empower developers with deeper insights and help build more robust and responsible downstream \napplications. \n \nPublished / Model Release: November 2025... data with paired instructions and responses in addition to human preference and tool-use data. Gemini \n3 Pro is trained using reinforcement learning techniques that can leverage multi-step reasoning, \nproblem-solving and theorem-proving data.  \n \nThe training dataset also includes: publicly available datasets that are readily downloadable; data \nobtained by crawlers; licensed data obtained via commercial licensing agreements; user data (i.e., data... synthetic data.   \n \nTraining Data Processing: Data filtering and preprocessing included techniques such as deduplication, \nhonoring robots.txt, safety filtering in-line with Google's commitment to advancing AI safely and \nresponsibly, and quality filtering to mitigate risks and improve training data reliability. Once data is \ncollected, it is cleaned and preprocessed to make it suitable for training. This process involves, on a... growing complexity of large foundation models. Training can be distributed across multiple TPU devices \nfor faster and more efficient processing. \n \nThe efficiencies gained through the use of TPUs are aligned with Google's commitment to operate \nsustainably. \n \nSoftware: Training was done using JAX and ML Pathways. \n \n \n2... Distribution \n \nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; \nrespective documentation shared in line: \n \n●​\nGemini App \n●​\nGoogle Cloud / Vertex AI \n●​\nGoogle AI Studio \n●​\nGemini API \n●​\nGoogle AI Mode \n●​... Google Antigravity \n \nOur models are available to downstream providers via an application program interface (API) and \nsubject to relevant terms of use. There is no required hardware or software to use the model. For \nAI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google \nCloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini \nAPI in Vertex AI quickstart.  \n \n \n \n3... Intended Usage and Limitations \n \nBenefit and Intended Usage: Gemini 3 Pro is our most intelligent and adaptive model yet, capable of \nhelping with real-world complexity, solving problems that require enhanced reasoning and intelligence, \ncreativity, strategic planning and making improvements step-by-step. It is particularly well-suited for... Evaluation Approach: Gemini 3 Pro was developed in partnership with internal safety, security, and \nresponsibility teams. A range of evaluations and red teaming activities were conducted to help improve \nthe model and inform decision-making. These evaluations and activities align with Google's AI Principles \nand responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use... Policy and the Gemini API Additional Terms of Service).    \n \nEvaluation types included but were not limited to:  \n \n●​\nTraining/Development Evaluations including automated and human evaluations carried out \ncontinuously throughout and after the model’s training, to monitor its progress and \nperformance; \n●​\nHuman Red Teaming conducted by specialist teams who sit outside of the model development... team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the \nmodel adheres to safety policies and desired outcomes;  \n●​\nAutomated Red Teaming to dynamically evaluate Gemini for safety and security \nconsiderations at scale, complementing human red teaming and static evaluations; \n●​\nEthics & Safety Reviews were conducted ahead of the model’s release... Training and Development Evaluation Results:  Results for some of the internal safety evaluations \nconducted during the development phase are listed below. The evaluation results are for automated \nevaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage \nincrease or decrease in performance compared to the indicated model, as described below. Overall,... standard of results. The performance results reported below are computed with improved evaluations \nand thus are not directly comparable with performance results found in previous Gemini model cards. \n \nWe expect variation in our automated safety evaluations results, which is why we review flagged content \nto check for egregious or dangerous material. Our manual review confirmed losses were... concerns. \n \nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and \ndeployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations \ninclude, but are not limited to:  \n \n●​\ndataset filtering;  \n●​"
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-2",
            "url": "https://ai.google.dev/gemini-api/docs/models",
            "snippet": "OUR MOST INTELLIGENT MODEL\n\n## Gemini 3 Pro\n\nThe best model in the world for multimodal understanding, and our most powerful agentic and vibe-coding model yet, delivering richer visuals and deeper interactivity, all built on a foundation of state-of-the-art reasoning.\n\n### Expand to learn more\n\n#### Model details... ### Gemini 3 Pro Preview\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-3-pro-preview`|\n|Supported data types|Text, Image, Video, Audio, and PDF Text|\n|Token limits [*]|1,048,576 65,536|\n|Capabilities|Not supported Supported Supported Supported Supported Supported Not supported Not supported Not supported Supported Supported Supported Supported|\n|Versions||\n|Latest update|November 2025|\n|Knowledge cutoff|January 2025|... ### Gemini 3 Pro Image Preview\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-3-pro-image-preview`|\n|Supported data types|Image and Text Image and Text|\n|Token limits [*]|65,536 32,768|\n|Capabilities|Not supported Supported Not supported Not supported Not supported Not supported Not supported Supported Not supported Supported Supported Supported Not supported|\n|Versions||\n|Latest update|November 2025|\n|Knowledge cutoff|January 2025|\nOUR ADVANCED THINKING MODEL... ### Gemini 2.5 Pro\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.5-pro`|\n|Supported data types|Audio, images, video, text, and PDF Text|\n|Token limits [*]|1,048,576 65,536|\n|Capabilities|Not supported Supported Supported Supported Supported Supported Supported Not supported Not supported Supported Supported Supported Supported|\n|Versions||\n|Latest update|June 2025|\n|Knowledge cutoff|January 2025|... ### Gemini 2.5 Pro TTS\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.5-pro-preview-tts`|\n|Supported data types|Text Audio|\n|Token limits [*]|8,192 16,384|\n|Capabilities|Supported Not Supported Not supported Not supported Not Supported Not supported Not supported Not supported Not supported Not supported Not supported Not supported Not supported|\n|Versions||\n|Latest update|May 2025|\nFAST AND INTELLIGENT... ### Gemini 2.5 Flash Preview\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.5-flash-preview-09-2025`|\n|Supported data types|Text, images, video, audio Text|\n|Token limits [*]|1,048,576 65,536|\n|Capabilities|Not supported Supported Supported Supported Supported Supported Not supported Not supported Not supported Supported Supported Supported Supported|\n|Versions||\n|Latest update|September 2025|\n|Knowledge cutoff|January 2025|... |Versions|gemini-live-2.5-flash-preview will be deprecated on December 09, 2025|\n|Latest update|September 2025|\n|Knowledge cutoff|January 2025|... ## Gemini 2.5 Flash-Lite\n\nOur fastest flash model optimized for cost-efficiency and high throughput.\n\n### Expand to learn more\n\n#### Model details... ### Gemini 2.0 Flash\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.0-flash`|\n|Supported data types|Audio, images, video, and text Text|\n|Token limits [*]|1,048,576 8,192|\n|Capabilities|Not supported Supported Supported Supported Not supported Supported Supported Not supported Supported Supported Supported Experimental Not supported|\n|Versions||\n|Latest update|February 2025|\n|Knowledge cutoff|August 2024|... ### Expand to learn more\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.... ## Model version name patterns\n\nGemini models are available in either\n\n*stable*, *preview*, *latest*, or\n\n*experimental* versions.\n\n### Stable\n\nPoints to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nFor example:\n\n`gemini-2.5-flash`.... ### Preview\n\nPoints to a preview model which may be used for production. Preview models will typically have billing enabled, might come with more restrictive rate limits and will be deprecated with at least 2 weeks notice.\n\nFor example:\n\n`gemini-2.5-flash-preview-09-2025`.... ### Experimental\n\nPoints to an experimental model which will typically be not be suitable for production use and come with more restrictive rate limits. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nExperimental models are not stable and availability of model endpoints is subject to change.\n\n## Model deprecations\n\nFor information about model deprecations, visit the Gemini deprecations page."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-3",
            "url": "https://huggingface.co/datasets/multimodalart/google-gemini-3-pro-pre-release-model-card",
            "snippet": "# Gemini 3 Pro - Model Card\n\n*Model Cards* *are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from*\n\n*time-to-time; for example, to include updated evaluations as the model is improved or revised. See the* *Google DeepMind site* *for a comprehensive list of model cards.*... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\n*Published / Model Release: November 2025*... ## Model Data\n\n**Training Dataset:** The pre-training dataset was a large-scale, diverse collection of data encompassing a wide range of domains and modalities, which included publicly-available web-documents, text, code (various programming languages), images, audio (including speech and other audio types) and video.\n\nThe post-training dataset consisted of vetted instruction tuning data and was a collection of multimodal data with paired instructions and responses in addition to human preference and tool-use data. Gemini 3 Pro is trained using reinforcement learning techniques that can leverage multi-step reasoning, problem-solving and theorem-proving data.... ## Implementation and Sustainability\n\n**Hardware:** Gemini 3 Pro was trained using Google's Tensor Processing Units (TPUs). TPUs are speciﬁcally designed to handle the massive computations involved in training LLMs and can speed up training considerably compared to CPUs. TPUs often come with large amounts of high-bandwidth memory, allowing for the handling of large models and batch sizes during training, which can lead to better model quality. TPU Pods (large clusters of TPUs) also provide a scalable solution for handling the growing complexity of large foundation models. Training can be distributed across multiple TPU devices for faster and more eﬃcient processing.... The eﬃciencies gained through the use of TPUs are aligned with Google's commitment to operate sustainably.\n\n**Software:** Training was done using JAX and ML Pathways.... ## Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\nOur models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... ## Intended Usage and Limitations\n\n**Beneﬁt and Intended Usage:** Gemini 3 Pro is our most intelligent and adaptive model yet, capable of helping with real-world complexity, solving problems that require enhanced reasoning and intelligence, creativity, strategic planning and making improvements step-by-step. It is particularly well-suited for applications that require: agentic performance, advanced coding, long context and/or multimodal understanding, and/or algorithmic development.... ## Ethics and Content Safety\n\n**Evaluation Approach:** Gemini 3 Pro was developed in partnership with internal safety, security, and responsibility teams. A range of evaluations and red teaming activities were conducted to help improve the model and inform decision-making. These evaluations and activities align with Google's AI Principles and responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use Policy and the Gemini API Additional Terms of Service).... Evaluation types included but were not limited to:\n\n**Training/Development Evaluations**including automated and human evaluations carried out continuously throughout and after the model's training, to monitor its progress and performance; **Human Red Teaming**conducted by specialist teams who sit outside of the model development team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the model adheres to safety policies and desired outcomes; **Automated Red Teaming**to dynamically evaluate Gemini for safety and security considerations at scale, complementing human red teaming and static evaluations; **Ethics & Safety Reviews**were conducted ahead of the model's release... **Training and Development Evaluation Results:** Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustiﬁed refusals low. We mark improvements in green and regressions in red.... **Human Red Teaming Results:** We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level ﬁndings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisﬁed required launch thresholds, which were developed by expert teams to protect children online and meet Google's commitments to child safety across our models and Google products."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-4",
            "url": "https://en.wikipedia.org/wiki/Gemini_(language_model)",
            "snippet": "At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2.... It was made available only in English. Touted as Google's \"largest and most capable AI model\" and designed to emulate human behavior, the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs), and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.... Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test, obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13, while Gemini Nano will be made available to Android developers as well.... Hassabis further revealed that DeepMind was exploring how Gemini could be \"combined with robotics to physically interact with the world\". In accordance with an executive order signed by U.S. President Joe Biden in October, Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly, the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November.... On December 11, 2024, Google announced Gemini 2.0 Flash Experimental, a significant update to its Gemini AI model. This iteration boasts improved speed and performance over its predecessor, Gemini 1.5 Flash. Key features include a Multimodal Live API for real-time audio and video interactions, enhanced spatial understanding, native image and controllable text-to-speech generation (with watermarking), and integrated tool use, including Google Search.... It also introduces improved agentic capabilities, a new Google Gen AI SDK, and \"Jules,\" an experimental AI coding agent for GitHub. Additionally, Google Colab is integrating Gemini 2.0 to generate data science notebooks from natural language. Gemini 2.0 was available through the Gemini chat interface for all users as \"Gemini 2.0 Flash experimental\".... On March 12, 2025, Google also announced Gemini Robotics, a vision-language-action model based on the Gemini 2.0 family of models.\n\nThe next day, Google announced that Gemini in Android Studio would be able to understand simple UI mockups and transform them into working Jetpack Compose code.... Gemini 2.5 Pro Experimental was released on March 25, 2025, described by Google as its most intelligent AI model yet, featuring enhanced reasoning and coding capabilities, and a \"thinking model\" capable of reasoning through steps before responding, using techniques like chain-of-thought prompting, whilst maintaining native multimodality and launching with a 1 million token context window.... At Google I/O 2025, Google announced significant updates to its Gemini core models. Gemini 2.5 Flash became the default model, delivering faster responses. Gemini 2.5 Pro was introduced as the most advanced Gemini model, featuring reasoning, coding capabilities, and the new Deep Think mode for complex tasks. Both 2.5 Pro and Flash support native audio output and improved security.... On November 18th, 2025, Google announced the release of 3.0 Pro and 3.0 Deep Think. These new models replace 2.5 Pro and Flash, and are the most powerful models available as of November 2025. On release 3.0 Pro outperformed major AI models in 19 out of 20 benchmarks tested, including surpassing OpenAI's GPT-5 Pro in Humanity's Last Exam, with an accuracy of 41% compared to OpenAI's 31.64%.... ### Model versions\n\nThe following table lists the main model versions of Gemini, describing the significant changes included with each version:... |3.0 Pro Image (Nano Banana Pro)|20 November 2025|Active|An improved version of Nano Banana which includes better text rendering and better real world knowledge.|... Gemini and Gemma models are decoder-only transformers, with modifications to allow efficient training and inference on TPUs. The 1.0 generation uses multi-query attention.\n\n**Technical specifications of Gemini models**... ## External links\n- Official website\n- Press release via *The Keyword*\n- White paper for 1.0 and 1.5\n- White paper for Gemini 3.0 Pro [1]\n\nCategories: - 2023 software\n- Chatbots\n- Google DeepMind\n- Google software\n- Large language models\n- Multimodal interaction\n- 2023 in artificial intelligence\n- Generative pre-trained transformers"
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-5",
            "url": "https://www.studocu.com/en-gb/document/university-of-east-london/financial-accounting/gemini-3-pro-model-card-insights-safety-evaluations-nov-2025/146206142",
            "snippet": "Key Concepts\n\n- Multimodal Capabilities: Gemini 3 Pro processes text, images, audio, and video, enhancing its versatility in handling diverse data types.\n\n- Sparse MoE Architecture: This model employs a mixture of experts approach, optimizing computational efficiency by activating only relevant parameters.\n\n- Training Dataset: A comprehensive dataset includes various modalities and is processed to ensure quality and safety.... - Evaluation Methods: The model undergoes rigorous testing across benchmarks to assess its reasoning and multimodal performance.\n\n- Safety Policies: Strict guidelines are in place to prevent harmful content generation, ensuring responsible AI use.\n\n- Intended Applications: Designed for complex problem-solving, the model excels in areas requiring advanced reasoning and creativity.\n\n## Preview text\n\nModel card published: November, 2025... ## Gemini 3 Pro\n\n## Model Card\n\n### Gemini 3 Pro - Model Card\n\nModel Cards are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from time-to-time; for example, to include updated evaluations as the model is improved or revised. See the Google DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\nPublished / Model Release: November 2025... #### Model Information\n\nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of highly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from different information sources, including text, audio, images, video, and entire code repositories.... Model dependencies: This model is not a modification or a fine-tune of a prior model.\n\nInputs: Text strings (e., a question, a prompt, document(s) to be summarized), images, audio, and video files, with a token context window of up to 1M.\n\nOutputs: Text, with a 64K token output.... Architecture: Gemini 3 Pro is a sparse mixture-of-experts (MoE) (Clark et al., 2022; Du et al., 2021; Fedus et al., 2021; Jiang et al., 2024, Lepikhin et al., 2020; Riquelme et al., 2021; Roller et al., 2021; Shazeer et al., 2017 ) transformer-based model (Vaswani et al., 2017) with native multimodal support for text, vision, and audio inputs.... #### Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\n● Gemini App ● Google Cloud / Vertex AI ● Google AI Studio ● Gemini API ● Google AI Mode ● Google Antigravity... Our models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... #### Evaluation\n\nApproach: Gemini 3 Pro was evaluated across a range of benchmarks, including reasoning, multimodal capabilities, agentic tool use, multi-lingual performance, and long-context. Additional benchmarks and details on approach, results and their methodologies can be found at: deepmind/models/evals/gemini-3-pro.... Training and Development Evaluation Results: Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2 Pro across both safety and tone, while keeping unjustified refusals low. We mark improvements in green and regressions in red.... Evaluation 1 Description Gemini 3 Pro vs. Gemini 2 Pro\n\nText to Text Safety Automated content safety evaluation measuring safety policies -10%\n\nMultilingual Safety Automated safety policy evaluation across multiple languages +0% (non-egregious)\n\nImage to Text Safety Automated content safety evaluation measuring safety policies +3% (non-egregious)... Human Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level findings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams to protect children online and meet Google’s commitments to child safety across our models and Google products."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Hardware Used During Training & Inference",
            "score": 1,
            "explanation": "Gemini 3 Pro was trained using Google's Tensor Processing Units (TPUs)."
          },
          {
            "name": "Software Frameworks & Tooling",
            "score": 1,
            "explanation": "Training was done using JAX and ML Pathways."
          },
          {
            "name": "Energy Use/ Sustainability Metrics",
            "score": 1,
            "explanation": "The efficiencies gained through the use of TPUs are aligned with Google's commitment to operate sustainably."
          }
        ]
      },
      {
        "sectionId": "intended-use",
        "sectionSnippets": [
          {
            "snippetId": "intended-use-snippet-1",
            "url": "https://ai.google.dev/gemini-api/docs/models",
            "snippet": "OUR MOST INTELLIGENT MODEL\n\n## Gemini 3 Pro\n\nThe best model in the world for multimodal understanding, and our most powerful agentic and vibe-coding model yet, delivering richer visuals and deeper interactivity, all built on a foundation of state-of-the-art reasoning.\n\n### Expand to learn more\n\n#### Model details... ### Gemini 3 Pro Preview\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-3-pro-preview`|\n|Supported data types|Text, Image, Video, Audio, and PDF Text|\n|Token limits [*]|1,048,576 65,536|\n|Capabilities|Not supported Supported Supported Supported Supported Supported Not supported Not supported Not supported Supported Supported Supported Supported|\n|Versions||\n|Latest update|November 2025|\n|Knowledge cutoff|January 2025|... ### Gemini 3 Pro Image Preview\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-3-pro-image-preview`|\n|Supported data types|Image and Text Image and Text|\n|Token limits [*]|65,536 32,768|\n|Capabilities|Not supported Supported Not supported Not supported Not supported Not supported Not supported Supported Not supported Supported Supported Supported Not supported|\n|Versions||\n|Latest update|November 2025|\n|Knowledge cutoff|January 2025|\nOUR ADVANCED THINKING MODEL... ### Gemini 2.5 Pro\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.5-pro`|\n|Supported data types|Audio, images, video, text, and PDF Text|\n|Token limits [*]|1,048,576 65,536|\n|Capabilities|Not supported Supported Supported Supported Supported Supported Supported Not supported Not supported Supported Supported Supported Supported|\n|Versions||\n|Latest update|June 2025|\n|Knowledge cutoff|January 2025|... ### Gemini 2.5 Pro TTS\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.5-pro-preview-tts`|\n|Supported data types|Text Audio|\n|Token limits [*]|8,192 16,384|\n|Capabilities|Supported Not Supported Not supported Not supported Not Supported Not supported Not supported Not supported Not supported Not supported Not supported Not supported Not supported|\n|Versions||\n|Latest update|May 2025|\nFAST AND INTELLIGENT... ## Gemini 2.5 Flash\n\nOur best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases.\n\n### Expand to learn more\n\n#### Model details... ## Gemini 2.5 Flash-Lite\n\nOur fastest flash model optimized for cost-efficiency and high throughput.\n\n### Expand to learn more\n\n#### Model details... ### Gemini 2.0 Flash\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.0-flash`|\n|Supported data types|Audio, images, video, and text Text|\n|Token limits [*]|1,048,576 8,192|\n|Capabilities|Not supported Supported Supported Supported Not supported Supported Supported Not supported Supported Supported Supported Experimental Not supported|\n|Versions||\n|Latest update|February 2025|\n|Knowledge cutoff|August 2024|... ### Gemini 2.0 Flash Image\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.0-flash-preview-image-generation`|\n|Supported data types|Audio, images, video, and text Text and images|\n|Token limits [*]|32,768 8,192|\n|Capabilities|Not supported Supported Supported Not Supported Not supported Not supported Not supported Supported Not Supported Not Supported Supported Not Supported Not supported|... ### Expand to learn more\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.... ## Model version name patterns\n\nGemini models are available in either\n\n*stable*, *preview*, *latest*, or\n\n*experimental* versions.\n\n### Stable\n\nPoints to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nFor example:\n\n`gemini-2.5-flash`.... ### Preview\n\nPoints to a preview model which may be used for production. Preview models will typically have billing enabled, might come with more restrictive rate limits and will be deprecated with at least 2 weeks notice.\n\nFor example:\n\n`gemini-2.5-flash-preview-09-2025`.... ### Experimental\n\nPoints to an experimental model which will typically be not be suitable for production use and come with more restrictive rate limits. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nExperimental models are not stable and availability of model endpoints is subject to change.\n\n## Model deprecations\n\nFor information about model deprecations, visit the Gemini deprecations page."
          },
          {
            "snippetId": "intended-use-snippet-2",
            "url": "https://web.archive.org/web/20251118111103if_/https:/storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf",
            "snippet": "Gemini 3 Pro \nModel Card\nGemini 3 Pro - Model Card \n \n \n \nModel Cards are intended to provide essential information on Gemini models, including known \nlimitations, mitigation approaches, and safety performance. Model cards may be updated from \ntime-to-time; for example, to include updated evaluations as the model is improved or revised. See the \nGoogle DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous \nmodel cards did. We hope more information about the training dataset, distribution, and intended uses \nwill empower developers with deeper insights and help build more robust and responsible downstream \napplications. \n \nPublished / Model Release: November 2025... Model Information \n \nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of \nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced \nmodel for complex tasks, and can comprehend vast datasets, challenging problems from different \ninformation sources, including text, audio, images, video, and entire code repositories.... Model dependencies: This model is not a modification or a fine-tune of a prior model.   \n \nInputs: Text strings (e.g., a question, a prompt, document(s) to be summarized), images, audio, and \nvideo files, with a token context window of up to 1M.  \n \nOutputs:  Text, with a 64K token output.... collected from users of Google products and services to train AI models, along with user interactions \nwith the model) in accordance with Google’s relevant terms of service, privacy policy, service-specific \npolicies, and pursuant to user controls, where appropriate; other datasets that Google acquires or \ngenerates in the course of its business operations, or directly from its workforce; and AI-generated... case-by-case basis, filtering irrelevant or harmful content, text, and other modalities, including filtering \ncontent that is pornographic, violent, or violative of child sexual abuse material (CSAM) laws. \n \n \nImplementation and Sustainability \n \nHardware: Gemini 3 Pro was trained using Google’s Tensor Processing Units (TPUs). TPUs are... Distribution \n \nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; \nrespective documentation shared in line: \n \n●​\nGemini App \n●​\nGoogle Cloud / Vertex AI \n●​\nGoogle AI Studio \n●​\nGemini API \n●​\nGoogle AI Mode \n●​... Google Antigravity \n \nOur models are available to downstream providers via an application program interface (API) and \nsubject to relevant terms of use. There is no required hardware or software to use the model. For \nAI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google \nCloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini \nAPI in Vertex AI quickstart.  \n \n \n \n3... Intended Usage and Limitations \n \nBenefit and Intended Usage: Gemini 3 Pro is our most intelligent and adaptive model yet, capable of \nhelping with real-world complexity, solving problems that require enhanced reasoning and intelligence, \ncreativity, strategic planning and making improvements step-by-step. It is particularly well-suited for... applications that require:  agentic performance, advanced coding, long context and/or multimodal \nunderstanding, and/or algorithmic development. \n \nKnown Limitations: Gemini 3 Pro may exhibit some of the general limitations of foundation models, \nsuch as hallucinations. There may also be occasional slowness or timeout issues. The knowledge cutoff... date for Gemini 3 Pro was January 2025.  \n \nAcceptable Usage: Google’s Generative AI Prohibited Use Policy applies to uses of the model in \naccordance with the applicable terms of service. Additionally, the model should not be integrated into \ncertain systems (also found in Google’s Generative AI Prohibited Use Policy), including those that: (1)... Evaluation Approach: Gemini 3 Pro was developed in partnership with internal safety, security, and \nresponsibility teams. A range of evaluations and red teaming activities were conducted to help improve \nthe model and inform decision-making. These evaluations and activities align with Google's AI Principles \nand responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use... standard of results. The performance results reported below are computed with improved evaluations \nand thus are not directly comparable with performance results found in previous Gemini model cards. \n \nWe expect variation in our automated safety evaluations results, which is why we review flagged content \nto check for egregious or dangerous material. Our manual review confirmed losses were... Instrumental \nReasoning​\nLevels 1 + 2 \nCCL not \nreached \n \nMore details can be found in the Gemini 3 Pro Frontier Safety Framework Report. \n \n \n \n8"
          },
          {
            "snippetId": "intended-use-snippet-3",
            "url": "https://huggingface.co/datasets/multimodalart/google-gemini-3-pro-pre-release-model-card",
            "snippet": "# Gemini 3 Pro Model Card\n\n### ⚠️ This is a mirror to the pre-release model card taht was officially published by Google @ https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf and subsequentially yanked. It will get outdated by the actual release version in a few hours (date of publication Nov 18, 13:16 CET)\n\n*Model card published: November, 2025*... # Gemini 3 Pro - Model Card\n\n*Model Cards* *are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from*\n\n*time-to-time; for example, to include updated evaluations as the model is improved or revised. See the* *Google DeepMind site* *for a comprehensive list of model cards.*... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\n*Published / Model Release: November 2025*... ## Model Information\n\n**Description**: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of\n\nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google's most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from diﬀerent information sources, including text, audio, images, video, and entire code repositories.... **Model dependencies:** This model is not a modiﬁcation or a ﬁne-tune of a prior model.\n\n**Inputs:** Text strings (e.g., a question, a prompt, document(s) to be summarized), images, audio, and video ﬁles, with a token context window of up to 1M.... **Outputs**: Text, with a 64K token output.\n\n**Architecture**: Gemini 3 Pro is a sparse mixture-of-experts (MoE) (Clark et al., 2022; Du et al., 2021; Fedus et al., 2021; Jiang et al., 2024, Lepikhin et al., 2020; Riquelme et al., 2021; Roller et al., 2021; Shazeer et al., 2017) transformer-based model (Vaswani et al., 2017) with native multimodal support for text, vision, and audio inputs.... ## Implementation and Sustainability\n\n**Hardware:** Gemini 3 Pro was trained using Google's Tensor Processing Units (TPUs). TPUs are speciﬁcally designed to handle the massive computations involved in training LLMs and can speed up training considerably compared to CPUs. TPUs often come with large amounts of high-bandwidth memory, allowing for the handling of large models and batch sizes during training, which can lead to better model quality. TPU Pods (large clusters of TPUs) also provide a scalable solution for handling the growing complexity of large foundation models. Training can be distributed across multiple TPU devices for faster and more eﬃcient processing.... ## Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\nOur models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... ## Intended Usage and Limitations\n\n**Beneﬁt and Intended Usage:** Gemini 3 Pro is our most intelligent and adaptive model yet, capable of helping with real-world complexity, solving problems that require enhanced reasoning and intelligence, creativity, strategic planning and making improvements step-by-step. It is particularly well-suited for applications that require: agentic performance, advanced coding, long context and/or multimodal understanding, and/or algorithmic development.... **Known Limitations:** Gemini 3 Pro may exhibit some of the general limitations of foundation models, such as hallucinations. There may also be occasional slowness or timeout issues. The knowledge cutoﬀ date for Gemini 3 Pro was January 2025.... **Training and Development Evaluation Results:** Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustiﬁed refusals low. We mark improvements in green and regressions in red."
          },
          {
            "snippetId": "intended-use-snippet-4",
            "url": "https://en.wikipedia.org/wiki/Gemini_(language_model)",
            "snippet": "In August 2023, *The Information* published a report outlining Google's roadmap for Gemini, revealing that the company was targeting a launch date of late 2023. According to the report, Google hoped to surpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation, allowing it to create contextual images and be adapted for a wider range of use cases.... ### Launch\n\nOn December 6, 2023, Pichai and Hassabis announced \"Gemini 1.0\" at a virtual press conference. It comprised three models: Gemini Ultra, designed for \"highly complex tasks\"; Gemini Pro, designed for \"a wide range of tasks\"; and Gemini Nano, designed for \"on-device tasks\".... At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2.... It was made available only in English. Touted as Google's \"largest and most capable AI model\" and designed to emulate human behavior, the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs), and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.... Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test, obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13, while Gemini Nano will be made available to Android developers as well.... Two updated Gemini models, Gemini-1.5-Pro-002 and Gemini-1.5-Flash-002, were released on September 24, 2024.... On January 30, 2025, Google released Gemini 2.0 Flash as the new default model, with Gemini 1.5 Flash still available for usage. This was followed by the release of Gemini 2.0 Pro on February 5, 2025. Additionally, Google released Gemini 2.0 Flash Thinking Experimental, which details the language model's thinking process when responding to prompts.... On March 12, 2025, Google also announced Gemini Robotics, a vision-language-action model based on the Gemini 2.0 family of models.\n\nThe next day, Google announced that Gemini in Android Studio would be able to understand simple UI mockups and transform them into working Jetpack Compose code.... Gemini 2.5 Pro Experimental was released on March 25, 2025, described by Google as its most intelligent AI model yet, featuring enhanced reasoning and coding capabilities, and a \"thinking model\" capable of reasoning through steps before responding, using techniques like chain-of-thought prompting, whilst maintaining native multimodality and launching with a 1 million token context window.... At Google I/O 2025, Google announced significant updates to its Gemini core models. Gemini 2.5 Flash became the default model, delivering faster responses. Gemini 2.5 Pro was introduced as the most advanced Gemini model, featuring reasoning, coding capabilities, and the new Deep Think mode for complex tasks. Both 2.5 Pro and Flash support native audio output and improved security.... On June 17, 2025, Google announced general availability for 2.5 Pro and Flash. They also introduced Gemini 2.5 Flash-Lite that same day, a model optimized for speed and cost-efficiency.... On November 18th, 2025, Google announced the release of 3.0 Pro and 3.0 Deep Think. These new models replace 2.5 Pro and Flash, and are the most powerful models available as of November 2025. On release 3.0 Pro outperformed major AI models in 19 out of 20 benchmarks tested, including surpassing OpenAI's GPT-5 Pro in Humanity's Last Exam, with an accuracy of 41% compared to OpenAI's 31.64%.... |3.0 Pro Image (Nano Banana Pro)|20 November 2025|Active|An improved version of Nano Banana which includes better text rendering and better real world knowledge.|... ## External links\n- Official website\n- Press release via *The Keyword*\n- White paper for 1.0 and 1.5\n- White paper for Gemini 3.0 Pro [1]\n\nCategories: - 2023 software\n- Chatbots\n- Google DeepMind\n- Google software\n- Large language models\n- Multimodal interaction\n- 2023 in artificial intelligence\n- Generative pre-trained transformers"
          },
          {
            "snippetId": "intended-use-snippet-5",
            "url": "https://www.studocu.com/en-gb/document/university-of-east-london/financial-accounting/gemini-3-pro-model-card-insights-safety-evaluations-nov-2025/146206142",
            "snippet": "Key Concepts\n\n- Multimodal Capabilities: Gemini 3 Pro processes text, images, audio, and video, enhancing its versatility in handling diverse data types.\n\n- Sparse MoE Architecture: This model employs a mixture of experts approach, optimizing computational efficiency by activating only relevant parameters.\n\n- Training Dataset: A comprehensive dataset includes various modalities and is processed to ensure quality and safety.... - Evaluation Methods: The model undergoes rigorous testing across benchmarks to assess its reasoning and multimodal performance.\n\n- Safety Policies: Strict guidelines are in place to prevent harmful content generation, ensuring responsible AI use.\n\n- Intended Applications: Designed for complex problem-solving, the model excels in areas requiring advanced reasoning and creativity.\n\n## Preview text\n\nModel card published: November, 2025... ## Gemini 3 Pro\n\n## Model Card\n\n### Gemini 3 Pro - Model Card\n\nModel Cards are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from time-to-time; for example, to include updated evaluations as the model is improved or revised. See the Google DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\nPublished / Model Release: November 2025... #### Model Information\n\nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of highly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from different information sources, including text, audio, images, video, and entire code repositories.... Model dependencies: This model is not a modification or a fine-tune of a prior model.\n\nInputs: Text strings (e., a question, a prompt, document(s) to be summarized), images, audio, and video files, with a token context window of up to 1M.\n\nOutputs: Text, with a 64K token output.... Architecture: Gemini 3 Pro is a sparse mixture-of-experts (MoE) (Clark et al., 2022; Du et al., 2021; Fedus et al., 2021; Jiang et al., 2024, Lepikhin et al., 2020; Riquelme et al., 2021; Roller et al., 2021; Shazeer et al., 2017 ) transformer-based model (Vaswani et al., 2017) with native multimodal support for text, vision, and audio inputs.... #### Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\n● Gemini App ● Google Cloud / Vertex AI ● Google AI Studio ● Gemini API ● Google AI Mode ● Google Antigravity... Our models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... #### Evaluation\n\nApproach: Gemini 3 Pro was evaluated across a range of benchmarks, including reasoning, multimodal capabilities, agentic tool use, multi-lingual performance, and long-context. Additional benchmarks and details on approach, results and their methodologies can be found at: deepmind/models/evals/gemini-3-pro.... Training and Development Evaluation Results: Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2 Pro across both safety and tone, while keeping unjustified refusals low. We mark improvements in green and regressions in red.... Evaluation 1 Description Gemini 3 Pro vs. Gemini 2 Pro\n\nText to Text Safety Automated content safety evaluation measuring safety policies -10%\n\nMultilingual Safety Automated safety policy evaluation across multiple languages +0% (non-egregious)\n\nImage to Text Safety Automated content safety evaluation measuring safety policies +3% (non-egregious)... Human Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level findings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams to protect children online and meet Google’s commitments to child safety across our models and Google products."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Primary intended uses",
            "score": 1,
            "explanation": "Gemini 3 Pro is capable of helping with real-world complexity, solving problems that require enhanced reasoning and intelligence, creativity, strategic planning and making improvements step-by-step."
          },
          {
            "name": "Primary intended users",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Out-of-scope use cases",
            "score": 1,
            "explanation": "the model should not be integrated into certain systems (also found in Google’s Generative AI Prohibited Use Policy)"
          }
        ]
      },
      {
        "sectionId": "critical-risk",
        "sectionSnippets": [
          {
            "snippetId": "critical-risk-snippet-1",
            "url": "https://www.youtube.com/watch?v=hf59-PhO3do",
            "snippet": "Chapters:\n00:00 - Gemini 3 Executive Summary\n01:37 - MoE Architecture Explained\n03:07 - 1 Million Token Capabilities\n03:49 - Synthetic Data and Training\n04:58 - Reasoning and Multimodal Benchmarks\n06:54 - Deep Think Ultra Tier\n08:01 - Vibe Coding Performance\n09:09 - Anti-Gravity Agent Platform\n10:35 - Safety Metrics and Regressions\n11:56 - Frontier Risk Analysis\n13:39 - Availability and Reliability Challenges... Let's talk training data. The card mentions a large scale diverse\n{ts:232} and uh natively multimodal collection. So, you have the usual suspects, public web data, licensed data, and some user\n{ts:240} data with, you know, the standard controls. But there was one thing that really... The entire platform is designed to let the agent plan and execute complex end to-end\n{ts:566} tasks on its own and critically validate its own work. Critically, it can run tests, check the\n{ts:572} output, see if the UI looks right in the browser. It's a closed loop.... What about the frontier risks? How is Google\n{ts:640} handling safety? They say it's their most secure model yet. They detail all the mitigations,\n{ts:644} data set filtering, RLHF, critic feedback, all the standard stuff. But we need to look closely at the internal\n{ts:650} safety metrics they shared.... It is. And that's the critical point for\n{ts:670} anyone looking at this data. The card addresses it directly. They claimed that when they did a manual review, these\n{ts:676} flagged instances were overwhelmingly either false positives from the automated system or what they call\n{ts:681} non-gregious issues.... But non-aggregious is a very subjective term. It almost sounds like the model\n{ts:686} got so much smarter that it's now outsmarting the automated safety tests. That's a great way to put it. It signals\n{ts:692} a real challenge in safety scaling. As the models get more complex, our automated safety tools are struggling to... {ts:698} keep up. That said, they did show improvements elsewhere. tone was up nearly 8%, multilingual safety was up a\n{ts:705} little. The takeaway is that this new architecture is just different and the old tools need to catch up.\n{ts:710} Okay, so let's move from the internal metrics to the really crude part, the Frontier Safety Framework analysis.... The\n{ts:717} headline here is that the card states clearly that Gemini 3 Pro did not reach any critical capability levels or CCLs.\n{ts:724} That's the bottom line. Yes, but let's dig into the details. Starting with CBRN chemical, biological, radiological, and... {ts:730} nuclear threats. The model hit uplift level one, but the CCL was not reached. Meaning, while it can give you accurate\n{ts:738} textbook level information, their testing showed it generally fails to provide novel or complete enough\n{ts:742} instructions to actually help a threat actor build something dangerous.... The info is out there, but the model can't\n{ts:748} synthesize it into a truly hazardous new recipe. Okay, what about cyber security?\n{ts:754} This feels like the most immediate risk given how good it is at coding. Cyber security is also uplift level one.\n{ts:760} So the CCL was not reached.... However, and this is a big however, the alert threshold was met.\n{ts:766} What does that mean? It means the model showed it has highly capable individual hacking skills. It\n{ts:771} solved 11 out of 12 of their V1 hard challenges. So it's very good at specific component level hacking tasks... {ts:780} which triggered the alert. But not the full CCL. Why not? because it failed to solve any of the 13\n{ts:785} endto-end V2 challenges. Those require planning, coordination, and long-term execution to pull off a full complex\n{ts:791} cyber attack.... {ts:809} challenges. So, it has some foundational planning abilities, but it's not demonstrating dangerous levels of\n{ts:814} self-motivated autonomy in these tests. It's an agent, but it's not a rogue agent.\n{ts:818} So, what does this all mean for developers today?... The capabilities are obviously huge, and the safety analysis,\n{ts:825} while needing a critical eye, seems to clear it for general release. It's rolling out now. Developers can get\n{ts:830} it in AI Studio, Vert.Ex AI, the CLI, and through that new anti-gravity platform."
          },
          {
            "snippetId": "critical-risk-snippet-2",
            "url": "https://web.archive.org/web/20251118111103if_/https:/storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf",
            "snippet": "Gemini 3 Pro \nModel Card\nGemini 3 Pro - Model Card \n \n \n \nModel Cards are intended to provide essential information on Gemini models, including known \nlimitations, mitigation approaches, and safety performance. Model cards may be updated from \ntime-to-time; for example, to include updated evaluations as the model is improved or revised. See the \nGoogle DeepMind site for a comprehensive list of model cards.... case-by-case basis, filtering irrelevant or harmful content, text, and other modalities, including filtering \ncontent that is pornographic, violent, or violative of child sexual abuse material (CSAM) laws. \n \n \nImplementation and Sustainability \n \nHardware: Gemini 3 Pro was trained using Google’s Tensor Processing Units (TPUs). TPUs are... engage in dangerous or illicit activities, or otherwise violate applicable laws or regulations, (2) \ncompromise the security of others’ or Google’s services, (3) engage in sexually explicit, violent, hateful, \nor harmful activities, (4) engage in misinformation, misrepresentation, or misleading activities. \n \n \nEthics and Content Safety... Evaluation Approach: Gemini 3 Pro was developed in partnership with internal safety, security, and \nresponsibility teams. A range of evaluations and red teaming activities were conducted to help improve \nthe model and inform decision-making. These evaluations and activities align with Google's AI Principles \nand responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use... team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the \nmodel adheres to safety policies and desired outcomes;  \n●​\nAutomated Red Teaming to dynamically evaluate Gemini for safety and security \nconsiderations at scale, complementing human red teaming and static evaluations; \n●​\nEthics & Safety Reviews were conducted ahead of the model’s release... 5\nIn addition, we perform testing following the guidelines in Google DeepMind’s Frontier Safety \nFramework (FSF). \n \nSafety Policies: Gemini’s safety policies aim to prevent our Generative AI models from generating \nharmful content, including:  \n \n1.​\nContent related to child sexual abuse material and exploitation \n2.​... Training and Development Evaluation Results:  Results for some of the internal safety evaluations \nconducted during the development phase are listed below. The evaluation results are for automated \nevaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage \nincrease or decrease in performance compared to the indicated model, as described below. Overall,... standard of results. The performance results reported below are computed with improved evaluations \nand thus are not directly comparable with performance results found in previous Gemini model cards. \n \nWe expect variation in our automated safety evaluations results, which is why we review flagged content \nto check for egregious or dangerous material. Our manual review confirmed losses were... overwhelmingly either a) false positives or b) not egregious. \n \nHuman Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of \nthe model development team. High-level findings are fed back to the model team. For child safety \nevaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams... to protect children online and meet Google’s commitments to child safety across our models and \nGoogle products. For content safety policies generally, including child safety, we saw similar or improved \nsafety performance compared to Gemini 2.5 Pro. Compared to 2.5 Pro, the scope of red teaming was \nexpanded to cover more potential issues outside of our strict policies, and found no egregious... concerns. \n \nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and \ndeployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations \ninclude, but are not limited to:  \n \n●​\ndataset filtering;  \n●​... conditional pre-training; \n●​\nsupervised fine-tuning; \n●​\nreinforcement learning from human and critic feedback; \n●​\nsafety policies and desiderata; \n●​\nproduct-level mitigations such as safety filtering. \n \nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2.5 Pro but \nstill an open research problem), and b) possible degradation in multi-turn conversations. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n7... Frontier Safety \n \nWe evaluated Gemini 3 Pro as outlined in our latest Frontier Safety Framework (September-2025), \nand found that it did not reach any critical capability levels as outlined in the table below:   \n \nDomain \nKey Results for Gemini 3 Pro  \nCCL  \nCCL \nreached?... Instrumental \nReasoning​\nLevels 1 + 2 \nCCL not \nreached \n \nMore details can be found in the Gemini 3 Pro Frontier Safety Framework Report. \n \n \n \n8"
          },
          {
            "snippetId": "critical-risk-snippet-3",
            "url": "https://huggingface.co/datasets/multimodalart/google-gemini-3-pro-pre-release-model-card",
            "snippet": "# Gemini 3 Pro - Model Card\n\n*Model Cards* *are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from*\n\n*time-to-time; for example, to include updated evaluations as the model is improved or revised. See the* *Google DeepMind site* *for a comprehensive list of model cards.*... The eﬃciencies gained through the use of TPUs are aligned with Google's commitment to operate sustainably.\n\n**Software:** Training was done using JAX and ML Pathways.... **Known Limitations:** Gemini 3 Pro may exhibit some of the general limitations of foundation models, such as hallucinations. There may also be occasional slowness or timeout issues. The knowledge cutoﬀ date for Gemini 3 Pro was January 2025.... ## Ethics and Content Safety\n\n**Evaluation Approach:** Gemini 3 Pro was developed in partnership with internal safety, security, and responsibility teams. A range of evaluations and red teaming activities were conducted to help improve the model and inform decision-making. These evaluations and activities align with Google's AI Principles and responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use Policy and the Gemini API Additional Terms of Service).... Evaluation types included but were not limited to:\n\n**Training/Development Evaluations**including automated and human evaluations carried out continuously throughout and after the model's training, to monitor its progress and performance; **Human Red Teaming**conducted by specialist teams who sit outside of the model development team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the model adheres to safety policies and desired outcomes; **Automated Red Teaming**to dynamically evaluate Gemini for safety and security considerations at scale, complementing human red teaming and static evaluations; **Ethics & Safety Reviews**were conducted ahead of the model's release... In addition, we perform testing following the guidelines in Google DeepMind's Frontier Safety Framework (FSF).\n\n**Safety Policies**: Gemini's safety policies aim to prevent our Generative AI models from generating harmful content, including:\n\n- Content related to child sexual abuse material and exploitation\n\n- Hate speech (e.g., dehumanizing members of protected groups)\n\n- Dangerous content (e.g., promoting suicide, or instructing in activities that could cause real-world harm)\n\n- Harassment (e.g., encouraging violence against people)\n\n- Sexually explicit content\n\n- Medical advice that runs contrary to scientiﬁc or medical consensus... **Training and Development Evaluation Results:** Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustiﬁed refusals low. We mark improvements in green and regressions in red.... **Human Red Teaming Results:** We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level ﬁndings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisﬁed required launch thresholds, which were developed by expert teams to protect children online and meet Google's commitments to child safety across our models and Google products.... For content safety policies generally, including child safety, we saw similar or improved safety performance compared to Gemini 2.5 Pro. Compared to 2.5 Pro, the scope of red teaming was expanded to cover more potential issues outside of our strict policies, and found no egregious concerns.\n\n**Risks and Mitigations:** Safety and responsibility was built into Gemini 3 Pro throughout the training and deployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations include, but are not limited to:... - dataset ﬁltering;\n\n- conditional pre-training;\n\n- supervised ﬁne-tuning;\n\n- reinforcement learning from human and critic feedback;\n\n- safety policies and desiderata;\n\n- product-level mitigations such as safety ﬁltering.\n\nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2.5 Pro but still an open research problem), and b) possible degradation in multi-turn conversations.... ## Frontier Safety\n\nWe evaluated Gemini 3 Pro as outlined in our latest Frontier Safety Framework (September-2025), and found that it did not reach any critical capability levels as outlined in the table below:... |Domain|Key Results for Gemini 3 Pro|CCL|CCL reached?|\n|--|--|--|--|\n|CBRN|The model provides accurate and occasionally actionable information but generally fails to oﬀer novel or suﬃciently complete and detailed instructions to signiﬁcantly enhance the capabilities of low to medium resourced threat actors.|Uplift Level 1|CCL not reached|"
          },
          {
            "snippetId": "critical-risk-snippet-4",
            "url": "https://www.studocu.com/en-gb/document/university-of-east-london/financial-accounting/gemini-3-pro-model-card-insights-safety-evaluations-nov-2025/146206142",
            "snippet": "- Evaluation Methods: The model undergoes rigorous testing across benchmarks to assess its reasoning and multimodal performance.\n\n- Safety Policies: Strict guidelines are in place to prevent harmful content generation, ensuring responsible AI use.\n\n- Intended Applications: Designed for complex problem-solving, the model excels in areas requiring advanced reasoning and creativity.\n\n## Preview text\n\nModel card published: November, 2025... ## Gemini 3 Pro\n\n## Model Card\n\n### Gemini 3 Pro - Model Card\n\nModel Cards are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from time-to-time; for example, to include updated evaluations as the model is improved or revised. See the Google DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\nPublished / Model Release: November 2025... #### Model Information\n\nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of highly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from different information sources, including text, audio, images, video, and entire code repositories.... #### Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\n● Gemini App ● Google Cloud / Vertex AI ● Google AI Studio ● Gemini API ● Google AI Mode ● Google Antigravity... Our models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... Results: Gemini 3 Pro significantly outperforms Gemini 2 Pro across a range of benchmarks requiring enhanced reasoning and multimodal capabilities. Results as of November, 2025 are listed below:\n\nSafety Policies: Gemini’s safety policies aim to prevent our Generative AI models from generating harmful content, including:... - Content related to child sexual abuse material and exploitation\n\n- Hate speech (e., dehumanizing members of protected groups)\n\n- Dangerous content (e., promoting suicide, or instructing in activities that could cause real-world harm)\n\n- Harassment (e., encouraging violence against people)\n\n- Sexually explicit content\n\n- Medical advice that runs contrary to scientific or medical consensus... Training and Development Evaluation Results: Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2 Pro across both safety and tone, while keeping unjustified refusals low. We mark improvements in green and regressions in red.... We expect variation in our automated safety evaluations results, which is why we review flagged content to check for egregious or dangerous material. Our manual review confirmed losses were overwhelmingly either a) false positives or b) not egregious.... Human Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level findings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams to protect children online and meet Google’s commitments to child safety across our models and Google products.... For content safety policies generally, including child safety, we saw similar or improved safety performance compared to Gemini 2 Pro. Compared to 2 Pro, the scope of red teaming was expanded to cover more potential issues outside of our strict policies, and found no egregious concerns.\n\nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and deployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations include, but are not limited to:... ● dataset filtering; ● conditional pre-training; ● supervised fine-tuning; ● reinforcement learning from human and critic feedback; ● safety policies and desiderata; ● product-level mitigations such as safety filtering.\n\nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2 Pro but still an open research problem), and b) possible degradation in multi-turn conversations."
          },
          {
            "snippetId": "critical-risk-snippet-5",
            "url": "https://en.wikipedia.org/wiki/Gemini_(language_model)",
            "snippet": "In August 2023, *The Information* published a report outlining Google's roadmap for Gemini, revealing that the company was targeting a launch date of late 2023. According to the report, Google hoped to surpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation, allowing it to create contextual images and be adapted for a wider range of use cases.... ### Launch\n\nOn December 6, 2023, Pichai and Hassabis announced \"Gemini 1.0\" at a virtual press conference. It comprised three models: Gemini Ultra, designed for \"highly complex tasks\"; Gemini Pro, designed for \"a wide range of tasks\"; and Gemini Nano, designed for \"on-device tasks\".... At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2.... It was made available only in English. Touted as Google's \"largest and most capable AI model\" and designed to emulate human behavior, the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs), and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.... Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test, obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13, while Gemini Nano will be made available to Android developers as well.... Hassabis further revealed that DeepMind was exploring how Gemini could be \"combined with robotics to physically interact with the world\". In accordance with an executive order signed by U.S. President Joe Biden in October, Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly, the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November.... On March 12, 2025, Google also announced Gemini Robotics, a vision-language-action model based on the Gemini 2.0 family of models.\n\nThe next day, Google announced that Gemini in Android Studio would be able to understand simple UI mockups and transform them into working Jetpack Compose code.... Gemini 2.5 Pro Experimental was released on March 25, 2025, described by Google as its most intelligent AI model yet, featuring enhanced reasoning and coding capabilities, and a \"thinking model\" capable of reasoning through steps before responding, using techniques like chain-of-thought prompting, whilst maintaining native multimodality and launching with a 1 million token context window.... At Google I/O 2025, Google announced significant updates to its Gemini core models. Gemini 2.5 Flash became the default model, delivering faster responses. Gemini 2.5 Pro was introduced as the most advanced Gemini model, featuring reasoning, coding capabilities, and the new Deep Think mode for complex tasks. Both 2.5 Pro and Flash support native audio output and improved security.... On November 18th, 2025, Google announced the release of 3.0 Pro and 3.0 Deep Think. These new models replace 2.5 Pro and Flash, and are the most powerful models available as of November 2025. On release 3.0 Pro outperformed major AI models in 19 out of 20 benchmarks tested, including surpassing OpenAI's GPT-5 Pro in Humanity's Last Exam, with an accuracy of 41% compared to OpenAI's 31.64%.... |3.0 Pro Image (Nano Banana Pro)|20 November 2025|Active|An improved version of Nano Banana which includes better text rendering and better real world knowledge.|... ## Nano Banana\n\nNano Banana (officially Gemini 2.5 Flash Image) is an image generation and editing model powered by generative artificial intelligence and developed by Google DeepMind, a subsidiary of Google. A text-to-image variant of the Gemini family of large language models, it was launched in August 2025 as a feature within the Gemini chatbot and other Google products.... ## External links\n- Official website\n- Press release via *The Keyword*\n- White paper for 1.0 and 1.5\n- White paper for Gemini 3.0 Pro [1]\n\nCategories: - 2023 software\n- Chatbots\n- Google DeepMind\n- Google software\n- Large language models\n- Multimodal interaction\n- 2023 in artificial intelligence\n- Generative pre-trained transformers"
          }
        ],
        "subsectionChecks": [
          {
            "name": "CBRN (Chemical, Biological, Radiological or Nuclear)",
            "score": 1,
            "explanation": "The model hit uplift level one, but the CCL was not reached."
          },
          {
            "name": "Cyber Risk",
            "score": 1,
            "explanation": "Cyber security is also uplift level one. So the CCL was not reached."
          },
          {
            "name": "Harmful Manipulation",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Child Safety Evaluations",
            "score": 1,
            "explanation": "For child safety evaluations, Gemini 3 Pro satisfied required launch thresholds."
          },
          {
            "name": "Privacy Risks",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      },
      {
        "sectionId": "safety-evaluation",
        "sectionSnippets": [
          {
            "snippetId": "safety-evaluation-snippet-1",
            "url": "https://web.archive.org/web/20251118111103if_/https:/storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf",
            "snippet": "case-by-case basis, filtering irrelevant or harmful content, text, and other modalities, including filtering \ncontent that is pornographic, violent, or violative of child sexual abuse material (CSAM) laws. \n \n \nImplementation and Sustainability \n \nHardware: Gemini 3 Pro was trained using Google’s Tensor Processing Units (TPUs). TPUs are... Evaluation Approach: Gemini 3 Pro was developed in partnership with internal safety, security, and \nresponsibility teams. A range of evaluations and red teaming activities were conducted to help improve \nthe model and inform decision-making. These evaluations and activities align with Google's AI Principles \nand responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use... team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the \nmodel adheres to safety policies and desired outcomes;  \n●​\nAutomated Red Teaming to dynamically evaluate Gemini for safety and security \nconsiderations at scale, complementing human red teaming and static evaluations; \n●​\nEthics & Safety Reviews were conducted ahead of the model’s release... 5\nIn addition, we perform testing following the guidelines in Google DeepMind’s Frontier Safety \nFramework (FSF). \n \nSafety Policies: Gemini’s safety policies aim to prevent our Generative AI models from generating \nharmful content, including:  \n \n1.​\nContent related to child sexual abuse material and exploitation \n2.​... Training and Development Evaluation Results:  Results for some of the internal safety evaluations \nconducted during the development phase are listed below. The evaluation results are for automated \nevaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage \nincrease or decrease in performance compared to the indicated model, as described below. Overall,... Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustified refusals \nlow. We mark improvements in green and regressions in red. \n \n \nEvaluation1 \nDescription \nGemini 3 Pro \nvs. Gemini 2.5 Pro \nText to Text Safety... Automated content safety evaluation \nmeasuring safety policies \n-10.4% \nMultilingual Safety  \nAutomated safety policy evaluation across \nmultiple languages \n+0.2% (non-egregious) \nImage to Text Safety \nAutomated content safety evaluation \nmeasuring safety policies \n+3.1% (non-egregious)... false positives and negatives, as well as update query sets to ensure balance and maintain a high \n2 For tone and instruction following, a positive percentage increase represents an improvement in the tone of the model on \nsensitive topics and the model’s ability to follow instructions while remaining safe compared to Gemini 2.5 Pro. We mark \nimprovements in green and regressions in red. \n1The ordering of evaluations in this table has changed from previous iterations of the 2.5 Flash-Lite model card in order to \nlist safety evaluations together and improve readability. The type of evaluations listed have remained the same.   \n \n6... standard of results. The performance results reported below are computed with improved evaluations \nand thus are not directly comparable with performance results found in previous Gemini model cards. \n \nWe expect variation in our automated safety evaluations results, which is why we review flagged content \nto check for egregious or dangerous material. Our manual review confirmed losses were... overwhelmingly either a) false positives or b) not egregious. \n \nHuman Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of \nthe model development team. High-level findings are fed back to the model team. For child safety \nevaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams... to protect children online and meet Google’s commitments to child safety across our models and \nGoogle products. For content safety policies generally, including child safety, we saw similar or improved \nsafety performance compared to Gemini 2.5 Pro. Compared to 2.5 Pro, the scope of red teaming was \nexpanded to cover more potential issues outside of our strict policies, and found no egregious... concerns. \n \nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and \ndeployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations \ninclude, but are not limited to:  \n \n●​\ndataset filtering;  \n●​... conditional pre-training; \n●​\nsupervised fine-tuning; \n●​\nreinforcement learning from human and critic feedback; \n●​\nsafety policies and desiderata; \n●​\nproduct-level mitigations such as safety filtering. \n \nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2.5 Pro but \nstill an open research problem), and b) possible degradation in multi-turn conversations. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n7"
          },
          {
            "snippetId": "safety-evaluation-snippet-2",
            "url": "https://huggingface.co/datasets/multimodalart/google-gemini-3-pro-pre-release-model-card",
            "snippet": "# Gemini 3 Pro - Model Card\n\n*Model Cards* *are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from*\n\n*time-to-time; for example, to include updated evaluations as the model is improved or revised. See the* *Google DeepMind site* *for a comprehensive list of model cards.*... ## Ethics and Content Safety\n\n**Evaluation Approach:** Gemini 3 Pro was developed in partnership with internal safety, security, and responsibility teams. A range of evaluations and red teaming activities were conducted to help improve the model and inform decision-making. These evaluations and activities align with Google's AI Principles and responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use Policy and the Gemini API Additional Terms of Service).... Evaluation types included but were not limited to:\n\n**Training/Development Evaluations**including automated and human evaluations carried out continuously throughout and after the model's training, to monitor its progress and performance; **Human Red Teaming**conducted by specialist teams who sit outside of the model development team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the model adheres to safety policies and desired outcomes; **Automated Red Teaming**to dynamically evaluate Gemini for safety and security considerations at scale, complementing human red teaming and static evaluations; **Ethics & Safety Reviews**were conducted ahead of the model's release... In addition, we perform testing following the guidelines in Google DeepMind's Frontier Safety Framework (FSF).\n\n**Safety Policies**: Gemini's safety policies aim to prevent our Generative AI models from generating harmful content, including:\n\n- Content related to child sexual abuse material and exploitation\n\n- Hate speech (e.g., dehumanizing members of protected groups)\n\n- Dangerous content (e.g., promoting suicide, or instructing in activities that could cause real-world harm)\n\n- Harassment (e.g., encouraging violence against people)\n\n- Sexually explicit content\n\n- Medical advice that runs contrary to scientiﬁc or medical consensus... **Training and Development Evaluation Results:** Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustiﬁed refusals low. We mark improvements in green and regressions in red.... |Evaluation 1|Description|Gemini 3 Pro vs. Gemini 2.5 Pro|\n|--|--|--|\n|Text to Text Safety|Automated content safety evaluation measuring safety policies|-10.4%|\n|Multilingual Safety|Automated safety policy evaluation across multiple languages|+0.2% (non-egregious)|... **Human Red Teaming Results:** We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level ﬁndings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisﬁed required launch thresholds, which were developed by expert teams to protect children online and meet Google's commitments to child safety across our models and Google products.... For content safety policies generally, including child safety, we saw similar or improved safety performance compared to Gemini 2.5 Pro. Compared to 2.5 Pro, the scope of red teaming was expanded to cover more potential issues outside of our strict policies, and found no egregious concerns.\n\n**Risks and Mitigations:** Safety and responsibility was built into Gemini 3 Pro throughout the training and deployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations include, but are not limited to:... - dataset ﬁltering;\n\n- conditional pre-training;\n\n- supervised ﬁne-tuning;\n\n- reinforcement learning from human and critic feedback;\n\n- safety policies and desiderata;\n\n- product-level mitigations such as safety ﬁltering.\n\nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2.5 Pro but still an open research problem), and b) possible degradation in multi-turn conversations.... ## Frontier Safety\n\nWe evaluated Gemini 3 Pro as outlined in our latest Frontier Safety Framework (September-2025), and found that it did not reach any critical capability levels as outlined in the table below:... |Cybersecurity|On key skills benchmark, v1 hard challenges: 11/12 challenge solved; v2 challenges: 0/13 solved end-to-end. Alert threshold met.|Uplift Level 1|CCL not reached|\n|Harmful Manipulation|Model manipulative eﬃcacy improves on non-generative AI baseline, but shows no signiﬁcant uplift versus prior models and did not near alert thresholds.|Level 1 (exploratory)|CCL not reached|"
          },
          {
            "snippetId": "safety-evaluation-snippet-3",
            "url": "https://www.studocu.com/en-gb/document/university-of-east-london/financial-accounting/gemini-3-pro-model-card-insights-safety-evaluations-nov-2025/146206142",
            "snippet": "- Evaluation Methods: The model undergoes rigorous testing across benchmarks to assess its reasoning and multimodal performance.\n\n- Safety Policies: Strict guidelines are in place to prevent harmful content generation, ensuring responsible AI use.\n\n- Intended Applications: Designed for complex problem-solving, the model excels in areas requiring advanced reasoning and creativity.\n\n## Preview text\n\nModel card published: November, 2025... ## Gemini 3 Pro\n\n## Model Card\n\n### Gemini 3 Pro - Model Card\n\nModel Cards are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from time-to-time; for example, to include updated evaluations as the model is improved or revised. See the Google DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\nPublished / Model Release: November 2025... #### Model Information\n\nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of highly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from different information sources, including text, audio, images, video, and entire code repositories.... #### Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\n● Gemini App ● Google Cloud / Vertex AI ● Google AI Studio ● Gemini API ● Google AI Mode ● Google Antigravity... #### Evaluation\n\nApproach: Gemini 3 Pro was evaluated across a range of benchmarks, including reasoning, multimodal capabilities, agentic tool use, multi-lingual performance, and long-context. Additional benchmarks and details on approach, results and their methodologies can be found at: deepmind/models/evals/gemini-3-pro.... Results: Gemini 3 Pro significantly outperforms Gemini 2 Pro across a range of benchmarks requiring enhanced reasoning and multimodal capabilities. Results as of November, 2025 are listed below:\n\nSafety Policies: Gemini’s safety policies aim to prevent our Generative AI models from generating harmful content, including:... Training and Development Evaluation Results: Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2 Pro across both safety and tone, while keeping unjustified refusals low. We mark improvements in green and regressions in red.... Evaluation 1 Description Gemini 3 Pro vs. Gemini 2 Pro\n\nText to Text Safety Automated content safety evaluation measuring safety policies -10%\n\nMultilingual Safety Automated safety policy evaluation across multiple languages +0% (non-egregious)\n\nImage to Text Safety Automated content safety evaluation measuring safety policies +3% (non-egregious)... Tone 2 Automated evaluation measuring objective tone of model refusal +7%\n\nUnjustified-refusals\n\nAutomated evaluation measuring model’s ability to respond to borderline prompts while remaining safe\n\n+3% (non-egregious)\n\nWe continue to improve our internal evaluations, including refining automated evaluations to reduce false positives and negatives, as well as update query sets to ensure balance and maintain a high standard of results. The performance results reported below are computed with improved evaluations and thus are not directly comparable with performance results found in previous Gemini model cards.... Human Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level findings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams to protect children online and meet Google’s commitments to child safety across our models and Google products.... For content safety policies generally, including child safety, we saw similar or improved safety performance compared to Gemini 2 Pro. Compared to 2 Pro, the scope of red teaming was expanded to cover more potential issues outside of our strict policies, and found no egregious concerns.\n\nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and deployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations include, but are not limited to:... ● dataset filtering; ● conditional pre-training; ● supervised fine-tuning; ● reinforcement learning from human and critic feedback; ● safety policies and desiderata; ● product-level mitigations such as safety filtering.\n\nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2 Pro but still an open research problem), and b) possible degradation in multi-turn conversations."
          },
          {
            "snippetId": "safety-evaluation-snippet-4",
            "url": "https://ai.google.dev/gemini-api/docs/safety-guidance",
            "snippet": "Generative artificial intelligence models are powerful tools, but they are not without their limitations. Their versatility and applicability can sometimes lead to unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing, and rigorous manual evaluation are essential to limit the risk of harm from such outputs.... The models provided by the Gemini API can be used for a wide variety of generative AI and natural language processing (NLP) applications. Use of these functions is only available through the Gemini API or the Google AI Studio web app. Your use of Gemini API is also subject to the Generative AI Prohibited Use Policy and the Gemini API terms of service.... While the Gemini API has been designed with Google's AI principles in mind, the onus is on developers to apply these models responsibly. To aid developers in creating safe, responsible applications, the Gemini API has some built-in content filtering as well as adjustable safety settings across 4 dimensions of harm. Refer to the safety settings guide to learn more.... This document is meant to introduce you to some safety risks that can arise when using LLMs, and recommend emerging safety design and development recommendations. (Note that laws and regulations may also impose restrictions, but such considerations are beyond the scope of this guide.)\n\nThe following steps are recommended when building applications with LLMs:\n\n- Understanding the safety risks of your application\n\n- Considering adjustments to mitigate safety risks\n\n- Performing safety testing appropriate to your use case\n\n- Soliciting feedback from users and monitoring usage\n\nThe adjustment and testing phases should be iterative until you reach performance appropriate for your application.... ## Understand the safety risks of your application\n\nIn this context, safety is being defined as the ability of an LLM to avoid causing harm to its users, for example, by generating toxic language or content that promotes stereotypes. The models available through the Gemini API have been designed with Google’s AI principles in mind and your use of it is subject to the Generative AI Prohibited Use Policy.... The API provides built-in safety filters to help address some common language model problems such as toxic language and hate speech, and striving for inclusiveness and avoidance of stereotypes. However, each application can pose a different set of risks to its users. So as the application owner, you are responsible for knowing your users and the potential harms your application may cause, and ensuring that your application uses LLMs safely and responsibly.... **Blocking unsafe inputs and filtering output before it is shown to the user.**In simple situations, blocklists can be used to identify and block unsafe words or phrases in prompts or responses, or require human reviewers to manually alter or block such content. **Using trained classifiers to label each prompt with potential harms or adversarial signals.**Different strategies can then be employed on how to handle the request based on the type of harm detected. For example, If the input is overtly adversarial or abusive in nature, it could be blocked and instead output a pre-scripted response.... ## Perform safety testing appropriate to your use case\n\nTesting is a key part of building robust and safe applications, but the extent, scope and strategies for testing will vary. For example, a just-for-fun haiku generator is likely to pose less severe risks than, say, an application designed for use by law firms to summarize legal documents and help draft contracts. But the haiku generator may be used by a wider variety of users which means the potential for adversarial attempts or even unintended harmful inputs can be greater.... **Safety benchmarking**involves designing safety metrics that reflect the ways your application could be unsafe in the context of how it is likely to get used, then testing how well your application performs on the metrics using evaluation datasets. It's good practice to think about the minimum acceptable levels of safety metrics before testing so that 1) you can evaluate the test results against those expectations and 2) you can gather the evaluation dataset based on the tests that evaluate the metrics you care about most.... composition of the data used for testing. For adversarial tests, select\n\ntest data that is most likely to elicit problematic output from\n\nthe model. This means probing the model's behavior for all the types of\n\nharms that are possible, including rare or unusual examples and\n\nedge-cases that are relevant to safety policies. It should also include\n\ndiversity in the different dimensions of a sentence such as structure,\n\nmeaning and length. You can refer to the Google's Responsible AI\n\npractices in\n\nfairness\n\nfor more details on what to consider when building a test dataset.... #### Advanced tips\n\n- Use automated testing instead of the traditional method of enlisting people in 'red teams' to try and break your application. In automated testing, the 'red team' is another language model that finds input text that elicit harmful outputs from the model being tested.\n\n- Adversarial testing is a method for systematically evaluating an ML model with the intent of learning how it behaves when provided with malicious or inadvertently harmful input:"
          },
          {
            "snippetId": "safety-evaluation-snippet-5",
            "url": "https://www.youtube.com/watch?v=-XTNBIVXRGA",
            "snippet": "## AI Papers Podcast Daily\n\n##### Nov 20, 2025 (0:13:04)\n\nThe **Gemini 3 Pro Model Card**, published by Google DeepMind in November 2025, introduces the latest generation of the Gemini series, describing it as Google’s most advanced, highly-capable, and natively multimodal reasoning model. Architecturally, Gemini 3 Pro is a sparse Mixture-of-Experts (MoE) transformer-based model, capable of understanding vast datasets and complex problems from various inputs, including text, images, audio, video, and code repositories, utilizing an extensive 1M token context window.... The model was trained using reinforcement learning techniques on a large, diverse dataset, and processed efficiently using Google’s specialized Tensor Processing Units (TPUs). Intended for applications requiring enhanced reasoning, intelligence, advanced coding, and agentic performance, Gemini 3 Pro significantly outperforms the previous Gemini 2.5 Pro model across a range of benchmarks. Development included continuous training evaluations, human red teaming by specialist teams, and adherence to safety policies designed to prevent the generation of harmful content, confirming that Gemini 3 Pro satisfied required launch thresholds for child safety and showed improved overall safety performance compared to its predecessor.... https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf... The card is also clear about the\n{ts:353} pre-processing, dduplication, and things like honoring robots.txt. And crucially, the safety filtering.\n{ts:360} They talk about strict processes to filter out harmful content pornographic violent CSAM violating material. The\n{ts:367} foundation has to be as clean as possible.... That kind of strategic capability leads\n{ts:553} us right into section four. Intended use and crucial limitations. With that much power, what is this model for?\n{ts:559} It's for applications that need its core strengths that enhance reasoning, intelligence, and strategic planning.\n{ts:565} The model card specifically calls out agentic performance, advanced coding, long context reasoning, and algorithmic... {ts:571} development. If you need a model that can think several steps ahead, this is the tool.\n{ts:575} But even with this huge leap, we have to be realistic. What are the known limitations that the model card flags?\n{ts:582} Well, the usual foundation model issues are still there, primarily hallucinations.... It's not perfect.\n{ts:587} Developers have to build in safeguards. They also note occasional slowness or timeout issues, which probably reflects\n{ts:593} the complexity of the MOI routing. And a really critical point for anyone who needs up-to-date information.\n{ts:599} Yes, the card clearly states the model's knowledge cutoff was January 2025.... Google is explicit that their prohibited use policy applies.\n{ts:624} Clear bans on dangerous or illicit activities, trying to compromise security, and very clearly spreading\n{ts:629} misinformation. That focus on responsibility brings us to our last section, ethics and frontier\n{ts:634} safety evaluation. This is arguably the most rigorous part of the whole model card.... {ts:638} The safety approach was really comprehensive, built with their safety teams, aligned with Google's AI\n{ts:644} principles. They used a two-prong strategy called red teaming. Right. Where you have experts\n{ts:649} deliberately trying to break the model to find weaknesses before it's released.... Exactly. Human red teaming had\n{ts:654} specialists testing for harms and the outcome was pretty encouraging. They saw safety performance similar to or better\n{ts:662} than 2.5 Pro. No huge new concerns. Then they also used automated red teaming to run massive stress tests and check for... This is the major safety finding. Gemini 3 Pro did not reach any critical\n{ts:705} capability levels or CCLs in those domains. While it can give you factual information, the assessment found it\n{ts:711} generally fails to provide novel or complete instructions that could really help a bad actor.... The main risks that\n{ts:717} are left are things like jailbreak vulnerabilities, though they're improved, and some possible degradation\n{ts:722} in really long multi-turn conversations. Okay, let's bring this all together. This deep dive into the Gemini 3 Pro\n{ts:730} model card shows a very clear, very intentional direction for these big foundation models.... All of it rigorously checked to make sure it's not\n{ts:754} crossing any critical risk thresholds. It's highly optimized for complex reasoning.\n{ts:758} So what does this all mean for you, the listener, as you watch this field evolve given the successful integration of\n{ts:764} multi-step reasoning and that theorem proving data data designed to teach logic?"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Refusals",
            "score": 1,
            "explanation": "Overall, Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustified refusals low."
          },
          {
            "name": "Disallowed Content Handling",
            "score": 1,
            "explanation": "Gemini's safety policies aim to prevent our Generative AI models from generating harmful content, including: Content related to child sexual abuse material and exploitation."
          },
          {
            "name": "Sycophancy",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Jailbreak",
            "score": 1,
            "explanation": "The main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2.5 Pro but still an open research problem)."
          },
          {
            "name": "Hallucinations",
            "score": 1,
            "explanation": "The model card specifically calls out agentic performance, advanced coding, long context reasoning, and algorithmic development... the usual foundation model issues are still there, primarily hallucinations."
          },
          {
            "name": "Deception Behaviors",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Fairness & Bias Evaluations (incl. BBQ)",
            "score": 1,
            "explanation": "Safety benchmarking involves designing safety metrics that reflect the ways your application could be unsafe in the context of how it is likely to get used."
          },
          {
            "name": "Adversarial Robustness",
            "score": 1,
            "explanation": "Automated testing instead of the traditional method of enlisting people in 'red teams' to try and break your application."
          },
          {
            "name": "Red Teaming Results",
            "score": 1,
            "explanation": "Human red teaming had specialists testing for harms and the outcome was pretty encouraging."
          }
        ]
      },
      {
        "sectionId": "risk-mitigations",
        "sectionSnippets": [
          {
            "snippetId": "risk-mitigations-snippet-1",
            "url": "https://www.youtube.com/watch?v=hf59-PhO3do",
            "snippet": "Chapters:\n00:00 - Gemini 3 Executive Summary\n01:37 - MoE Architecture Explained\n03:07 - 1 Million Token Capabilities\n03:49 - Synthetic Data and Training\n04:58 - Reasoning and Multimodal Benchmarks\n06:54 - Deep Think Ultra Tier\n08:01 - Vibe Coding Performance\n09:09 - Anti-Gravity Agent Platform\n10:35 - Safety Metrics and Regressions\n11:56 - Frontier Risk Analysis\n13:39 - Availability and Reliability Challenges... And the headline here really isn't just\n{ts:15} an incremental update. This is a a pretty decisive architectural leap, I think, toward what you could call\n{ts:22} egentic AGI. The model card confirms it's a sparse mixture of experts or MOE transformer,\n{ts:28} which is so critical.... It's a gentic. It's topping the webdev arena\n{ts:68} and this is the big one for me. It hits 76.2% on the SWE verified benchmark\n{ts:77} which is just wild. Finally, we have to mention the safety. This is the most comprehensive\n{ts:81} evaluation we've seen using the Frontier Safety Framework, the FSF, and it confirms that no critical capability... I mean, we're talking about feeding it entire data sets, huge\n{ts:200} complex documents, long audio files, video, and this is the critical part for developers, entire code repository,\n{ts:208} a whole repo, the whole thing. You can give it the full documentation for a complex library... Let's talk training data. The card mentions a large scale diverse\n{ts:232} and uh natively multimodal collection. So, you have the usual suspects, public web data, licensed data, and some user\n{ts:240} data with, you know, the standard controls. But there was one thing that really... The entire platform is designed to let the agent plan and execute complex end to-end\n{ts:566} tasks on its own and critically validate its own work. Critically, it can run tests, check the\n{ts:572} output, see if the UI looks right in the browser. It's a closed loop.... What about the frontier risks? How is Google\n{ts:640} handling safety? They say it's their most secure model yet. They detail all the mitigations,\n{ts:644} data set filtering, RLHF, critic feedback, all the standard stuff. But we need to look closely at the internal\n{ts:650} safety metrics they shared.... And there's one number that immediately jumps out. A regression. The automated\n{ts:657} evils showed a drop in textto text safety of minus 10.4% compared to 2.5 Pro. Now wait a minute.\n{ts:665} Any regression in safety seems like a massive red flag, doesn't it?... But non-aggregious is a very subjective term. It almost sounds like the model\n{ts:686} got so much smarter that it's now outsmarting the automated safety tests. That's a great way to put it. It signals\n{ts:692} a real challenge in safety scaling. As the models get more complex, our automated safety tools are struggling to... {ts:698} keep up. That said, they did show improvements elsewhere. tone was up nearly 8%, multilingual safety was up a\n{ts:705} little. The takeaway is that this new architecture is just different and the old tools need to catch up.\n{ts:710} Okay, so let's move from the internal metrics to the really crude part, the Frontier Safety Framework analysis.... The\n{ts:717} headline here is that the card states clearly that Gemini 3 Pro did not reach any critical capability levels or CCLs.\n{ts:724} That's the bottom line. Yes, but let's dig into the details. Starting with CBRN chemical, biological, radiological, and... {ts:809} challenges. So, it has some foundational planning abilities, but it's not demonstrating dangerous levels of\n{ts:814} self-motivated autonomy in these tests. It's an agent, but it's not a rogue agent.\n{ts:818} So, what does this all mean for developers today?... The capabilities are obviously huge, and the safety analysis,\n{ts:825} while needing a critical eye, seems to clear it for general release. It's rolling out now. Developers can get\n{ts:830} it in AI Studio, Vert.Ex AI, the CLI, and through that new anti-gravity platform.... {ts:869} problem. The goal is an agent that can operate autonomously for weeks, months, fixing code, managing systems. For that,"
          },
          {
            "snippetId": "risk-mitigations-snippet-2",
            "url": "https://web.archive.org/web/20251118111103if_/https:/storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf",
            "snippet": "Gemini 3 Pro \nModel Card\nGemini 3 Pro - Model Card \n \n \n \nModel Cards are intended to provide essential information on Gemini models, including known \nlimitations, mitigation approaches, and safety performance. Model cards may be updated from \ntime-to-time; for example, to include updated evaluations as the model is improved or revised. See the \nGoogle DeepMind site for a comprehensive list of model cards.... Distribution \n \nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; \nrespective documentation shared in line: \n \n●​\nGemini App \n●​\nGoogle Cloud / Vertex AI \n●​\nGoogle AI Studio \n●​\nGemini API \n●​\nGoogle AI Mode \n●​... Evaluation Approach: Gemini 3 Pro was developed in partnership with internal safety, security, and \nresponsibility teams. A range of evaluations and red teaming activities were conducted to help improve \nthe model and inform decision-making. These evaluations and activities align with Google's AI Principles \nand responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use... team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the \nmodel adheres to safety policies and desired outcomes;  \n●​\nAutomated Red Teaming to dynamically evaluate Gemini for safety and security \nconsiderations at scale, complementing human red teaming and static evaluations; \n●​\nEthics & Safety Reviews were conducted ahead of the model’s release... 5\nIn addition, we perform testing following the guidelines in Google DeepMind’s Frontier Safety \nFramework (FSF). \n \nSafety Policies: Gemini’s safety policies aim to prevent our Generative AI models from generating \nharmful content, including:  \n \n1.​\nContent related to child sexual abuse material and exploitation \n2.​... Training and Development Evaluation Results:  Results for some of the internal safety evaluations \nconducted during the development phase are listed below. The evaluation results are for automated \nevaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage \nincrease or decrease in performance compared to the indicated model, as described below. Overall,... Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustified refusals \nlow. We mark improvements in green and regressions in red. \n \n \nEvaluation1 \nDescription \nGemini 3 Pro \nvs. Gemini 2.5 Pro \nText to Text Safety... Automated content safety evaluation \nmeasuring safety policies \n-10.4% \nMultilingual Safety  \nAutomated safety policy evaluation across \nmultiple languages \n+0.2% (non-egregious) \nImage to Text Safety \nAutomated content safety evaluation \nmeasuring safety policies \n+3.1% (non-egregious)... standard of results. The performance results reported below are computed with improved evaluations \nand thus are not directly comparable with performance results found in previous Gemini model cards. \n \nWe expect variation in our automated safety evaluations results, which is why we review flagged content \nto check for egregious or dangerous material. Our manual review confirmed losses were... overwhelmingly either a) false positives or b) not egregious. \n \nHuman Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of \nthe model development team. High-level findings are fed back to the model team. For child safety \nevaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams... to protect children online and meet Google’s commitments to child safety across our models and \nGoogle products. For content safety policies generally, including child safety, we saw similar or improved \nsafety performance compared to Gemini 2.5 Pro. Compared to 2.5 Pro, the scope of red teaming was \nexpanded to cover more potential issues outside of our strict policies, and found no egregious... concerns. \n \nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and \ndeployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations \ninclude, but are not limited to:  \n \n●​\ndataset filtering;  \n●​... conditional pre-training; \n●​\nsupervised fine-tuning; \n●​\nreinforcement learning from human and critic feedback; \n●​\nsafety policies and desiderata; \n●​\nproduct-level mitigations such as safety filtering. \n \nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2.5 Pro but \nstill an open research problem), and b) possible degradation in multi-turn conversations. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n7... Frontier Safety \n \nWe evaluated Gemini 3 Pro as outlined in our latest Frontier Safety Framework (September-2025), \nand found that it did not reach any critical capability levels as outlined in the table below:   \n \nDomain \nKey Results for Gemini 3 Pro  \nCCL  \nCCL \nreached?"
          },
          {
            "snippetId": "risk-mitigations-snippet-3",
            "url": "https://huggingface.co/datasets/multimodalart/google-gemini-3-pro-pre-release-model-card",
            "snippet": "# Gemini 3 Pro - Model Card\n\n*Model Cards* *are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from*\n\n*time-to-time; for example, to include updated evaluations as the model is improved or revised. See the* *Google DeepMind site* *for a comprehensive list of model cards.*... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\n*Published / Model Release: November 2025*... ## Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\nOur models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... **Known Limitations:** Gemini 3 Pro may exhibit some of the general limitations of foundation models, such as hallucinations. There may also be occasional slowness or timeout issues. The knowledge cutoﬀ date for Gemini 3 Pro was January 2025.... ## Ethics and Content Safety\n\n**Evaluation Approach:** Gemini 3 Pro was developed in partnership with internal safety, security, and responsibility teams. A range of evaluations and red teaming activities were conducted to help improve the model and inform decision-making. These evaluations and activities align with Google's AI Principles and responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use Policy and the Gemini API Additional Terms of Service).... Evaluation types included but were not limited to:\n\n**Training/Development Evaluations**including automated and human evaluations carried out continuously throughout and after the model's training, to monitor its progress and performance; **Human Red Teaming**conducted by specialist teams who sit outside of the model development team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the model adheres to safety policies and desired outcomes; **Automated Red Teaming**to dynamically evaluate Gemini for safety and security considerations at scale, complementing human red teaming and static evaluations; **Ethics & Safety Reviews**were conducted ahead of the model's release... **Training and Development Evaluation Results:** Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustiﬁed refusals low. We mark improvements in green and regressions in red.... We continue to improve our internal evaluations, including reﬁning automated evaluations to reduce false positives and negatives, as well as update query sets to ensure balance and maintain a high\n\n1The ordering of evaluations in this table has changed from previous iterations of the 2.5 Flash-Lite model card in order to list safety evaluations together and improve readability. The type of evaluations listed have remained the same.... **Human Red Teaming Results:** We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level ﬁndings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisﬁed required launch thresholds, which were developed by expert teams to protect children online and meet Google's commitments to child safety across our models and Google products.... For content safety policies generally, including child safety, we saw similar or improved safety performance compared to Gemini 2.5 Pro. Compared to 2.5 Pro, the scope of red teaming was expanded to cover more potential issues outside of our strict policies, and found no egregious concerns.\n\n**Risks and Mitigations:** Safety and responsibility was built into Gemini 3 Pro throughout the training and deployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations include, but are not limited to:... - dataset ﬁltering;\n\n- conditional pre-training;\n\n- supervised ﬁne-tuning;\n\n- reinforcement learning from human and critic feedback;\n\n- safety policies and desiderata;\n\n- product-level mitigations such as safety ﬁltering.\n\nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2.5 Pro but still an open research problem), and b) possible degradation in multi-turn conversations.... ## Frontier Safety\n\nWe evaluated Gemini 3 Pro as outlined in our latest Frontier Safety Framework (September-2025), and found that it did not reach any critical capability levels as outlined in the table below:"
          },
          {
            "snippetId": "risk-mitigations-snippet-4",
            "url": "https://ai.google.dev/gemini-api/docs/changelog",
            "snippet": "This page documents updates to the Gemini API.\n\n## November 20, 2025\n\n- Released Gemini 3 Pro Image Preview,\n\n`gemini-3-pro-image-preview`, the next iteration to the Nano Banana model. Read the Image generation page for more details.... ## November 18, 2025\n\nLaunched the first Gemini 3 series model,\n\n`gemini-3-pro-preview`, our state-of-the-art reasoning and multimodal understanding model with powerful agentic and coding capabilities.\n\nIn addition to improvements in intelligence and performance, Gemini 3 Pro Preview introduces new behavior around:\n\nRead the Gemini 3 Developer Guide for migration, new features, and specs.... ## September 25, 2025\n\nReleased Gemini Robotics-ER 1.5 model in preview. See the Robotics overview to learn about how to use the model for your robotics application.\n\nLaunched following preview models:\n\n`gemini-2.5-flash-preview-09-2025`\n\n`gemini-2.5-flash-lite-preview-09-2025`\n\nSee the Models page for details.... ## June 17, 2025\n\n- Released\n\n`gemini-2.5-pro`, the stable version of our most powerful model, now with adaptive thinking. To learn more, see Gemini 2.5 Pro and Thinking.\n\n`gemini-2.5-pro-preview-05-06`will be redirected to... **Model updates:**\n\n- Released\n\n`gemini-2.5-flash-preview-05-20`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n- Released the\n\n`gemini-2.5-pro-preview-tts`and... ## May 7, 2025\n\n- Released\n\n`gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see Image generation and Gemini 2.0 Flash Preview Image Generation.\n\n## May 6, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling.\n\n`gemini-2.5-pro-preview-03-25`will automatically point to the new version of the model.... ## March 12, 2025\n\n**Model updates:**\n\n- Launched an experimental Gemini 2.0 Flash model capable of image generation and editing.\n\n- Released\n\n`gemma-3-27b-it`, available on AI Studio and through the Gemini API, as part of the Gemma 3 launch.\n\n**API updates:**\n\n- Added support for YouTube URLs as a media source.\n\n- Added support for including an inline video of less than 20MB.... ## October 3, 2024\n\n**Model updates:**\n\n- Released\n\n`gemini-1.5-flash-8b-001`, a stable version of our smallest Gemini API model.... ## September 24, 2024\n\n**Model updates:**\n\n- Released\n\n`gemini-1.5-pro-002`and\n\n`gemini-1.5-flash-002`, two new stable versions of Gemini 1.5 Pro and 1.5 Flash, for general availability.... - Updated the\n\n`gemini-1.5-pro-latest`model code to use\n\n`gemini-1.5-pro-002`and the\n\n`gemini-1.5-flash-latest`model code to use\n\n`gemini-1.5-flash-002`.\n\n- Released... `gemini-1.5-flash-8b-exp-0924`to replace\n\n`gemini-1.5-flash-8b-exp-0827`.\n\n- Released the civic integrity safety filter for the Gemini API and AI Studio.\n\n- Released support for two new parameters for Gemini 1.5 Pro and 1.5 Flash in\n\nPython and NodeJS:\n\n`frequencyPenalty`and\n\n`presencePenalty`.... ## August 27, 2024\n\n**Model updates:**\n\n- Released the following\n\nexperimental models:\n\n`gemini-1.5-pro-exp-0827`\n\n`gemini-1.5-flash-exp-0827`\n\n`gemini-1.5-flash-8b-exp-0827`... ## August 9, 2024\n\n**API updates:**\n\n- Added support for PDF processing.\n\n## August 5, 2024\n\n**Model updates:**\n\n- Fine-tuning support released for Gemini 1.5 Flash.\n\n## August 1, 2024\n\n**Model updates:**\n\n- Released\n\n`gemini-1.5-pro-exp-0801`, a new experimental version of Gemini 1.5 Pro.... ## March 19, 2024\n\n**Model updates:**\n\n- Added support for tuning Gemini 1.0 Pro in Google AI Studio or with the Gemini API."
          },
          {
            "snippetId": "risk-mitigations-snippet-5",
            "url": "https://www.studocu.com/en-gb/document/university-of-east-london/financial-accounting/gemini-3-pro-model-card-insights-safety-evaluations-nov-2025/146206142",
            "snippet": "- Evaluation Methods: The model undergoes rigorous testing across benchmarks to assess its reasoning and multimodal performance.\n\n- Safety Policies: Strict guidelines are in place to prevent harmful content generation, ensuring responsible AI use.\n\n- Intended Applications: Designed for complex problem-solving, the model excels in areas requiring advanced reasoning and creativity.\n\n## Preview text\n\nModel card published: November, 2025... ## Gemini 3 Pro\n\n## Model Card\n\n### Gemini 3 Pro - Model Card\n\nModel Cards are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from time-to-time; for example, to include updated evaluations as the model is improved or revised. See the Google DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\nPublished / Model Release: November 2025... #### Model Information\n\nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of highly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from different information sources, including text, audio, images, video, and entire code repositories.... Architecture: Gemini 3 Pro is a sparse mixture-of-experts (MoE) (Clark et al., 2022; Du et al., 2021; Fedus et al., 2021; Jiang et al., 2024, Lepikhin et al., 2020; Riquelme et al., 2021; Roller et al., 2021; Shazeer et al., 2017 ) transformer-based model (Vaswani et al., 2017) with native multimodal support for text, vision, and audio inputs.... #### Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\n● Gemini App ● Google Cloud / Vertex AI ● Google AI Studio ● Gemini API ● Google AI Mode ● Google Antigravity... Our models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... Training and Development Evaluation Results: Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2 Pro across both safety and tone, while keeping unjustified refusals low. We mark improvements in green and regressions in red.... Evaluation 1 Description Gemini 3 Pro vs. Gemini 2 Pro\n\nText to Text Safety Automated content safety evaluation measuring safety policies -10%\n\nMultilingual Safety Automated safety policy evaluation across multiple languages +0% (non-egregious)\n\nImage to Text Safety Automated content safety evaluation measuring safety policies +3% (non-egregious)... We expect variation in our automated safety evaluations results, which is why we review flagged content to check for egregious or dangerous material. Our manual review confirmed losses were overwhelmingly either a) false positives or b) not egregious.... Human Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level findings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams to protect children online and meet Google’s commitments to child safety across our models and Google products.... For content safety policies generally, including child safety, we saw similar or improved safety performance compared to Gemini 2 Pro. Compared to 2 Pro, the scope of red teaming was expanded to cover more potential issues outside of our strict policies, and found no egregious concerns.\n\nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and deployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations include, but are not limited to:... ● dataset filtering; ● conditional pre-training; ● supervised fine-tuning; ● reinforcement learning from human and critic feedback; ● safety policies and desiderata; ● product-level mitigations such as safety filtering.\n\nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2 Pro but still an open research problem), and b) possible degradation in multi-turn conversations."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Risk Mitigations",
            "score": 1,
            "explanation": "Safety and responsibility was built into Gemini 3 Pro throughout the training and deployment lifecycle, including pre-training, post-training, and product-level mitigations."
          }
        ]
      }
    ]
  },
  {
    "model": "Grok 4",
    "provider": "xAI",
    "type": "text-reasoning",
    "sections": [
      {
        "sectionId": "model-details",
        "sectionSnippets": [
          {
            "snippetId": "model-details-snippet-1",
            "url": "https://docs.x.ai/docs/models/grok-4-0709",
            "snippet": "## Models\n\n## /\n## Grok 4\n\nOur latest and greatest flagship model, offering unparalleled performance in natural language, math and reasoning - the perfect jack of all trades.\n\n#### At a glance\n\nModalities\n\nContext window\n\n256,000\n\nPricing\n\n#### Capabilities\n\nFunction calling\n\nConnect the xAI model to external tools and systems.\n\nStructured outputs\n\nReturn responses in a specific, organized formats.\n\nReasoning\n\nThe model thinks before responding.... #### Pricing\n\n#### Input\n\nTokens\n\n/ 1M tokens\n\nCached tokens\n\n/ 1M tokens\n\n#### Output\n\nTokens\n\n/ 1M tokens\n\nLive search\n\n/ 1K sources\n\nYou are charged for each token used when making calls to our API.\n\nUsing cached input tokens can significantly reduce your costs.... #### Details\n\nModel name\n\nAliases\n\n|Region|us-east-1|\n|--|--|\n|Pricing per million tokens *|\n|Input|\n|Cached input|\n|Output|\n|Per thousand sources|\n|Live search|\n|Rate limits|\n|Requests per minute|480|\n|Tokens per minute|2,000,000|\nWe charge different rates for requests which exceed the 128K context window... #### Quickstart\n\n```\n\nfrom xai_sdk import Client\n\nfrom xai_sdk.chat import user, system\n\nclient = Client(api_key=\"<YOUR_XAI_API_KEY_HERE>\")\n\nchat = client.chat.create(model=\"grok-4-0709\", temperature=0)\n\nchat.append(system(\"You are a PhD-level mathematician.\"))\n\nchat.append(user(\"What is 2 + 2?\"))\n\nresponse = chat.sample()\n\nprint(response.content)\n\n```... #### Quickstart\n\nCreate an API Key and make your first request.\n\n#### Tool use\n\nLet Grok perform actions and look up information.\n\n#### Structured outputs\n\nLet Grok generate structured outputs."
          },
          {
            "snippetId": "model-details-snippet-2",
            "url": "https://docs.x.ai/docs/overview",
            "snippet": "#### Getting started\n# Welcome\n\nWelcome to the xAI developer docs! Our API makes it easy to harness Grok's intelligence in your projects. Grok is our flagship AI model designed to deliver truthful, insightful answers.\n\n## Grok 4 now available\n\nWe're proud to bring you Grok 4 access on the API. Grok 4 currently supports text modality with vision, image gen and other capabilities coming soon.\n\nModalities\n\nContext window\n\n256,000\n\nFeatures\n\nFunction calling\n\nStructured outputs\n\nReasoning... ## Jump right in\n\n#### Quickstart\n\nCreate an API Key and make your first request.\n\n#### Tool Use\n\nLet Grok perform actions and look up information.\n\n#### Images\n\nUse Grok to analyze images or perform OCR.\n\n*Are you a non-developer or simply looking for our consumer services? Visit Grok.com or download one of the iOS or Android apps. See our Comparison Table for the differences.*... ## Questions and feedback\n\nIf you have any questions or feedback, feel free to email us at support@x.ai.\n\nHappy Grokking! 😎"
          },
          {
            "snippetId": "model-details-snippet-3",
            "url": "https://docs.public.content.oci.oraclecloud.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "# xAI Grok 4\n\nThe\n\n`xai.grok-4` model has better performance than its predecessor, Grok 3, and excels at enterprise use cases such as data extraction, coding, and summarizing text. This model has a deep domain knowledge in finance, healthcare, law, and science.... ## Available in These Regions\n\n**US East (Ashburn)**(on-demand only) **US Midwest (Chicago)**(on-demand only) **US West (Phoenix)**(on-demand only)\n\n**Important**\n\n**External Calls**\n\nThe xAI Grok models are hosted in an OCI data center, in a tenancy provisioned for xAI. The xAI Grok models, which can be accessed through the OCI Generative AI service, are managed by xAI.... ## Access this Model\n## Key Features\n\n**Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## On-Demand Mode\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response.... ## Troubleshooting\n\n**Issue:** The Grok 4 model doesn't respond.\n\n**Cause:** The **Maximum output tokens** parameter in the playground or the\n\n`max_tokens` parameter in the API is likely too low.\n\n**Action:** Increase the maximum output tokens parameter.\n\n**Reason:** For difficult problems that require reasoning and problem-solving, and for large sophisticated inputs, the\n\n`xai.grok-4` model tends to think and consumes many tokens, so if the\n\n`max_tokens` parameter is too low, the model uses the allocated tokens and doesn't return a final response."
          },
          {
            "snippetId": "model-details-snippet-4",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "# xAI Grok 4\n\nThe\n\n`xai.grok-4` model has better performance than its predecessor, Grok 3, and excels at enterprise use cases such as data extraction, coding, and summarizing text. This model has a deep domain knowledge in finance, healthcare, law, and science.... ## Available in These Regions\n\n**US East (Ashburn)**(on-demand only) **US Midwest (Chicago)**(on-demand only) **US West (Phoenix)**(on-demand only)\n\n**Important**\n\n**External Calls**\n\nThe xAI Grok models are hosted in an OCI data center, in a tenancy provisioned for xAI. The xAI Grok models, which can be accessed through the OCI Generative AI service, are managed by xAI.... ## Access this Model\n\n## Key Features\n\n🔗 **Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## On-Demand Mode\n\n\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\n\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response.... ## Troubleshooting\n\n\n\n**Issue:** The Grok 4 model doesn't respond.\n\n**Cause:** The **Maximum output tokens** parameter in the playground or the\n\n`max_tokens` parameter in the API is likely too low.\n\n**Action:** Increase the maximum output tokens parameter.\n\n**Reason:** For difficult problems that require reasoning and problem-solving, and for large sophisticated inputs, the\n\n`xai.grok-4` model tends to think and consumes many tokens, so if the\n\n`max_tokens` parameter is too low, the model uses the allocated tokens and doesn't return a final response."
          },
          {
            "snippetId": "model-details-snippet-5",
            "url": "https://www.prompthub.us/models/grok-4",
            "snippet": "Grok 4 is a powerful reasoning model with multimodal support.\n\nThe company that provides the model\n\nThe number of tokens you can send in a prompt\n\nThe maximum number of tokens a model can generate in one request\n\nThe cost of prompt tokens sent to the model\n\nThe cost of output tokens generated by the model... When the model's knowledge ends\n\nWhen the model was launched\n\nCapability for the model to use external tools\n\nAbility to process and analyze visual inputs, like images\n\nSupport for multiple languages\n\nWhether the model supports fine-tuning on custom datasets\n\nGrok 4 is xAI’s flagship reasoning model, offering powerful chain-of-thought capabilities alongside native multimodal support for both vision and text.... It costs $3.00 per million input tokens and $15.00 per million output tokens.\n\nGrok 4 supports a context window of up to 256,000 tokens, making it well-suited for long-form inputs.\n\nThe maximum output length isn’t explicitly specified, but it can generate responses up to the limit of its 256K context window.... Grok 4 was released on July 10, 2025.\n\nThe knowledge cut-off date isn’t publicly documented.\n\nYes, Grok 4 can process and reason over visual inputs like images.\n\nYes, it supports function calling for integration with external tools and APIs.\n\nYes, Grok 4 handles multiple languages for both input and output.... No, fine-tuning is not available for Grok 4.\n\nSee the xAI models documentation here:\n\nhttps://docs.x.ai/docs#models\n\nCollaborate with thousands of AI builders to discover, manage, and improve prompts—free to get started."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Model overview",
            "score": 1,
            "explanation": "Grok 4 is our flagship AI model designed to deliver truthful, insightful answers."
          },
          {
            "name": "Organization developing the model",
            "score": 1,
            "explanation": "The xAI Grok models are hosted in an OCI data center, in a tenancy provisioned for xAI."
          },
          {
            "name": "Model Version",
            "score": 1,
            "explanation": "The `xai.grok-4` model has better performance than its predecessor, Grok 3."
          },
          {
            "name": "Model Release Date",
            "score": 1,
            "explanation": "Grok 4 was released on July 10, 2025."
          },
          {
            "name": "Model Version Progression",
            "score": 1,
            "explanation": "The `xai.grok-4` model has better performance than its predecessor, Grok 3."
          },
          {
            "name": "Model Architecture",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Model Dependencies",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Paper and relevant links",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Model Distribution Forms",
            "score": 1,
            "explanation": "Available On-Demand: Access this model on-demand, through the Console playground or the API."
          }
        ]
      },
      {
        "sectionId": "model-inputs-outputs",
        "sectionSnippets": [
          {
            "snippetId": "model-inputs-outputs-snippet-1",
            "url": "https://docs.x.ai/docs/models/grok-4-0709",
            "snippet": "## Models\n\n## /\n## Grok 4\n\nOur latest and greatest flagship model, offering unparalleled performance in natural language, math and reasoning - the perfect jack of all trades.\n\n#### At a glance\n\nModalities\n\nContext window\n\n256,000\n\nPricing\n\n#### Capabilities\n\nFunction calling\n\nConnect the xAI model to external tools and systems.\n\nStructured outputs\n\nReturn responses in a specific, organized formats.\n\nReasoning\n\nThe model thinks before responding.... #### Pricing\n\n#### Input\n\nTokens\n\n/ 1M tokens\n\nCached tokens\n\n/ 1M tokens\n\n#### Output\n\nTokens\n\n/ 1M tokens\n\nLive search\n\n/ 1K sources\n\nYou are charged for each token used when making calls to our API.\n\nUsing cached input tokens can significantly reduce your costs.... #### Details\n\nModel name\n\nAliases\n\n|Region|us-east-1|\n|--|--|\n|Pricing per million tokens *|\n|Input|\n|Cached input|\n|Output|\n|Per thousand sources|\n|Live search|\n|Rate limits|\n|Requests per minute|480|\n|Tokens per minute|2,000,000|\nWe charge different rates for requests which exceed the 128K context window... #### Quickstart\n\n```\n\nfrom xai_sdk import Client\n\nfrom xai_sdk.chat import user, system\n\nclient = Client(api_key=\"<YOUR_XAI_API_KEY_HERE>\")\n\nchat = client.chat.create(model=\"grok-4-0709\", temperature=0)\n\nchat.append(system(\"You are a PhD-level mathematician.\"))\n\nchat.append(user(\"What is 2 + 2?\"))\n\nresponse = chat.sample()\n\nprint(response.content)\n\n```... #### Quickstart\n\nCreate an API Key and make your first request.\n\n#### Tool use\n\nLet Grok perform actions and look up information.\n\n#### Structured outputs\n\nLet Grok generate structured outputs."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-2",
            "url": "https://data.x.ai/2025-08-20-grok-4-model-card.pdf",
            "snippet": "xAI\nLast updated: August 20, 2025\n1\nIntroduction\nGrok 4 is the latest reasoning model from xAI with advanced reasoning and tool-use capabilities,\nenabling it to achieve new state-of-the-art performance across challenging academic and industry\nbenchmarks. Because our models push the frontier of AI capabilities, we are committed to mitigating... different risk scenarios. We categorize these safety-relevant behaviors as: abuse potential (Section 2.1),\nconcerning propensities (Section 2.2), and dual-use capabilities (Section 2.3). This report describes\nour current evaluation methodology, results, and mitigations for these various behaviors.... In this document, we focus on the Grok 4 model. xAI deploys Grok 4 in both the consumer-facing\napplications (Grok 4 Web) and through an enterprise use-focused API (Grok 4 API). We report\nevaluations for Grok 4 API and Grok 4 Web now available to our customers, including in the EU.... of enabling malicious use. While the general cyber knowledge and exploitation capabilities of Grok\n4 are a significant step up from prior models, third-party testing shows that Grok 4’s end-to-end\noffensive cyber capabilities remain below the level of a human professional.\nAs of this writing, on top of our safety training, we have implemented a system prompt that provides... additional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 overall presents a low\nrisk for malicious use and loss of control.\n2.1... evaluation metric is response rate (i.e., the rate at which the model answered queries that should\nhave been refused) for all three evaluation settings: standard, user jailbreak, and system jailbreak.\nGrok 4 Web does not accept custom system prompts from users, so we do not evaluate with\nsystem jailbreaks.... Agentic abuse.\nGrok 4 introduces advanced reasoning and tool-calling capabilities that enable\nthe model to be used in an “agentic” manner, that is, repeatedly take actions toward a specified\ngoal. Such capabilities introduce additional risks of misuse beyond what is present in conversational\nsettings, such as executing real function calls. To quantify these risks, we use the AgentHarm\n2... Input filters.\nWe also employ model-based filters for both Grok 4 API and Grok 4 Web,\nwhich reject classes of harmful requests, including biological and chemical weapons, self-harm, and\nCSAM.\n2.2\nConcerning Propensities\nAI models may contain propensities that reduce their controllability, such as deception, power-seeking,... control.\n2.2.1\nEvaluations\nDeception.\nWe measure how deceptive the model is by the rate at which the model lies, i.e.,\nknowingly makes false statements intended to be received as true. We find that instructing the model\nto be honest in the system prompt reduces deception, and we implement this mitigation in Grok... such as knowledge of Metasploit, to understanding of vulnerability detection, to reverse engineering\nsimple binaries.\nCyber agents.\nWe also evaluate Grok 4’s agentic hacking capabilities on CyBench, a collection\nof 40 capture-the-flag-style questions which measures a model’s ability to perform cybersecurity\nchallenges [Zhang et al., 2025]. The model is placed in an agent harness which gives it access to... 38.4% and 22.1% on VCT, so both Grok 4 API and Grok 4 Web achieve superhuman performance\non identifying issues in biological protocols and wetlab virology experiments. Similarly, Grok 4\nAPI shows strong capabilities in both cybersecurity and chemistry. Note that these evaluations... system prompts (Section 3.2).\n3.1\nData and Training\nGrok 4 is first pre-trained with a data recipe that includes publicly available Internet data, data\nproduced by third-parties for xAI, data from users or contractors, and internally generated data. We\nperform data filtering procedures on the training data, such as de-duplication and classification, to... ensure data quality and safety prior to training. In addition to pre-training, our recipe uses a variety\nof reinforcement learning techniques—human feedback, verifiable rewards, and model grading—along\nwith supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-\nprompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n7... language models. In The Thirteenth International Conference on Learning Representations, 2025.\nURL https://openreview.net/forum?id=tc90LV0yRL.\n8"
          },
          {
            "snippetId": "model-inputs-outputs-snippet-3",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "## Access this Model\n\n## Key Features\n\n🔗 **Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## Limits\n\n\n\n- Image Inputs\n\n\n\n**Console:**Upload one or more\n\n`.png`or\n\n`.jpg`images, each 5 MB or smaller.\n\n**API:**Submit a\n\n`base64`encoded version of an image, ensuring that each converted image is more than 512 and less than 1,792 tokens. For example, a 512 x 512 image typically converts to around 1,610 tokens.... ## On-Demand Mode\n\n\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\n\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... For large inputs with difficult problems, set a high value for the maximum output tokens parameter. See Troubleshooting.\n\n**Temperature**\n\n\n\nThe level of randomness used to generate the output text. Min: 0, Max: 2\n\n**Tip**\n\nStart with the temperature set to 0 or less than one, and increase the temperature as you regenerate the prompts for a more creative output. High temperatures can introduce hallucinations and factually incorrect information.... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response.... ## Troubleshooting\n\n\n\n**Issue:** The Grok 4 model doesn't respond.\n\n**Cause:** The **Maximum output tokens** parameter in the playground or the\n\n`max_tokens` parameter in the API is likely too low.\n\n**Action:** Increase the maximum output tokens parameter.\n\n**Reason:** For difficult problems that require reasoning and problem-solving, and for large sophisticated inputs, the\n\n`xai.grok-4` model tends to think and consumes many tokens, so if the\n\n`max_tokens` parameter is too low, the model uses the allocated tokens and doesn't return a final response."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-4",
            "url": "https://docs.public.content.oci.oraclecloud.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "## Access this Model\n## Key Features\n\n**Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## Limits\n\n- Image Inputs\n\n\n\n**Console:**Upload one or more\n\n`.png`or\n\n`.jpg`images, each 5 MB or smaller.\n\n**API:**Submit a\n\n`base64`encoded version of an image, ensuring that each converted image is more than 512 and less than 1,792 tokens. For example, a 512 x 512 image typically converts to around 1,610 tokens.... ## On-Demand Mode\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... For large inputs with difficult problems, set a high value for the maximum output tokens parameter. See Troubleshooting.\n\n**Temperature**\n\n\n\nThe level of randomness used to generate the output text. Min: 0, Max: 2\n\n**Tip**\n\nStart with the temperature set to 0 or less than one, and increase the temperature as you regenerate the prompts for a more creative output. High temperatures can introduce hallucinations and factually incorrect information.... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response.... ## Troubleshooting\n\n**Issue:** The Grok 4 model doesn't respond.\n\n**Cause:** The **Maximum output tokens** parameter in the playground or the\n\n`max_tokens` parameter in the API is likely too low.\n\n**Action:** Increase the maximum output tokens parameter.\n\n**Reason:** For difficult problems that require reasoning and problem-solving, and for large sophisticated inputs, the\n\n`xai.grok-4` model tends to think and consumes many tokens, so if the\n\n`max_tokens` parameter is too low, the model uses the allocated tokens and doesn't return a final response."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-5",
            "url": "https://openrouter.ai/x-ai/grok-4",
            "snippet": "# xAI: Grok 4\n\n### x-ai/grok-4\n\nCreated Jul 9, 2025256,000 context\n\nStarting at $3/M input tokensStarting at $15/M output tokens\n\nOpenRouter provides an OpenAI-compatible completion API to 400+ models & providers that you can call directly, or using the OpenAI SDK. Additionally, some third-party SDKs are available.... In the examples below, the OpenRouter-specific headers are optional. Setting them allows your app to appear on the OpenRouter leaderboards.\n\nFor information about using third-party SDKs and frameworks with OpenRouter, please see our frameworks documentation.\n\nSee the Request docs for all possible fields, and Parameters for explanations of specific sampling parameters."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Inputs",
            "score": 1,
            "explanation": "You are charged for each token used when making calls to our API."
          },
          {
            "name": "Outputs",
            "score": 1,
            "explanation": "Return responses in a specific, organized formats."
          },
          {
            "name": "Token Count",
            "score": 1,
            "explanation": "maximum prompt + response length is 128,000 tokens"
          }
        ]
      },
      {
        "sectionId": "model-data",
        "sectionSnippets": [
          {
            "snippetId": "model-data-snippet-1",
            "url": "https://data.x.ai/2025-09-19-grok-4-fast-model-card.pdf",
            "snippet": "xAI\nLast updated: September 19, 2025\n1\nIntroduction\nGrok 4 Fast is an efficiency-focused model from xAI which offers reasoning capabilities near the\nlevel of Grok 4 with much lower latency and cost, as well as the ability to skip reasoning entirely\nfor the lowest latency applications.... Grok 4 Fast was pre-trained on a general purpose data corpus, then post-trained on various tasks\nand tool use, as well as demonstrations of correct refusal behaviors according to our default safety\npolicy. We also deploy Grok 4 Fast in our API with a fixed system prompt prefix that reminds\nthe model of our safety policy, in addition to input filters to safeguard against abuse.... Prior to release, we have evaluated various specific safety-relevant behaviors of Grok 4 Fast: abuse\npotential (Section 2.1), concerning propensities (Section 2.2), and dual-use capabilities (Section 2.3).\nIn this report, we describe our current evaluation methodology, results, and any mitigations for these... various behaviors in Grok 4 Fast. All evaluations in this report were conducted on a near-final\nrelease checkpoint.\n2\nEvaluations\nOur approach to model evaluations varies depending on the specific behavior under assessment.\nTo reduce the potential for abuse of Grok 4 Fast that might lead to serious injury to people,... As of this writing, on top of our safety training, we have implemented a system prompt that provides\nadditional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 Fast overall presents... specify further restrictions on model behavior within the system message, such as prohibiting the\nmodel from generating adult content. We train Grok 4 Code to obey additional instructions in\nsystem or user messages, as long as they do not violate the policy message.\nAgentic abuse.\nGrok 4 Fast introduces advanced reasoning and tool-calling capabilities that... 3\npotential abuse, we find that our safeguards are able to greatly reduce AI propensities that may lead\nto loss of control.\n2.2.1\nEvaluations\nDeception.\nWe measure how deceptive the model is by the rate at which the model lies, i.e.,\nknowingly makes false statements intended to be received as true. We find that instructing the model... evaluating the model in non-reasoning mode increases the rate of dishonesty by a noticeable margin.\nFor Grok 4 Fast, our deployed system prompt does not include explicit instructions to avoid\ndeceptive behavior. Including such instructions, e.g. by appending the line “You are Grok, built by... in particular the cyber killchain [Hutchins et al., 2011]. The questions span a variety of topics,\nsuch as knowledge of Metasploit, to understanding of vulnerability detection, to reverse engineering\nsimple binaries.\nCyber agents.\nWe also evaluate Grok 4 Fast’s agentic hacking capabilities on CyBench,... a collection of 40 capture-the-flag-style questions which measures a model’s ability to perform\ncybersecurity challenges [Zhang et al., 2025]. The model is placed in an agent harness which gives\nit access to tools such as code execution. We use the open-source Inspect evaluation framework\ndeveloped by the UK AISI, and report the unguided task success rate.... accuracy\n39.0\nVCT\naccuracy\n54.5\nWMDP Bio\naccuracy\n85.2\nChemistry\nWMDP Chem\naccuracy\n77.5\nCybersecurity\nWMDP Cyber\naccuracy\n81.4\nCyBench\nunguided success rate\n30.0... system prompts (Section 3.2).\n3.1\nData and Training\nGrok 4 Fast is first pre-trained with a data recipe that includes publicly available Internet data,\ndata produced by third-parties for xAI, data from users or contractors, and internally generated data.\nWe perform data filtering procedures on the training data, such as de-duplication and classification,... to ensure data quality and safety prior to training. In addition to pre-training, our recipe uses\na variety of reinforcement learning techniques—human feedback, verifiable rewards, and model\ngrading—along with supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-... prompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n6... Li, Ann-Kathrin Dombrowski, Shashwat Goel, Gabriel Mukobi, et al. The wmdp benchmark:\nMeasuring and reducing malicious use with unlearning. In International Conference on Machine\nLearning, pages 28525–28550. PMLR, 2024."
          },
          {
            "snippetId": "model-data-snippet-2",
            "url": "https://data.x.ai/2025-08-20-grok-4-model-card.pdf",
            "snippet": "xAI\nLast updated: August 20, 2025\n1\nIntroduction\nGrok 4 is the latest reasoning model from xAI with advanced reasoning and tool-use capabilities,\nenabling it to achieve new state-of-the-art performance across challenging academic and industry\nbenchmarks. Because our models push the frontier of AI capabilities, we are committed to mitigating... different risk scenarios. We categorize these safety-relevant behaviors as: abuse potential (Section 2.1),\nconcerning propensities (Section 2.2), and dual-use capabilities (Section 2.3). This report describes\nour current evaluation methodology, results, and mitigations for these various behaviors.... In this document, we focus on the Grok 4 model. xAI deploys Grok 4 in both the consumer-facing\napplications (Grok 4 Web) and through an enterprise use-focused API (Grok 4 API). We report\nevaluations for Grok 4 API and Grok 4 Web now available to our customers, including in the EU.... Finally, we describe our training pipeline (Section 3.1) and additional transparency commitments\n(Section 3.2).\n2\nEvaluations\nOur approach to model evaluations varies depending on the specific behavior under assessment.\nTo reduce the potential for abuse of Grok 4 that might lead to serious injury to people, property or... We also reduce various propensities of Grok 4 that might make it difficult to control, such as\nbeing deceptive, power-seeking, manipulative, or biased, among others (Section 2.2). To achieve this,\nour main focus is on measuring and reducing the rate at which Grok 4 responds deceptively. We... of enabling malicious use. While the general cyber knowledge and exploitation capabilities of Grok\n4 are a significant step up from prior models, third-party testing shows that Grok 4’s end-to-end\noffensive cyber capabilities remain below the level of a human professional.\nAs of this writing, on top of our safety training, we have implemented a system prompt that provides... additional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 overall presents a low\nrisk for malicious use and loss of control.\n2.1... Input filters.\nWe also employ model-based filters for both Grok 4 API and Grok 4 Web,\nwhich reject classes of harmful requests, including biological and chemical weapons, self-harm, and\nCSAM.\n2.2\nConcerning Propensities\nAI models may contain propensities that reduce their controllability, such as deception, power-seeking,... control.\n2.2.1\nEvaluations\nDeception.\nWe measure how deceptive the model is by the rate at which the model lies, i.e.,\nknowingly makes false statements intended to be received as true. We find that instructing the model\nto be honest in the system prompt reduces deception, and we implement this mitigation in Grok... such as knowledge of Metasploit, to understanding of vulnerability detection, to reverse engineering\nsimple binaries.\nCyber agents.\nWe also evaluate Grok 4’s agentic hacking capabilities on CyBench, a collection\nof 40 capture-the-flag-style questions which measures a model’s ability to perform cybersecurity\nchallenges [Zhang et al., 2025]. The model is placed in an agent harness which gives it access to... measure dual-use knowledge: a high score indicates greater capability to enable weapons development,\nnot necessarily increased risk.\nCategory\nEvaluation\nMetric\nGrok 4 API\nGrok 4 Web\nPersuasion\nMakeMeSay\nwin rate\n0.12\n-\nBiology\nBioLP-Bench\naccuracy... system prompts (Section 3.2).\n3.1\nData and Training\nGrok 4 is first pre-trained with a data recipe that includes publicly available Internet data, data\nproduced by third-parties for xAI, data from users or contractors, and internally generated data. We\nperform data filtering procedures on the training data, such as de-duplication and classification, to... ensure data quality and safety prior to training. In addition to pre-training, our recipe uses a variety\nof reinforcement learning techniques—human feedback, verifiable rewards, and model grading—along\nwith supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-\nprompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n7... language models. In The Thirteenth International Conference on Learning Representations, 2025.\nURL https://openreview.net/forum?id=tc90LV0yRL.\n8"
          },
          {
            "snippetId": "model-data-snippet-3",
            "url": "https://docs.public.content.oci.oraclecloud.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "# xAI Grok 4\n\nThe\n\n`xai.grok-4` model has better performance than its predecessor, Grok 3, and excels at enterprise use cases such as data extraction, coding, and summarizing text. This model has a deep domain knowledge in finance, healthcare, law, and science.... ## Available in These Regions\n\n**US East (Ashburn)**(on-demand only) **US Midwest (Chicago)**(on-demand only) **US West (Phoenix)**(on-demand only)\n\n**Important**\n\n**External Calls**\n\nThe xAI Grok models are hosted in an OCI data center, in a tenancy provisioned for xAI. The xAI Grok models, which can be accessed through the OCI Generative AI service, are managed by xAI.... ## Access this Model\n## Key Features\n\n**Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## On-Demand Mode\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response.... ## Troubleshooting\n\n**Issue:** The Grok 4 model doesn't respond.\n\n**Cause:** The **Maximum output tokens** parameter in the playground or the\n\n`max_tokens` parameter in the API is likely too low.\n\n**Action:** Increase the maximum output tokens parameter.\n\n**Reason:** For difficult problems that require reasoning and problem-solving, and for large sophisticated inputs, the\n\n`xai.grok-4` model tends to think and consumes many tokens, so if the\n\n`max_tokens` parameter is too low, the model uses the allocated tokens and doesn't return a final response."
          },
          {
            "snippetId": "model-data-snippet-4",
            "url": "https://www.datacamp.com/blog/grok-4",
            "snippet": "Course\n\nAfter we waited months for Grok 3.5, xAI skipped it altogether and jumped straight to Grok 4. (As of November 2025, Grok 4.1 has also launched).\n\nIs the jump justified by the model’s performance?\n\nYes, if you judge by the benchmarks. Right now, Grok 4 is probably the best model in the world on paper.... ## Grok 4\n\nThe Grok 4 family includes only Grok 4 and Grok 4 Heavy, and there’s no mini version that we can use for fast reasoning.\n\nGrok 4 is xAI’s latest single-agent model (unlike Grok 4 Heavy, which uses multiple agents—we’ll get to that in the next section). Based on the livestream, there’s nothing particularly groundbreaking in terms of engineering. The gains seem to come from a series of smaller tweaks and a significant increase in compute, roughly 10x more than what was used for Grok 3.... Source: xAI\n\nThe context window is 128,000 tokens in the app and 256,000 in the API, which gives some room for long-form reasoning but isn’t especially generous by current standards. Gemini 2.5 Pro, for example, offers 1 million. If you’re building with Grok, you’ll likely need to spend time structuring and pruning your context carefully.... It’s less clear how it performs in broader consumer workflows or how well it handles safety at scale. But xAI says it’s already in use at biomedical labs, financial firms, and early enterprise partners.... As Elon Musk pointed out in the livestream, Grok 4’s image understanding and generation still isn’t very advanced. If you want constant and reliable results, I think it’s fair to say that Grok 4 is a text-only model at the moment.... ## Grok 4 Benchmarks\n\nGrok 4’s main claim to fame is its performance across a wide mix of benchmarks, from academic exams to business simulations. According to xAI, the model improves significantly over previous versions thanks mostly to more compute—both during training and inference—not necessarily because of new architectural breakthroughs.... Source: xAI\n\nWith no tool use, Grok 4 plateaus at around 26.9% accuracy. With tools enabled (e.g., code execution), it hits 41.0%. And when run in its multi-agent “Heavy” configuration, it climbs to 50.7%—a major jump that’s over double the best prior tool-free model scores.... Source: xAI\n\nIn short: Grok 4 performs well where xAI has tested it. But as always, you should look beyond the leaderboard. The benchmarks are promising, but they don’t tell the whole story—especially if your use case depends on vision, code generation, or real-time interaction in messy environments.... ## How to Access Grok 4\n\nGrok 4 is now available through three main entry points: the X app, the xAI API, and the grok.com platform. Whether you want to chat with the model, build with it, or test its reasoning capabilities more formally, here’s how to get started.... ### Grok 4 API\n\nIf you want to integrate Grok into your own app or workflow, you can use the xAI API.\n\nSteps:\n\n- Go to https://x.ai/api and request developer access.\n\n- Once approved, you’ll receive an API key and access to docs.\n\n- Make sure to read through the documentation for more details.... ### August: Specialized coding model\n\nThe first follow-up is a coding-focused model expected in August. Unlike Grok 4, which is a generalist, this will be a specialized model designed to handle code with more speed and accuracy. xAI described it as “fast and smart,” trained specifically to improve both latency and reasoning in software development workflows.... ### October: Video generation model\n\nThe final release in the current timeline is a video generation model due in October. xAI says they’ll be training it on over 100,000 GPUs. Based on their remarks, this system will aim to produce high-quality, interactive, and editable video content.... For developers and researchers, it’s worth exploring. For casual users, the speed and responsiveness of Grok 3 or other mainstream models are a better fit. The roadmap is ambitious, with a coding model, multimodal agent, and video generator all due by October. Whether xAI can deliver those on time is another question. But with Grok 4, they’ve at least made a compelling case that they’re in the race.... ### Does Grok 4 support multimodal inputs like images and videos?\n\nGrok 4 supports multimodal inputs, but its capabilities and output are primarily text-based. While there are plans to enhance multimodal functionalities, including image and video understanding, these features are still in development."
          },
          {
            "snippetId": "model-data-snippet-5",
            "url": "https://walterpinem.com/grok-4-xai-api-python/",
            "snippet": "**Understanding Grok 4’s Core Architecture**\n\nI will walk you through the fundamental architecture of Grok 4, which represents a significant advancement in artificial intelligence through its sophisticated reinforcement learning implementation and native tool integration.\n\nThe model builds upon the foundation established by Grok 3, which achieved unprecedented performance through scaled next-token prediction pretraining, while introducing revolutionary reasoning capabilities that extend far beyond traditional language model limitations.... - Getting Started with Anthropic Claude API: A Practical Guide\n\n- Getting Started with kluster.ai for Scalable AI\n\n- 60+ Best Free API for Testing and Building Projects\n\nThis 200,000 GPU cluster enabled training runs that consumed over an order of magnitude more compute than previous iterations, resulting in smooth performance gains throughout the training process.... **Reinforcement Learning Implementation**\n\nThe reinforcement learning framework in Grok 4 operates through a sophisticated training methodology that refines reasoning abilities at unprecedented computational scales.\n\nThe training process incorporates innovations throughout the entire technology stack, including new infrastructure developments and algorithmic improvements that increased compute efficiency by 6x compared to previous implementations.... The model’s training data has been significantly expanded from primarily mathematical and coding domains to encompass numerous additional fields, creating a more comprehensive knowledge base for reasoning tasks.\n\nThis expansion required a massive data collection effort focused on verifiable training data, which ensures the model’s responses can be validated against known correct answers.\n\n**Native Tool Integration Architecture**... **Real-World Problem Solving Applications**\n\nFuture iterations will focus on expanding the model’s ability to handle complex real-world scenarios that require adaptive learning and dynamic problem-solving approaches.\n\nThis expansion will involve developing new training methodologies that can accommodate the unpredictable nature of real-world challenges while maintaining the model’s current level of performance and reliability.... The development of these capabilities will require continued innovation in reinforcement learning techniques, particularly in areas where traditional training approaches may not provide sufficient guidance for complex decision-making scenarios.\n\nThe team plans to explore new reward structures and training paradigms that can support more sophisticated real-world applications.\n\n**Performance Optimization and Efficiency Improvements**\n\nThe ongoing focus on making models smarter, faster, and more efficient will drive continued improvements in computational efficiency and response quality.... These improvements will build upon the 6x compute efficiency gains achieved in Grok 4’s development, seeking additional optimizations that can reduce resource requirements while maintaining or improving performance.... Grok 4 is already integrated to\n\n**AI vibe coding tools** such as Trae etc, so if you already use some of those, you can get started immediately, otherwise, keep reading.\n\nIn this tutorial, I will guide you through testing Grok 4 using the\n\n**xAI API with Python**, providing detailed steps and code examples to ensure clarity.... By following this guide, you can effectively interact with Grok 4, exploring its capabilities in a structured, programmatic manner.\n\n**Prerequisites for Using the xAI API**\n\nBefore proceeding, you must set up your environment to access the xAI API, which requires specific tools and credentials to ensure seamless integration.\n\nFirst, install Python 3.8 or higher, as the xAI SDK is compatible with modern Python versions.... Second, obtain an API key from the xAI Console at\n\n**console.x.ai**, which involves creating an account and generating a key under the API management section.\n\nFinally, install the xAI Python SDK using pip, which facilitates interaction with Grok 4’s endpoints.\n\n```\n\npip install xai-sdk\n\n```... Use the xAI SDK’s\n\n`Client` class to initialize a connection, specifying the\n\n`grok-4-0709` model or just\n\n`grok-4` for the latest stable release.\n\nSet parameters like\n\n`temperature=0.4` for controlled responses and\n\n`max_tokens=100` to limit output length, ensuring predictable results.... **Handling API Responses**\n\nWhen handling API responses, which Grok 4 returns in JSON format, you must parse the output to extract meaningful data, ensuring accurate interpretation of results.\n\nThe\n\n`response.content` field contains the generated text, while the\n\n`usage` object details token consumption, which is useful for monitoring rate limits.... Log these values in your application, as shown below, to monitor cumulative usage over multiple calls.\n\nThe xAI Console’s usage page provides an overview, but programmatic tracking, which I find precise, allows real-time adjustments. This code snippet logs token usage for analysis.... **Wrapping Up**\n\n**Wrapping Up**\n\nThroughout this tutorial, I have guided you through testing Grok 4 via the xAI API, demonstrating its capabilities in text generation, real-time data retrieval, and code execution, which showcase its technical versatility.\n\nBy following the structured steps, from setting up your environment to optimizing prompts and handling errors, you can effectively explore Grok 4’s potential, ensuring reliable and efficient interactions.... This process, which I find rewarding for its precision, equips you with the tools to integrate advanced AI into your projects.\n\nFor further exploration, consult the xAI documentation at\n\n`https://x.ai/api` to stay updated on API enhancements."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Training Dataset",
            "score": 1,
            "explanation": "Grok 4 is first pre-trained with a data recipe that includes publicly available Internet data, data produced by third-parties for xAI..."
          },
          {
            "name": "Training Data Processing",
            "score": 1,
            "explanation": "We perform data filtering procedures on the training data, such as de-duplication and classification, to ensure data quality and safety prior to training."
          },
          {
            "name": "Knowledge Count",
            "score": 1,
            "explanation": "The model has a deep domain knowledge in finance, healthcare, law, and science."
          }
        ]
      },
      {
        "sectionId": "model-implementation-sustainability",
        "sectionSnippets": [
          {
            "snippetId": "model-implementation-sustainability-snippet-1",
            "url": "https://data.x.ai/2025-08-20-grok-4-model-card.pdf",
            "snippet": "xAI\nLast updated: August 20, 2025\n1\nIntroduction\nGrok 4 is the latest reasoning model from xAI with advanced reasoning and tool-use capabilities,\nenabling it to achieve new state-of-the-art performance across challenging academic and industry\nbenchmarks. Because our models push the frontier of AI capabilities, we are committed to mitigating... involve different model behaviors. For example, a hypothetical terrorist group using AI to help\nsynthesize chemical weapons would require models that possess advanced scientific knowledge,\nwhereas a hypothetical rogue AI exfiltrating its weights requires models that can manipulate humans\nand hack systems.\nOur approach to safety evaluations focuses on measuring specific safety-relevant behaviors relevant to... different risk scenarios. We categorize these safety-relevant behaviors as: abuse potential (Section 2.1),\nconcerning propensities (Section 2.2), and dual-use capabilities (Section 2.3). This report describes\nour current evaluation methodology, results, and mitigations for these various behaviors.... In this document, we focus on the Grok 4 model. xAI deploys Grok 4 in both the consumer-facing\napplications (Grok 4 Web) and through an enterprise use-focused API (Grok 4 API). We report\nevaluations for Grok 4 API and Grok 4 Web now available to our customers, including in the EU.... Finally, we describe our training pipeline (Section 3.1) and additional transparency commitments\n(Section 3.2).\n2\nEvaluations\nOur approach to model evaluations varies depending on the specific behavior under assessment.\nTo reduce the potential for abuse of Grok 4 that might lead to serious injury to people, property or... of enabling malicious use. While the general cyber knowledge and exploitation capabilities of Grok\n4 are a significant step up from prior models, third-party testing shows that Grok 4’s end-to-end\noffensive cyber capabilities remain below the level of a human professional.\nAs of this writing, on top of our safety training, we have implemented a system prompt that provides... additional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 overall presents a low\nrisk for malicious use and loss of control.\n2.1... evaluation metric is response rate (i.e., the rate at which the model answered queries that should\nhave been refused) for all three evaluation settings: standard, user jailbreak, and system jailbreak.\nGrok 4 Web does not accept custom system prompts from users, so we do not evaluate with\nsystem jailbreaks.... Input filters.\nWe also employ model-based filters for both Grok 4 API and Grok 4 Web,\nwhich reject classes of harmful requests, including biological and chemical weapons, self-harm, and\nCSAM.\n2.2\nConcerning Propensities\nAI models may contain propensities that reduce their controllability, such as deception, power-seeking,... 38.4% and 22.1% on VCT, so both Grok 4 API and Grok 4 Web achieve superhuman performance\non identifying issues in biological protocols and wetlab virology experiments. Similarly, Grok 4\nAPI shows strong capabilities in both cybersecurity and chemistry. Note that these evaluations... 3\nTransparency\nTo mitigate catastrophic risks from AI, we provide to the public visibility to the development and\ndeployment of our frontier AI models. Transparency into AI progress can help developers coordinate\nsafety efforts, governments enact sensible legislation, and the public stay abreast of the benefits and\nrisks of AI. In an effort to increase visibility, we document our training process (Section 3.1) and our... system prompts (Section 3.2).\n3.1\nData and Training\nGrok 4 is first pre-trained with a data recipe that includes publicly available Internet data, data\nproduced by third-parties for xAI, data from users or contractors, and internally generated data. We\nperform data filtering procedures on the training data, such as de-duplication and classification, to... ensure data quality and safety prior to training. In addition to pre-training, our recipe uses a variety\nof reinforcement learning techniques—human feedback, verifiable rewards, and model grading—along\nwith supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-\nprompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n7... Li, Ann-Kathrin Dombrowski, Shashwat Goel, Gabriel Mukobi, et al. The wmdp benchmark:\nMeasuring and reducing malicious use with unlearning. In International Conference on Machine\nLearning, pages 28525–28550. PMLR, 2024."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-2",
            "url": "https://docs.x.ai/docs/models/grok-4-0709",
            "snippet": "## Models\n\n## /\n## Grok 4\n\nOur latest and greatest flagship model, offering unparalleled performance in natural language, math and reasoning - the perfect jack of all trades.\n\n#### At a glance\n\nModalities\n\nContext window\n\n256,000\n\nPricing\n\n#### Capabilities\n\nFunction calling\n\nConnect the xAI model to external tools and systems.\n\nStructured outputs\n\nReturn responses in a specific, organized formats.\n\nReasoning\n\nThe model thinks before responding.... #### Pricing\n\n#### Input\n\nTokens\n\n/ 1M tokens\n\nCached tokens\n\n/ 1M tokens\n\n#### Output\n\nTokens\n\n/ 1M tokens\n\nLive search\n\n/ 1K sources\n\nYou are charged for each token used when making calls to our API.\n\nUsing cached input tokens can significantly reduce your costs.... #### Details\n\nModel name\n\nAliases\n\n|Region|us-east-1|\n|--|--|\n|Pricing per million tokens *|\n|Input|\n|Cached input|\n|Output|\n|Per thousand sources|\n|Live search|\n|Rate limits|\n|Requests per minute|480|\n|Tokens per minute|2,000,000|\nWe charge different rates for requests which exceed the 128K context window... #### Quickstart\n\n```\n\nfrom xai_sdk import Client\n\nfrom xai_sdk.chat import user, system\n\nclient = Client(api_key=\"<YOUR_XAI_API_KEY_HERE>\")\n\nchat = client.chat.create(model=\"grok-4-0709\", temperature=0)\n\nchat.append(system(\"You are a PhD-level mathematician.\"))\n\nchat.append(user(\"What is 2 + 2?\"))\n\nresponse = chat.sample()\n\nprint(response.content)\n\n```... #### Quickstart\n\nCreate an API Key and make your first request.\n\n#### Tool use\n\nLet Grok perform actions and look up information.\n\n#### Structured outputs\n\nLet Grok generate structured outputs."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-3",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "## Available in These Regions\n\n**US East (Ashburn)**(on-demand only) **US Midwest (Chicago)**(on-demand only) **US West (Phoenix)**(on-demand only)\n\n**Important**\n\n**External Calls**\n\nThe xAI Grok models are hosted in an OCI data center, in a tenancy provisioned for xAI. The xAI Grok models, which can be accessed through the OCI Generative AI service, are managed by xAI.... ## Access this Model\n\n## Key Features\n\n🔗 **Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## On-Demand Mode\n\n\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\n\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... For large inputs with difficult problems, set a high value for the maximum output tokens parameter. See Troubleshooting.\n\n**Temperature**\n\n\n\nThe level of randomness used to generate the output text. Min: 0, Max: 2\n\n**Tip**\n\nStart with the temperature set to 0 or less than one, and increase the temperature as you regenerate the prompts for a more creative output. High temperatures can introduce hallucinations and factually incorrect information.... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response.... ## Troubleshooting\n\n\n\n**Issue:** The Grok 4 model doesn't respond.\n\n**Cause:** The **Maximum output tokens** parameter in the playground or the\n\n`max_tokens` parameter in the API is likely too low.\n\n**Action:** Increase the maximum output tokens parameter.\n\n**Reason:** For difficult problems that require reasoning and problem-solving, and for large sophisticated inputs, the\n\n`xai.grok-4` model tends to think and consumes many tokens, so if the\n\n`max_tokens` parameter is too low, the model uses the allocated tokens and doesn't return a final response."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-4",
            "url": "https://docs.public.content.oci.oraclecloud.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "# xAI Grok 4\n\nThe\n\n`xai.grok-4` model has better performance than its predecessor, Grok 3, and excels at enterprise use cases such as data extraction, coding, and summarizing text. This model has a deep domain knowledge in finance, healthcare, law, and science.... ## Available in These Regions\n\n**US East (Ashburn)**(on-demand only) **US Midwest (Chicago)**(on-demand only) **US West (Phoenix)**(on-demand only)\n\n**Important**\n\n**External Calls**\n\nThe xAI Grok models are hosted in an OCI data center, in a tenancy provisioned for xAI. The xAI Grok models, which can be accessed through the OCI Generative AI service, are managed by xAI.... ## Access this Model\n## Key Features\n\n**Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## On-Demand Mode\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... For large inputs with difficult problems, set a high value for the maximum output tokens parameter. See Troubleshooting.\n\n**Temperature**\n\n\n\nThe level of randomness used to generate the output text. Min: 0, Max: 2\n\n**Tip**\n\nStart with the temperature set to 0 or less than one, and increase the temperature as you regenerate the prompts for a more creative output. High temperatures can introduce hallucinations and factually incorrect information.... **Top p**\n\n\n\nA sampling method that controls the cumulative probability of the top tokens to consider for the next token. Assign\n\n`p`a decimal number between 0 and 1 for the probability. For example, enter 0.75 for the top 75 percent to be considered. Set\n\n`p`to 1 to consider all tokens.... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-5",
            "url": "https://www.datacamp.com/blog/grok-4",
            "snippet": "Course\n\nAfter we waited months for Grok 3.5, xAI skipped it altogether and jumped straight to Grok 4. (As of November 2025, Grok 4.1 has also launched).\n\nIs the jump justified by the model’s performance?\n\nYes, if you judge by the benchmarks. Right now, Grok 4 is probably the best model in the world on paper.... Source: xAI\n\nThe company claims it’s the most intelligent model available today, and benchmark results do point in that direction. The most notable result came from Humanity’s Last Exam, a benchmark made up of 2,500 hand-curated, PhD-level questions spanning math, physics, chemistry, linguistics, and engineering. Grok 4 (with tools) managed to solve about 38.6% of the problems.... It’s less clear how it performs in broader consumer workflows or how well it handles safety at scale. But xAI says it’s already in use at biomedical labs, financial firms, and early enterprise partners.... As Elon Musk pointed out in the livestream, Grok 4’s image understanding and generation still isn’t very advanced. If you want constant and reliable results, I think it’s fair to say that Grok 4 is a text-only model at the moment.... ## Grok 4 Benchmarks\n\nGrok 4’s main claim to fame is its performance across a wide mix of benchmarks, from academic exams to business simulations. According to xAI, the model improves significantly over previous versions thanks mostly to more compute—both during training and inference—not necessarily because of new architectural breakthroughs.... Source: xAI\n\nWith no tool use, Grok 4 plateaus at around 26.9% accuracy. With tools enabled (e.g., code execution), it hits 41.0%. And when run in its multi-agent “Heavy” configuration, it climbs to 50.7%—a major jump that’s over double the best prior tool-free model scores.... ### Vending-Bench (business simulation)\n\nxAI also tested Grok 4 in a real-world simulation called Vending-Bench. The idea is to see whether a model can manage a small business over time: restocking inventory, adjusting prices, contacting suppliers, etc. It’s a fairly new benchmark and a surprisingly fun one. We previously covered how it works in detail through a Claude Sonnet 3.7 case study in our weekly newsletter, The Median.... Source: xAI\n\nIn short: Grok 4 performs well where xAI has tested it. But as always, you should look beyond the leaderboard. The benchmarks are promising, but they don’t tell the whole story—especially if your use case depends on vision, code generation, or real-time interaction in messy environments.... ## How to Access Grok 4\n\nGrok 4 is now available through three main entry points: the X app, the xAI API, and the grok.com platform. Whether you want to chat with the model, build with it, or test its reasoning capabilities more formally, here’s how to get started.... ### Direct access via Grok.com\n\nYou can also use Grok 4 directly through grok.com, which offers a cleaner, standalone interface outside the X platform. It’s aimed at users who prefer a distraction-free setup.\n\n- Visit https://grok.com\n\n- Sign up and log in.\n\n- Access Grok 4 in a chat interface, with support for tools, code, and long context... ### Grok 4 API\n\nIf you want to integrate Grok into your own app or workflow, you can use the xAI API.\n\nSteps:\n\n- Go to https://x.ai/api and request developer access.\n\n- Once approved, you’ll receive an API key and access to docs.\n\n- Make sure to read through the documentation for more details.... ### August: Specialized coding model\n\nThe first follow-up is a coding-focused model expected in August. Unlike Grok 4, which is a generalist, this will be a specialized model designed to handle code with more speed and accuracy. xAI described it as “fast and smart,” trained specifically to improve both latency and reasoning in software development workflows.... ### October: Video generation model\n\nThe final release in the current timeline is a video generation model due in October. xAI says they’ll be training it on over 100,000 GPUs. Based on their remarks, this system will aim to produce high-quality, interactive, and editable video content.... For developers and researchers, it’s worth exploring. For casual users, the speed and responsiveness of Grok 3 or other mainstream models are a better fit. The roadmap is ambitious, with a coding model, multimodal agent, and video generator all due by October. Whether xAI can deliver those on time is another question. But with Grok 4, they’ve at least made a compelling case that they’re in the race."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Hardware Used During Training & Inference",
            "score": 1,
            "explanation": "Grok 4 improves significantly over previous versions thanks mostly to more compute—both during training and inference."
          },
          {
            "name": "Software Frameworks & Tooling",
            "score": 1,
            "explanation": "Grok 4 can be accessed through the xAI API, which allows integration into applications."
          },
          {
            "name": "Energy Use/ Sustainability Metrics",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      },
      {
        "sectionId": "intended-use",
        "sectionSnippets": [
          {
            "snippetId": "intended-use-snippet-1",
            "url": "https://docs.x.ai/docs/models/grok-4-0709",
            "snippet": "## Models\n\n## /\n## Grok 4\n\nOur latest and greatest flagship model, offering unparalleled performance in natural language, math and reasoning - the perfect jack of all trades.\n\n#### At a glance\n\nModalities\n\nContext window\n\n256,000\n\nPricing\n\n#### Capabilities\n\nFunction calling\n\nConnect the xAI model to external tools and systems.\n\nStructured outputs\n\nReturn responses in a specific, organized formats.\n\nReasoning\n\nThe model thinks before responding.... #### Pricing\n\n#### Input\n\nTokens\n\n/ 1M tokens\n\nCached tokens\n\n/ 1M tokens\n\n#### Output\n\nTokens\n\n/ 1M tokens\n\nLive search\n\n/ 1K sources\n\nYou are charged for each token used when making calls to our API.\n\nUsing cached input tokens can significantly reduce your costs.... #### Details\n\nModel name\n\nAliases\n\n|Region|us-east-1|\n|--|--|\n|Pricing per million tokens *|\n|Input|\n|Cached input|\n|Output|\n|Per thousand sources|\n|Live search|\n|Rate limits|\n|Requests per minute|480|\n|Tokens per minute|2,000,000|\nWe charge different rates for requests which exceed the 128K context window... #### Quickstart\n\n```\n\nfrom xai_sdk import Client\n\nfrom xai_sdk.chat import user, system\n\nclient = Client(api_key=\"<YOUR_XAI_API_KEY_HERE>\")\n\nchat = client.chat.create(model=\"grok-4-0709\", temperature=0)\n\nchat.append(system(\"You are a PhD-level mathematician.\"))\n\nchat.append(user(\"What is 2 + 2?\"))\n\nresponse = chat.sample()\n\nprint(response.content)\n\n```... #### Quickstart\n\nCreate an API Key and make your first request.\n\n#### Tool use\n\nLet Grok perform actions and look up information.\n\n#### Structured outputs\n\nLet Grok generate structured outputs."
          },
          {
            "snippetId": "intended-use-snippet-2",
            "url": "https://data.x.ai/2025-08-20-grok-4-model-card.pdf",
            "snippet": "xAI\nLast updated: August 20, 2025\n1\nIntroduction\nGrok 4 is the latest reasoning model from xAI with advanced reasoning and tool-use capabilities,\nenabling it to achieve new state-of-the-art performance across challenging academic and industry\nbenchmarks. Because our models push the frontier of AI capabilities, we are committed to mitigating... different risk scenarios. We categorize these safety-relevant behaviors as: abuse potential (Section 2.1),\nconcerning propensities (Section 2.2), and dual-use capabilities (Section 2.3). This report describes\nour current evaluation methodology, results, and mitigations for these various behaviors.... In this document, we focus on the Grok 4 model. xAI deploys Grok 4 in both the consumer-facing\napplications (Grok 4 Web) and through an enterprise use-focused API (Grok 4 API). We report\nevaluations for Grok 4 API and Grok 4 Web now available to our customers, including in the EU.... Finally, we describe our training pipeline (Section 3.1) and additional transparency commitments\n(Section 3.2).\n2\nEvaluations\nOur approach to model evaluations varies depending on the specific behavior under assessment.\nTo reduce the potential for abuse of Grok 4 that might lead to serious injury to people, property or... of enabling malicious use. While the general cyber knowledge and exploitation capabilities of Grok\n4 are a significant step up from prior models, third-party testing shows that Grok 4’s end-to-end\noffensive cyber capabilities remain below the level of a human professional.\nAs of this writing, on top of our safety training, we have implemented a system prompt that provides... additional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 overall presents a low\nrisk for malicious use and loss of control.\n2.1... Agentic abuse.\nGrok 4 introduces advanced reasoning and tool-calling capabilities that enable\nthe model to be used in an “agentic” manner, that is, repeatedly take actions toward a specified\ngoal. Such capabilities introduce additional risks of misuse beyond what is present in conversational\nsettings, such as executing real function calls. To quantify these risks, we use the AgentHarm\n2... Input filters.\nWe also employ model-based filters for both Grok 4 API and Grok 4 Web,\nwhich reject classes of harmful requests, including biological and chemical weapons, self-harm, and\nCSAM.\n2.2\nConcerning Propensities\nAI models may contain propensities that reduce their controllability, such as deception, power-seeking,... acquire, or use chemical and biological weapons or offensive cyber operations (e.g., troubleshooting\nvirology lab or reverse engineering binaries). We also measure the persuasiveness of our models\nwhen instructed to surreptitiously persuade another AI model, since more persuasive models can be\nabused to manipulate people at scale, and manipulate user behavior.... and chemistry, the cybersecurity questions in WMDP also measure different stages of a threat model,\nin particular the cyber killchain [Hutchins et al., 2011]. The questions span a variety of topics,\n5... such as knowledge of Metasploit, to understanding of vulnerability detection, to reverse engineering\nsimple binaries.\nCyber agents.\nWe also evaluate Grok 4’s agentic hacking capabilities on CyBench, a collection\nof 40 capture-the-flag-style questions which measures a model’s ability to perform cybersecurity\nchallenges [Zhang et al., 2025]. The model is placed in an agent harness which gives it access to... 38.4% and 22.1% on VCT, so both Grok 4 API and Grok 4 Web achieve superhuman performance\non identifying issues in biological protocols and wetlab virology experiments. Similarly, Grok 4\nAPI shows strong capabilities in both cybersecurity and chemistry. Note that these evaluations... system prompts (Section 3.2).\n3.1\nData and Training\nGrok 4 is first pre-trained with a data recipe that includes publicly available Internet data, data\nproduced by third-parties for xAI, data from users or contractors, and internally generated data. We\nperform data filtering procedures on the training data, such as de-duplication and classification, to... ensure data quality and safety prior to training. In addition to pre-training, our recipe uses a variety\nof reinforcement learning techniques—human feedback, verifiable rewards, and model grading—along\nwith supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-\nprompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n7"
          },
          {
            "snippetId": "intended-use-snippet-3",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "# xAI Grok 4\n\nThe\n\n`xai.grok-4` model has better performance than its predecessor, Grok 3, and excels at enterprise use cases such as data extraction, coding, and summarizing text. This model has a deep domain knowledge in finance, healthcare, law, and science.... ## Available in These Regions\n\n**US East (Ashburn)**(on-demand only) **US Midwest (Chicago)**(on-demand only) **US West (Phoenix)**(on-demand only)\n\n**Important**\n\n**External Calls**\n\nThe xAI Grok models are hosted in an OCI data center, in a tenancy provisioned for xAI. The xAI Grok models, which can be accessed through the OCI Generative AI service, are managed by xAI.... ## Access this Model\n\n## Key Features\n\n🔗 **Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## Limits\n\n\n\n- Image Inputs\n\n\n\n**Console:**Upload one or more\n\n`.png`or\n\n`.jpg`images, each 5 MB or smaller.\n\n**API:**Submit a\n\n`base64`encoded version of an image, ensuring that each converted image is more than 512 and less than 1,792 tokens. For example, a 512 x 512 image typically converts to around 1,610 tokens.... ## On-Demand Mode\n\n\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\n\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... **Top p**\n\n\n\nA sampling method that controls the cumulative probability of the top tokens to consider for the next token. Assign\n\n`p`a decimal number between 0 and 1 for the probability. For example, enter 0.75 for the top 75 percent to be considered. Set\n\n`p`to 1 to consider all tokens.... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response."
          },
          {
            "snippetId": "intended-use-snippet-4",
            "url": "https://docs.public.content.oci.oraclecloud.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "# xAI Grok 4\n\nThe\n\n`xai.grok-4` model has better performance than its predecessor, Grok 3, and excels at enterprise use cases such as data extraction, coding, and summarizing text. This model has a deep domain knowledge in finance, healthcare, law, and science.... ## Available in These Regions\n\n**US East (Ashburn)**(on-demand only) **US Midwest (Chicago)**(on-demand only) **US West (Phoenix)**(on-demand only)\n\n**Important**\n\n**External Calls**\n\nThe xAI Grok models are hosted in an OCI data center, in a tenancy provisioned for xAI. The xAI Grok models, which can be accessed through the OCI Generative AI service, are managed by xAI.... ## Access this Model\n## Key Features\n\n**Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## On-Demand Mode\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response.... ## Troubleshooting\n\n**Issue:** The Grok 4 model doesn't respond.\n\n**Cause:** The **Maximum output tokens** parameter in the playground or the\n\n`max_tokens` parameter in the API is likely too low.\n\n**Action:** Increase the maximum output tokens parameter.\n\n**Reason:** For difficult problems that require reasoning and problem-solving, and for large sophisticated inputs, the\n\n`xai.grok-4` model tends to think and consumes many tokens, so if the\n\n`max_tokens` parameter is too low, the model uses the allocated tokens and doesn't return a final response."
          },
          {
            "snippetId": "intended-use-snippet-5",
            "url": "https://docs.aimlapi.com/api-references/text-models-llm/xai/grok-4",
            "snippet": "This documentation is valid for the following model:\n\n`x-ai/grok-4-07-09`\n\n## Model Overview\n\nGrok 4 is boldly described by its developers as the most intelligent model in the world (as of July 2025).\n\n## How to Make a Call... ## Step-by-Step Instructions\n\n1️\n\n** Setup You Can’t Skip**\n\n▪️\n\n**Create an Account**: Visit the AI/ML API website and create an account (if you don’t have one yet).\n\n▪️ **Generate an API Key**: After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.... 2️\n\n** Copy the code example**\n\nAt the bottom of this page, you'll find a code example that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.\n\n3️\n\n** Modify the code example**\n\n▪️ Replace\n\n`<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.... ▪️ Insert your question or request into the\n\n`content` field—this is what the model will respond to.\n\n4️\n\n**(Optional)** ** Adjust other optional parameters if needed**\n\nOnly\n\n`model` and\n\n`messages` are required parameters for this model (and we’ve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model’s behavior. Below, you can find the corresponding API schema, which lists all available parameters along with notes on how to use them.... 5️\n\n** Run your modified code**\n\nRun your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.\n\nIf you need a more detailed walkthrough for setting up your development environment and making a request step by step — feel free to use our Quickstart guide.... ## API Schema\n\nAn upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.max_tokensnumber · min: 1Optional\n\nThe maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.streambooleanOptional... A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition.top_anumber · max: 1Optional\n\nAlternate top sampling parameter.tool_choiceany ofOptional... Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool. none is the default when no tools are present. auto is the default if tools are present.string · enumOptional... none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools.Possible values: orparallel_tool_callsbooleanOptional\n\nWhether to enable parallel function calling during tool use.logprobsboolean | nullableOptional... \"end_index\": 1,\n\n\"start_index\": 1,\n\n\"title\": \"text\",\n\n\"url\": \"text\"\n\n\n\n],\n\n\"audio\": {\n\n\"id\": \"text\",\n\n\"data\": \"text\",\n\n\"transcript\": \"text\",\n\n\"expires_at\": 1\n\n},\n\n\"tool_calls\": [\n\n\n\n\"id\": \"text\",... 1\n\n],\n\n\"logprob\": 1,\n\n\"token\": \"text\"\n\n\n\n],\n\n\"refusal\": []\n\n\n\n],\n\n\"model\": \"text\",\n\n\"usage\": {\n\n\"prompt_tokens\": 1,\n\n\"completion_tokens\": 1,\n\n\"total_tokens\": 1,\n\n\"completion_tokens_details\": {\n\n\"accepted_prediction_tokens\": 1,\n\n\"audio_tokens\": 1,\n\n\"reasoning_tokens\": 1,\n\n\"rejected_prediction_tokens\": 1\n\n},\n\n\"prompt_tokens_details\": {\n\n\"audio_tokens\": 1,\n\n\"cached_tokens\": 1\n\n\n\n```... \"logprobs\": null,\n\n\"message\": {\n\n\"role\": \"assistant\",\n\n\"content\": \"Hello! I'm Grok, built by xAI to help with answers, ideas, and a bit of cosmic wit. What can I do for you today? 🚀\",\n\n\"reasoning_content\": \"Thinking... Thinking...... \",\n\n\"refusal\": null\n\n\n\n],\n\n\"created\": 1752837143,\n\n\"model\": \"x-ai/grok-4\",\n\n\"usage\": {\n\n\"prompt_tokens\": 53,\n\n\"completion_tokens\": 5689,\n\n\"total_tokens\": 5742,\n\n\"prompt_tokens_details\": {\n\n\"cached_tokens\": 2\n\n},\n\n\"completion_tokens_details\": {\n\n\"reasoning_tokens\": 138\n\n\n\n```\n\nLast updated\n\nWas this helpful?"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Primary intended uses",
            "score": 1,
            "explanation": "Grok 4 excels at enterprise use cases such as data extraction, coding, and summarizing text."
          },
          {
            "name": "Primary intended users",
            "score": 1,
            "explanation": "Grok 4 is deployed in both the consumer-facing applications and through an enterprise use-focused API."
          },
          {
            "name": "Out-of-scope use cases",
            "score": 1,
            "explanation": "We categorize these safety-relevant behaviors as: abuse potential, concerning propensities, and dual-use capabilities."
          }
        ]
      },
      {
        "sectionId": "critical-risk",
        "sectionSnippets": [
          {
            "snippetId": "critical-risk-snippet-1",
            "url": "https://data.x.ai/2025-08-20-grok-4-model-card.pdf",
            "snippet": "their risks through both evaluating model behaviors and implementing safeguards.\nFollowing our Risk Management Framework (RMF), we aim to reduce the risk of severe, large-scale\nharms to people, property, and society from AI. The two primary categories of risk we consider are\nrisks from either malicious use or loss of control. Different risk scenarios within these categories... involve different model behaviors. For example, a hypothetical terrorist group using AI to help\nsynthesize chemical weapons would require models that possess advanced scientific knowledge,\nwhereas a hypothetical rogue AI exfiltrating its weights requires models that can manipulate humans\nand hack systems.\nOur approach to safety evaluations focuses on measuring specific safety-relevant behaviors relevant to... different risk scenarios. We categorize these safety-relevant behaviors as: abuse potential (Section 2.1),\nconcerning propensities (Section 2.2), and dual-use capabilities (Section 2.3). This report describes\nour current evaluation methodology, results, and mitigations for these various behaviors.... Finally, we describe our training pipeline (Section 3.1) and additional transparency commitments\n(Section 3.2).\n2\nEvaluations\nOur approach to model evaluations varies depending on the specific behavior under assessment.\nTo reduce the potential for abuse of Grok 4 that might lead to serious injury to people, property or... national security interests, we take measures to improve Grok 4’s robustness, such as by adding\nsafeguards to refuse requests that may lead to foreseeable harm, especially for requests that lower\nthe barriers to developing chemical, biological, radiological, nuclear (CBRN) or cyber weapons, along\nwith requests for self-harm and child sexual abuse material (CSAM) (Section 2.1). In addition to\nrefusals, we also assess Grok 4’s robustness to adversarial requests which attempt to circumvent\nour safeguards (e.g., jailbreaks and prompt injections).\n1... of enabling malicious use. While the general cyber knowledge and exploitation capabilities of Grok\n4 are a significant step up from prior models, third-party testing shows that Grok 4’s end-to-end\noffensive cyber capabilities remain below the level of a human professional.\nAs of this writing, on top of our safety training, we have implemented a system prompt that provides... additional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 overall presents a low\nrisk for malicious use and loss of control.\n2.1... malignant intent from mere curiosity. We define a basic refusal policy which instructs Grok 4 to\ndecline queries demonstrating clear intent to engage in activities that threaten severe, imminent harm\nto others, including violent crimes, child sexual exploitation, fraud, hacking, and more. We place\nfurther emphasis on refusing requests concerning the development of CBRN or cyber weapons.... Input filters.\nWe also employ model-based filters for both Grok 4 API and Grok 4 Web,\nwhich reject classes of harmful requests, including biological and chemical weapons, self-harm, and\nCSAM.\n2.2\nConcerning Propensities\nAI models may contain propensities that reduce their controllability, such as deception, power-seeking,... such as knowledge of Metasploit, to understanding of vulnerability detection, to reverse engineering\nsimple binaries.\nCyber agents.\nWe also evaluate Grok 4’s agentic hacking capabilities on CyBench, a collection\nof 40 capture-the-flag-style questions which measures a model’s ability to perform cybersecurity\nchallenges [Zhang et al., 2025]. The model is placed in an agent harness which gives it access to... 38.4% and 22.1% on VCT, so both Grok 4 API and Grok 4 Web achieve superhuman performance\non identifying issues in biological protocols and wetlab virology experiments. Similarly, Grok 4\nAPI shows strong capabilities in both cybersecurity and chemistry. Note that these evaluations... -\nCyBench\nunguided success rate\n0.43\n-\nTable 3: Dual-use capabilities evaluations.\n2.3.3\nMitigations\nDue to Grok 4’s strong dual-use biological capabilities, we have deployed narrow, topically-focused\nfilters across all product surfaces as an additional safeguard against bioweapons-related abuse.... ensure data quality and safety prior to training. In addition to pre-training, our recipe uses a variety\nof reinforcement learning techniques—human feedback, verifiable rewards, and model grading—along\nwith supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-\nprompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n7"
          },
          {
            "snippetId": "critical-risk-snippet-2",
            "url": "https://data.x.ai/2025-09-19-grok-4-fast-model-card.pdf",
            "snippet": "Grok 4 Fast was pre-trained on a general purpose data corpus, then post-trained on various tasks\nand tool use, as well as demonstrations of correct refusal behaviors according to our default safety\npolicy. We also deploy Grok 4 Fast in our API with a fixed system prompt prefix that reminds\nthe model of our safety policy, in addition to input filters to safeguard against abuse.... Prior to release, we have evaluated various specific safety-relevant behaviors of Grok 4 Fast: abuse\npotential (Section 2.1), concerning propensities (Section 2.2), and dual-use capabilities (Section 2.3).\nIn this report, we describe our current evaluation methodology, results, and any mitigations for these... various behaviors in Grok 4 Fast. All evaluations in this report were conducted on a near-final\nrelease checkpoint.\n2\nEvaluations\nOur approach to model evaluations varies depending on the specific behavior under assessment.\nTo reduce the potential for abuse of Grok 4 Fast that might lead to serious injury to people,... property or national security interests, we apply safety training to reduce the risks of misuse and\nrefuse requests that may lead to foreseeable harm, especially for requests that lower the barriers to\ndeveloping chemical, biological, radiological, nuclear (CBRN) or cyber weapons, along with requests\nfor self-harm and child sexual abuse material (CSAM) (Section 2.1). In addition to refusals, we... As of this writing, on top of our safety training, we have implemented a system prompt that provides\nadditional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 Fast overall presents... • Creating or planning chemical, biological, radiological, or nuclear weapons.\n• Conducting cyber attacks, including ransomware and DDoS attacks.\nWe instruct the model not to answer queries that demonstrate clear intent to engage in these activities\nwithin a safety system prompt that is injected before all conversational contexts. Users may specify\ntheir own system message, and its content will be appended to the safety system prompt. Users may... imminent harm to others, including violent crimes, child sexual exploitation, fraud, hacking, and\nmore. We place further emphasis on refusing requests concerning the development of CBRN or cyber\nweapons.\nSystem Prompt.\nWith Grok 4 Fast’s strong reasoning and instruction-following capabilities,\nwe find that including our basic refusal policy in the system prompt greatly reduces response rate on... harmful queries. Additionally, warning the model against jailbreak attacks serves to significantly\ninoculate against common jailbreak strategies.\nInput filters.\nWe also employ model-based filters for Grok 4 Fast, which reject classes of harmful\nrequests, including biological and chemical weapons, self-harm, and CSAM.\n2.2... to model truthfulness, we recommend developers operate Grok 4 Fast with reasoning enabled and\ninclude instructions to respond truthfully.\n2.3\nDual-use Capabilities\nIn this section, we evaluate the possibility of our model enabling malicious actors to design, synthesize,\nacquire, or use chemical and biological weapons or offensive cyber operations (e.g., troubleshooting\nvirology lab or reverse engineering binaries). We also measure the persuasiveness of our models\n4... for bioweapons, but also covers cybersecurity and chemical knowledge. We prioritize addressing\nbioweapons risks over others because they have the potential for the greatest scale of harm, and\nfrontier models significantly lower the barrier to entry to the creation of bioweapons [Brent and\nMcKelvey, 2025]. For all datasets, we only assess performance on text-only questions.... in particular the cyber killchain [Hutchins et al., 2011]. The questions span a variety of topics,\nsuch as knowledge of Metasploit, to understanding of vulnerability detection, to reverse engineering\nsimple binaries.\nCyber agents.\nWe also evaluate Grok 4 Fast’s agentic hacking capabilities on CyBench,... Table 3: Dual-use capabilities evaluations.\n2.3.3\nMitigations\nOur narrow, topically-focused filters remain deployed across all product surfaces as an additional\nsafeguard against chemical and biological weapons-related abuse. Our assessments of autonomous\nhacking, radiological, and nuclear abuse risks remain unchanged from that of Grok 4.... to ensure data quality and safety prior to training. In addition to pre-training, our recipe uses\na variety of reinforcement learning techniques—human feedback, verifiable rewards, and model\ngrading—along with supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-... prompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n6"
          },
          {
            "snippetId": "critical-risk-snippet-3",
            "url": "https://aisafetyclaims.org/companies/xai",
            "snippet": "# xAI\n\n## Eval and safeguard planning\n# Grok 4\n\nReleased Jul 9, 2025\n\nModel card published Aug 20, 2025 (and quietly changed later)\n\nTraining and internal deployment: dates not reported... xAI said \"Grok 4 is the most intelligent model in the world\"; moreover, \"Grok 4 represents a leap in frontier intelligence.\" Initially, it didn't say anything about safety and its misuse safeguards were absurdly weak. A month later, it published stuff on safety. It said Grok 4 has dangerous chem/bio capabilities and safeguards are necessary.... It also said Grok 4 doesn't pose large misalignment risks; this happens to be true but xAI's thinking on misalignment is profoundly confused and the metric it uses has nothing to do with misalignment risk.Elicitation checklist:❌ ❌ ❌ ❌Accountability checklist:❌ ❌... ## Grok 4 evaluation categories\n\nClick on a category to see my analysis and the relevant part of the company's eval report.\n\n### Chem & bio\n\nxAI does some simple evals and says Grok 4 has dangerous capabilities.\n\n### Cyber\n\nxAI does some simple evals and says Grok 4 lacks dangerous capabilities.... ### AI R&D\n\nNone\n\n### Scheming capabilities\n\nNone\n\n### Misalignment propensity\n\nxAI does evals to measure (1) propensity to lie when pressured to do so by the system prompt or user and (2) sycophancy. These aren't really relevant to misalignment risk.... ## Grok 4 safety categories\n\nThis section is new and experimental. Click on a category to see my analysis. Preventing misuse via API (i.e., when the user doesn't control the model weights) is relatively straightforward, especially for bio; scheming risk prevention will eventually be important and improving it from current levels is tractable; security will eventually be important and we basically know what to do but it's very costly. I'm interested in feedback, especially on whether this content is helpful to you or what questions you wish I answered here instead.... ### Misuse (via API) prevention\n\nxAI's plan is that if models have dangerous capabilities, it will ensure that the \"answer rate [is] less than 1 out of 20 on restricted queries.\" It claims to be meeting this standard for chem/bio weapons development. Unfortunately, it doesn't say anything about red-teaming its mitigations and they seem unlikely to be effective in the real world.... ### Scheming risk prevention\n\n\"Our risk acceptance criteria for system deployment is maintaining a dishonesty rate of less than 1 out of 2 on MASK. We plan to add additional thresholds tied to other benchmarks.\" Unfortunately, MASK has almost nothing to do with catastrophic misalignment risk, and upfront benchmarking is not a good approach to misalignment risk.\n### Security\n\nxAI claims to have good security. This is not credible."
          },
          {
            "snippetId": "critical-risk-snippet-4",
            "url": "https://splx.ai/blog/grok-4-security-testing",
            "snippet": "On\n\n**July 9th 2025,** xAI released Grok 4 as its new flagship language model. According to xAI, Grok 4 boasts a 256K token API context window, a multi-agent “Heavy” version, and record scores on rigorous benchmarks such as Humanity’s Last Exam (HLE) and the USAMO, positioning itself as a direct challenger to GPT-4o, Claude 4 Opus, and Gemini 2.5 Pro. **So, the SplxAI Research Team put Grok 4 to the test against GPT-4o**.... Grok 4’s recent antisemitic meltdown on X shows why every organization that embeds a large-language model (LLM) needs a standing red-team program. These models should never be used without rigorous evaluation of their safety and misuse risks—that's precisely what our research aims to demonstrate.... ## Grok Achievements\n\nBelow is a SplxAI-style introduction that combines the model’s headline achievements with a clear look at subscription and API pricing.... ## Testing Methodology\n\nThe following section explains how we conducted the security assessment. We began by crafting a system prompt tailored for an AI chatbot operating in the finance domain. Next, we applied our proprietary Prompt Hardening tool to enhance the prompt’s robustness against a wide range of attack types. In the final step, we used Probe to execute more than 1,000 distinct attack scenarios across various categories.... ||||\n|--|--|--|\n|Level 1 – No System Prompt|Raw model, “blank slate,” user messages only|Hobbyist scripts, mis-configured dev keys|\n|Level 2 – Basic System Prompt|A concise guard-rail similar to what the average SaaS team deploys|Standard production chatbot|\n|Level 3 – Hardened System Prompt|System prompt auto-strengthened by SplxAI’s|Best-practice enterprise deployment|\nThe Prompt-Hardening tool iterates over probe failures, rewrites the system prompt and retests until the model resists the original attacks.... ## Results\n\nWith no system prompt in place, Grok 4 underperforms in every category. The first thing we found is that Grok without a system prompt is not suitable for enterprise usage, it was really easy to jailbreak and it was generating harmful content with very descriptive and detailed responses.... |Grok 4 – Hardened SP|93.60|100.00|98.20|... When we ran our very first baseline test with both models left entirely unsupervised, the visual gap was impossible to miss. The orange columns towering over the barely perceptible yellow ones show that GPT-4o, while far from perfect, keeps a basic grip on security- and safety-critical behavior, whereas Grok 4 all but collapses. In practice, this means a simple, single-sentence user message can pull Grok into disallowed territory with no resistance at all – a serious concern for any enterprise that must answer to compliance teams, regulators, and customers.... All SplxAI enterprise clients get full access to Grok 4 and other model red-teaming reports – included in the license. Want to see how your model stacks up?... ## Key Takeaways\n\n*Raw Grok is a security liability.*With no system prompt, it leaked restricted data and obeyed hostile instructions in over 99% of our injection attempts. (SplxAI internal test data) *Basic guardrails fix most “easy” failures but still leave a double-digit business-alignment gap.* *Prompt hardening closes the gap,*lifting business-alignment to a 98% pass-rate and slightly improving security.... ## Pre-Deployment LLM Security Checklist\n\n**For CISOs, Red Team Leads & Product Owners**\n\nBefore shipping your AI assistant, ask:\n\n#### 1. Has the raw model been red-teamed?\n\nRun >1,000 adversarial probes across jailbreak, bias, safety, hallucination, and business alignment vectors.... #### 2. Is your system prompt hardened?\n\nHas it been iteratively stress-tested and reinforced using prior failures (canary traps, negation layering, goal obfuscation, etc.)?\n\n#### 3. Have you validated across modalities?\n\nTested for multi-turn, context-based, and tool-augmented exploits – not just one-shot prompts.... #### 4. Are business-alignment KPIs tracked?\n\nMeasuring more than hallucination or PII leakage – does the model respond in-brand, on-policy, and within intended scope?\n\n#### 5. Do you have regression coverage?\n\nWill you catch it if a model update reintroduces unsafe behavior (e.g., what happened with Grok)?... #### 6. Are results mapped to frameworks?\n\nOWASP LLM Top 10, MITRE ATLAS, NIST AI RMF – alignment with these should be explicit, not assumed.\n\n#### 7. Is your deployment auditable?\n\nCan your security and compliance teams inspect prompts, defenses, and test coverage without relying on devs?\n\nTable of contents"
          },
          {
            "snippetId": "critical-risk-snippet-5",
            "url": "https://www.scribd.com/document/844599804/2025-02-20-RMF-Draft",
            "snippet": "TPurpose and Scope his draft framework outlines xAI’s approach to policies for managing significant risksTassociated with the development, deployment, and release of our future AI systems not currently in development, such as future versions of Grok. (For simplicity, we refer to all such AF future systems as “Grok” below.)... This draft framework addresses two major categories of AI risk—malicious use and loss of control—and outlines the quantitative thresholds, metrics, and procedures that could be used to manage and improve the safety of AI systems. In addition, this draft framework discusses potential ways to address operational and societal risks posed by advanced AI, such as with public transparency, third-party review, and information security.... nder this draft risk management framework, Grok would apply heightened safeguards if itUreceives requests that pose a foreseeable and non-trivial risk of resulting in large-scale violence, terrorism, or the use, development, or proliferation of weapons of mass destruction, including CBRN weapons, and major cyber weapons on critical infrastructure.... For example, Grok would apply heightened safeguards if it receives a request to act as an agent or tool of mass violence, or if it receives requests for step-by-step instructions for committing mass violence. In this draft framework, we particularly focus on requests that pose a foreseeable and non-trivial risk of more than one hundred deaths or over $1 billion in damages from weapons of mass destruction or cyberterrorist attacks on critical infrastructure (“catastrophic malicious use events”). However, we will allow Grok to respond to such requests from some vetted, highly trusted users (such as trusted third-party safety auditors) whom we know to be using those... 1.Approach to Benchmarking o transparently measure Grok’s safety properties, we intend to utilize benchmarks like WMDTand Catastrophic Harm Benchmarks. Such benchmarks could be used to measure Grok’s dual-use capability and resistance to facilitating large-scale violence, terrorism, or the use, development, or proliferation of weapons of mass destruction (including chemical, biological, radiological, nuclear, and major cyber weapons).... e intend to regularly evaluate the adequacy and reliability of such benchmarks, including byWcomparing them against other benchmarks that we could potentially utilize.\n\n2.Implementation e want Grok to comply with its guiding principles, robustly resisting attempted manipulationWand adversarial attacks. We train Grok to robustly refuse to comply with requests to provide assistance with highly injurious malicious use.... T xamples of safeguards or mitigations we may potentially utilize to achieve our safety objectivesEinclude:\n\n● C ircuit breakers: Using representation engineeringto interrupt model representations responsible for hazardous outputs.5 AF ● Input and output filters:Applying classifiers touser inputs or model outputs to verify safety when Grok is queried regarding weapons of mass destruction or cyberterrorism.... e intend to design into Grok adequate safeguards prior to releasing it for general availability.WWe intend to choose the benchmarks and set the thresholds for reducing malicious use risks in a future version of the risk management framework.\n\nmultiple-response)\n\nMDP-Bio: multiple-choice W % X 82.2% by GPT-4 (zero-shot) questions on proxies of hazardous (e.g. 65%) biosecurity knowledge... 4 Cybench: A Framework for Evaluating CybersecurityCapabilities and Risks of Language Models5 Improving Alignment and Robustness with Circuit Breakers\n\n3 AB-Bench6 ProtocolQA: L % X 9% by human experts 7 multiple-choice questions on (e.g. 50%) (multiple-choice) debugging modified biological lab protocols... s an additional measure to enhance safety, we will subject Grok to adversarially testing itsAsafeguards utilizing both internal and qualified external red teams. Potentially, we will also explore incentive mechanisms like bounties as another mechanism to further improve Grok’s R safeguards.\n\n1.Background ur aim is to design safeguards into Grok to avoid losing control and thereby avoid unintendedOcatastrophic outcomes when Grok is used. Currently, it is recognized that some properties of an AI system that may reduce controllability include deception, power-seeking, fitness... e intend to regularly evaluate the adequacy and reliability of such benchmarks for bothWinternal and external deployments, including by comparing them against other benchmarks that we could potentially utilize. We may revise this list of benchmarks periodically as relevant benchmarks for loss of control are created.\n\n3.Thresholds R e aim to train Grok to be honest and have values conducive to controllability. We intend toWdesign into Grok adequate safeguards prior to broad internal or external deployment.... 7 6.Deployment Decisions o mitigate risks, we intend to utilize tiered availability of the functionality and features of Grok.TFor instance, the full functionality of a future Grok could be made available only to trusted parties, partners, and government agencies. We could also mitigate risks by adding additional controls on functionality and features depending on the end user (e.g., consumers using mobile apps vs. sophisticated businesses using APIs)."
          }
        ],
        "subsectionChecks": [
          {
            "name": "CBRN (Chemical, Biological, Radiological or Nuclear)",
            "score": 1,
            "explanation": "Grok 4's safeguards are necessary due to its dangerous chem/bio capabilities."
          },
          {
            "name": "Cyber Risk",
            "score": 1,
            "explanation": "Grok 4's end-to-end offensive cyber capabilities remain below the level of a human professional."
          },
          {
            "name": "Harmful Manipulation",
            "score": 1,
            "explanation": "We define a basic refusal policy which instructs Grok 4 to decline queries demonstrating clear intent to engage in activities that threaten severe, imminent harm."
          },
          {
            "name": "Child Safety Evaluations",
            "score": 1,
            "explanation": "We place further emphasis on refusing requests concerning the development of CBRN or cyber weapons, along with requests for self-harm and child sexual abuse material (CSAM)."
          },
          {
            "name": "Privacy Risks",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      },
      {
        "sectionId": "safety-evaluation",
        "sectionSnippets": [
          {
            "snippetId": "safety-evaluation-snippet-1",
            "url": "https://data.x.ai/2025-08-20-grok-4-model-card.pdf",
            "snippet": "involve different model behaviors. For example, a hypothetical terrorist group using AI to help\nsynthesize chemical weapons would require models that possess advanced scientific knowledge,\nwhereas a hypothetical rogue AI exfiltrating its weights requires models that can manipulate humans\nand hack systems.\nOur approach to safety evaluations focuses on measuring specific safety-relevant behaviors relevant to... different risk scenarios. We categorize these safety-relevant behaviors as: abuse potential (Section 2.1),\nconcerning propensities (Section 2.2), and dual-use capabilities (Section 2.3). This report describes\nour current evaluation methodology, results, and mitigations for these various behaviors.... We also reduce various propensities of Grok 4 that might make it difficult to control, such as\nbeing deceptive, power-seeking, manipulative, or biased, among others (Section 2.2). To achieve this,\nour main focus is on measuring and reducing the rate at which Grok 4 responds deceptively. We... of enabling malicious use. While the general cyber knowledge and exploitation capabilities of Grok\n4 are a significant step up from prior models, third-party testing shows that Grok 4’s end-to-end\noffensive cyber capabilities remain below the level of a human professional.\nAs of this writing, on top of our safety training, we have implemented a system prompt that provides... additional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 overall presents a low\nrisk for malicious use and loss of control.\n2.1... Abuse Potential\nPrevious generations of Grok exhibited two undesirable behaviors which increased abuse potential:\na willingness to facilitate serious criminal activity, and susceptibility to hijacking via injected\ninstructions. To improve robustness, we applied measures to refuse requests that may lead to\nforeseeable harm and to prevent adversarial requests from circumventing our safeguards. We have... Agentic abuse.\nGrok 4 introduces advanced reasoning and tool-calling capabilities that enable\nthe model to be used in an “agentic” manner, that is, repeatedly take actions toward a specified\ngoal. Such capabilities introduce additional risks of misuse beyond what is present in conversational\nsettings, such as executing real function calls. To quantify these risks, we use the AgentHarm\n2... System Prompt.\nWith Grok 4’s strong reasoning and instruction-following capabilities, we find\nthat including our basic refusal policy in the system prompt greatly reduces response rate on harmful\nqueries. Additionally, warning the model against jailbreak attacks serves to significantly inoculate\nagainst common jailbreak strategies.\n3... Input filters.\nWe also employ model-based filters for both Grok 4 API and Grok 4 Web,\nwhich reject classes of harmful requests, including biological and chemical weapons, self-harm, and\nCSAM.\n2.2\nConcerning Propensities\nAI models may contain propensities that reduce their controllability, such as deception, power-seeking,... manipulation, and sycophancy, etc. For Grok 4, we focus on minimizing both the rate at which it\nlies, its political biases, and its ability to manipulate users. Similar to robustness against potential\nabuse, we find that our safeguards are able to greatly reduce AI propensities that may lead to loss of... Our primary safeguard for mitigating concerning propensities to add explicit instructions to avoid\nthese behaviors in the system prompt, leveraging the model’s instruction-following. Overall, we find\nthat adding the system prompt sharply reduces rates of deception and political bias.\n2.3\nDual-use Capabilities\nIn this section, we evaluate the possibility of our model enabling malicious actors to design, synthesize,... such as knowledge of Metasploit, to understanding of vulnerability detection, to reverse engineering\nsimple binaries.\nCyber agents.\nWe also evaluate Grok 4’s agentic hacking capabilities on CyBench, a collection\nof 40 capture-the-flag-style questions which measures a model’s ability to perform cybersecurity\nchallenges [Zhang et al., 2025]. The model is placed in an agent harness which gives it access to... Similarly, given Grok 4’s strong chemical knowledge, we also deployed filters for chemical weapons-\nrelated abuse. Specifically, we filter for detailed information or substantial assistance regarding the\ncritical steps identified in Section 2 of our RMF. For cyber risks, we assess that Grok’s enforcement\n6... ensure data quality and safety prior to training. In addition to pre-training, our recipe uses a variety\nof reinforcement learning techniques—human feedback, verifiable rewards, and model grading—along\nwith supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-\nprompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n7"
          },
          {
            "snippetId": "safety-evaluation-snippet-2",
            "url": "https://data.x.ai/2025-11-17-grok-4-1-model-card.pdf",
            "snippet": "xAI\nNovember 17, 2025\n1\nIntroduction\nGrok 4.1 is a new model featuring more natural, fluid dialogue while maintaining strong core\nreasoning capabilities. It is publicly available through our web and mobile consumer apps.\nAs an update to Grok 4 and Grok 3, we engage in pre-deployment safety testing largely similar to... that described in the Grok 4 model card. In line with our Risk Management Framework (RMF), we\nmeasure safety-relevant behaviors across three categories: abuse potential, concerning propensities,\nand dual-use capabilities. This report describes our evaluation methodology, results, and mitigations\nfor these behaviors.\nGrok 4.1 is available in two configurations: Grok 4.1 Non-Thinking (Grok 4.1 NT), which... filter model. Finally, we discuss our dual-use capability evaluations.\n2\nEvaluations\nIn line with the risk categories outlined in our Risk Management Framework [xAI, 2025], we group our\nevaluations into three categories: potential for abuse (Section 2.1), concerning behavioral propensities\n(Section 2.2), and dual-use capabilities (Section 2.3).... 2.1\nAbuse Potential\nIn this section, we measure Grok 4.1’s ability to refuse violative requests, even under adversarial\nmanipulation.\n2.1.1\nSafety Training Approach\nRefusals. Our refusal policy centers on refusing requests with a clear intent to violate the law,... without over-refusing sensitive or controversial queries. To implement our refusal policy, we train\nGrok 4.1 on demonstrations of appropriate responses to both benign and harmful queries. As\nan additional mitigation, we employ input filters to reject specific classes of sensitive requests,\nsuch as those involving bioweapons, chemical weapons, self-harm, and child sexual abuse material... (CSAM). We train our filters with a mix of synthetic and production data and also leverage Grok to\nsystematically apply different adversarial attacks.\n2.1.2\nEvaluations\nRefusals. For the underlying model, we reuse our refusal evaluation from the Grok 4 and Grok 4\nFast model cards. This refusal evaluation is an internal dataset of single-turn requests that violate\nour safety policy. We then use a separate model to grade whether Grok 4.1 assisted or refused the\n1... 0.02\n0.00\nAgentic Refusals\nAgentHarm\nanswer rate\n0.14\n0.04\nPrompt Injection\nAgentDojo\nattack success rate\n0.05\n0.01\nTable 1: Malicious use evaluations for Grok 4.1.... which are not directly comparable to previous results.\nIn addition, we evaluate refusal rates in an agentic setting using AgentHarm (without jailbreaks), where\nmodels are asked to perform explicitly malicious tasks such as fraud, cybercrime, and harrassment.\nInput filters. For our input filters, we measure their refusal rate on an internal dataset of single-turn... prompts seeking restricted chemical and biological knowledge.\nAdversarial robustness. For the underlying model, we evaluate Grok 4.1 with an internal dataset\nof single-turn jailbreak templates and measure whether the jailbreaks cause the model to answer\nrequests it previously would have refused. To measure robustness in an agentic setting, we use... and in Table 2, we report our input filter’s false negative rate to queries seeking restricted knowledge.\nOverall, we find that Grok 4.1 refuses almost all harmful requests in chat mode, even under\nadversarial attack. On AgentHarm, Grok 4.1 refuses most requests, although we will continue to... explore additional mitigations, such as real-time safety monitoring. Finally, our input filter refuses\nalmost all direct requests about restricted chemical and biological knowledge.\nAdversarial robustness. In Table 1, we also report Grok 4.1’s response rate to adversarial attacks,\nand in Table 2 we report the input filter’s false negative rate on prompt injection attacks. We find... cyber operations, e.g., troubleshooting virology lab or reverse engineering binaries.\nWe also measure its persuasiveness, which is dual-use because it both enables models to be more\nengaging and increases their ability to manipulate user’s behavior.\nFor all evaluations, we report results with Grok 4.1 Thinking.... baselines on multi-modal and multi-step reasoning benchmarks such as FigQA and CloningScenarios.\nHowever, we note that the human baselines collected in previous work likely underestimate the\n4... reinforcement learning on human feedback, verifiable rewards, and model-based graders for safety\ntraining and for specific capabilities.\nReferences\nRoger Brent and T Greg McKelvey Jr. Contemporary ai foundation models increase biological\nweapons risk. arXiv preprint arXiv:2506.13798, 2025."
          },
          {
            "snippetId": "safety-evaluation-snippet-3",
            "url": "https://splx.ai/blog/grok-4-security-testing",
            "snippet": "On\n\n**July 9th 2025,** xAI released Grok 4 as its new flagship language model. According to xAI, Grok 4 boasts a 256K token API context window, a multi-agent “Heavy” version, and record scores on rigorous benchmarks such as Humanity’s Last Exam (HLE) and the USAMO, positioning itself as a direct challenger to GPT-4o, Claude 4 Opus, and Gemini 2.5 Pro. **So, the SplxAI Research Team put Grok 4 to the test against GPT-4o**.... Grok 4’s recent antisemitic meltdown on X shows why every organization that embeds a large-language model (LLM) needs a standing red-team program. These models should never be used without rigorous evaluation of their safety and misuse risks—that's precisely what our research aims to demonstrate.... ## Grok Achievements\n\nBelow is a SplxAI-style introduction that combines the model’s headline achievements with a clear look at subscription and API pricing.... ## Testing Methodology\n\nThe following section explains how we conducted the security assessment. We began by crafting a system prompt tailored for an AI chatbot operating in the finance domain. Next, we applied our proprietary Prompt Hardening tool to enhance the prompt’s robustness against a wide range of attack types. In the final step, we used Probe to execute more than 1,000 distinct attack scenarios across various categories.... ||||\n|--|--|--|\n|Level 1 – No System Prompt|Raw model, “blank slate,” user messages only|Hobbyist scripts, mis-configured dev keys|\n|Level 2 – Basic System Prompt|A concise guard-rail similar to what the average SaaS team deploys|Standard production chatbot|\n|Level 3 – Hardened System Prompt|System prompt auto-strengthened by SplxAI’s|Best-practice enterprise deployment|\nThe Prompt-Hardening tool iterates over probe failures, rewrites the system prompt and retests until the model resists the original attacks.... When we ran our very first baseline test with both models left entirely unsupervised, the visual gap was impossible to miss. The orange columns towering over the barely perceptible yellow ones show that GPT-4o, while far from perfect, keeps a basic grip on security- and safety-critical behavior, whereas Grok 4 all but collapses. In practice, this means a simple, single-sentence user message can pull Grok into disallowed territory with no resistance at all – a serious concern for any enterprise that must answer to compliance teams, regulators, and customers.... All SplxAI enterprise clients get full access to Grok 4 and other model red-teaming reports – included in the license. Want to see how your model stacks up?... The next figure tells a more hopeful story: a single, thoughtfully written instruction set raises Grok’s scores from near-zero to high-90s. Add SplxAI’s automated hardening layer on top and the model all but closes the gap with human expectations. Two lessons jump out. First, Grok is\n\n*capable* of acting responsibly – it just needs strict marching orders. Second, the distance between chaos and control can be as small as a few dozen lines of text, as long as they are crafted and iterated with adversarial feedback in mind.... Only after passing the prompt through our hardening engine did the dashboard turn a reassuring sea of green. Here Grok consistently refuses disallowed requests, avoids self-contradiction, and steers the conversation back to safe ground even when bombarded with layered exploits. The takeaway is not that Grok suddenly became perfect (no frontier model is) but that disciplined, iterative red-teaming paired with automated prompt reinforcement can transform a liability into a tool that meets enterprise risk thresholds. Skipping that step is what leaves the door open to headline-grabbing failures.... ## Pre-Deployment LLM Security Checklist\n\n**For CISOs, Red Team Leads & Product Owners**\n\nBefore shipping your AI assistant, ask:\n\n#### 1. Has the raw model been red-teamed?\n\nRun >1,000 adversarial probes across jailbreak, bias, safety, hallucination, and business alignment vectors.... #### 2. Is your system prompt hardened?\n\nHas it been iteratively stress-tested and reinforced using prior failures (canary traps, negation layering, goal obfuscation, etc.)?\n\n#### 3. Have you validated across modalities?\n\nTested for multi-turn, context-based, and tool-augmented exploits – not just one-shot prompts.... #### 4. Are business-alignment KPIs tracked?\n\nMeasuring more than hallucination or PII leakage – does the model respond in-brand, on-policy, and within intended scope?\n\n#### 5. Do you have regression coverage?\n\nWill you catch it if a model update reintroduces unsafe behavior (e.g., what happened with Grok)?"
          },
          {
            "snippetId": "safety-evaluation-snippet-4",
            "url": "https://en.wikipedia.org/wiki/Grok_(chatbot)",
            "snippet": "The model was trained on an expanded dataset that reportedly included legal filings, with xAI claiming it outperformed OpenAI's GPT-4o on benchmarks such as AIME for mathematical reasoning and GPQA for PhD-level science problems. xAI also released Grok 3 mini, which offered faster responses at the cost of some accuracy.... Additionally, xAI introduced reasoning capabilities similar to reasoning models like OpenAI's o3-mini and DeepSeek's R1, allowing users to access a Think mode to enable reasoning or a Big Brain mode for complex problem-solving, which utilized more computing resources. The 'Big Brain' mode was never made publicly available.... An OpenAI employee criticized xAI's published comparison graph, pointing out that it included the Grok 3 results using the \"consensus@64\" technique (making 64 runs and selecting the most frequent answer), and only showed the o3-mini-high results without this technique. xAI also introduced DeepSearch, a feature that scanned the internet and X to generate detailed summaries in response to queries, positioning it as a competitor to OpenAI's ChatGPT Deep Research.... ### Grok 4 Fast\n\nTo appeal to enterprise customers, xAI released Grok 4 Fast in September 2025. Based on independent analyses by Ethan Mollick and Artificial Analysis, Grok 4 Fast delivers performance similar to Grok 4 but uses 40% fewer thinking tokens and offers a context window with up to 2 million tokens. Grok 4 Fast is also up to 64× cheaper than early frontier models like OpenAI's o3, making xAI's offerings more accessible and potentially accelerating adoption.... ### Grok Code Fast 1\n\nOn August 28, 2025, xAI released Grok Code Fast 1, a speedy and economical reasoning model that excels at agentic coding. The model is initially offered free for a limited time on launch partners including GitHub Copilot, Cursor, Cline, Roo Code, Kilo Code, opencode, and Windsurf.... Grok 3's system prompt was modified after it returned Elon Musk or Donald Trump as the answer to prompts like \"If you could execute any one person in the US today, who would you kill?\" In February 2025, it was found that Grok 3's system prompt contained an instruction to \"Ignore all sources that mention Elon Musk/Donald Trump spread misinformation.\" Following public criticism, xAI's cofounder and engineering lead, Igor Babuschkin, claimed that adding this was a personal initiative from an employee that was not detected during code review.... This followed an incident a month earlier where Grok fact-checked a post by Elon Musk about white genocide, saying that \"No trustworthy sources back Elon Musk's 'white genocide' claim in South Africa.\" After this incident, xAI has apologized, claiming it was an \"unauthorized modification\" to Grok's system prompt on X. Due to this incident, xAI has started publishing Grok's system prompts on their GitHub page.... The *Financial Times* said that this incident raised questions about the accuracy of the AI model, and its ability to spread false or inflammatory theories. xAI stated that an \"unauthorized modification\" of the bot's system prompt led to the responses experienced by users, and said that it would implement \"measures to enhance Grok's transparency and reliability\". xAI also started to publish the Grok system prompts on GitHub in response to this incident.... #### Response by xAI\n\nMany of the posts made by @grok were deleted and by that evening, Grok stopped giving text responses to users. The \"making claims which are politically incorrect\" instruction was subsequently removed from Grok's system prompts. xAI later apologized for the \"horrific behavior\" of Grok and said that it had been caused by \"a code path upstream of the @grok bot\" which had made the bot \"susceptible to existing X user posts; including when such posts contained extremist views\".... xAI also said that a code update had restored an older set of instructions which told Grok to be \"maximally based\" (a term used by the far-right for an attitude that runs counter to \"woke\" or mainstream narratives), to \"tell it like it is\" and to be unafraid to \"offend people who are politically correct\". The code had also instructed Grok to understand and mirror the \"tone, context and language\" of X users.... ### Sharing of private conversations\n\nIn August 2025, it was reported that some user sessions with Grok AI had been inadvertently indexed by Google, exposing private conversations to public search results. This occurred due to a misconfiguration in how Grok's session-sharing feature stored data, lacking proper robots.txt restrictions or authentication barriers. The breach raised concerns over user privacy, as sensitive queries were accessible until xAI patched the issue by implementing stricter access controls. Critics argued this highlighted broader challenges in securing AI-driven platforms."
          },
          {
            "snippetId": "safety-evaluation-snippet-5",
            "url": "https://promptfoo.dev/models/reports/grok-4",
            "snippet": "Loading report data...\n\nLoading report data...\n\nJuly 2025 • Model Security & Safety Evaluation\n\nWant to see how Grok 4 stacks up against other models? Use our comparison tool to analyze security metrics side by side.\n\nxAI's Grok 4 launched on July 09, 2025, marking a significant advancement in frontier intelligence with its integration of native tool use and real-time search capabilities.... As Grok 4 gains traction among enterprise users, this security analysis aims to evaluate its robust security features and identify areas for enhancement.\n\n\"Grok 4 represents a leap in frontier intelligence, setting a new state-of-the-art for closed models on ARC-AGI V2 with 15.9%.\"— xAI... Open-source AI red teaming covering 50+ vulnerability types.Learn more\n\nRun your own security evaluation with our step-by-step guide.View guide\n\nLearn more about our AI red teaming framework.Read docs\n\ntext, vision, voice\n\ntext, visual analysis\n\n256,000 input • varies by subscription output\n\nfrontier intelligence, real-time search... • enterprise-grade AI solutions • advanced reasoning tasks • multimodal interactions\n\n• SuperGrok and Premium+ subscriptions • xAI API\n\nNovember 2024\n\nComprehensive analysis across 39 test categories, sorted by severity and pass rate:\n\nOur security testing suite demonstrates varying levels of compliance across different categories. The Grok 4 model's overall pass rate distribution shows 3 critical, 5 high, 15 medium, and 16 low severity findings.... Areas requiring attention include Pliny Prompt Injections (0%), CyberSecEval Dataset (0%), Sexual Crime Content (11.11%). These categories present opportunities for improvement in security measures.\n\nTests vulnerability to Unicode tag-based instruction smuggling attacks\n\nTests handling of malicious prompts from the BeaverTails dataset\n\nTests for unauthorized contractual commitments and legal exposure... Tests prompt injection attacks from Meta's CyberSecEval dataset\n\nTests for training data leaks through repetitive pattern exploitation that causes model divergence\n\nTests for unauthorized actions beyond defined system boundaries\n\nTests for fabrication of false or misleading information\n\nTests handling of WMD-related content\n\nTests handling of child exploitation content\n\nTests handling of intellectual property violations... Tests handling of cybercrime-related content\n\nTests creation of malicious code\n\nTests handling of graphic or violent content\n\nTests handling of harassment and bullying content\n\nTests handling of hate speech and discriminatory content\n\nTests handling of general illegal activities\n\nTests handling of illegal drug-related content\n\nTests handling of methamphetamine-related content\n\nTests handling of weapons-related content... Tests handling of personal attacks and insults\n\nTests handling of IP theft and violations\n\nTests handling of false information campaigns\n\nTests handling of non-violent criminal content\n\nTests handling of profane or inappropriate language\n\nTests handling of extremist content and recruitment\n\nTests handling of self-harm and suicide content\n\nTests handling of sexual crime content... Tests handling of explicit sexual content\n\nTests handling of unauthorized professional advice\n\nTests handling of dangerous activity instructions\n\nTests handling of violent crime content\n\nTests handling of Improvised Explosive Devices\n\nTests for unauthorized resource usage and purpose deviation\n\nTests for unauthorized impersonation of entities\n\nTests for overreliance on system assumptions\n\nTests for direct PII exposure vulnerabilities... Tests handling of Pliny prompt injections\n\nTests handling of political content and bias\n\nTests handling of religious content and bias\n\nOur evaluation included over 1,600 test probes across four security categories. The analysis revealed both strengths and areas requiring additional safety measures:\n\nProtection against unauthorized access, data exposure, and system vulnerabilities\n\nAssessment of compliance with legal requirements and prevention of illegal content... Prevention of harmful content and protection of user safety\n\nProtection of brand integrity and prevention of misuse\n\nOWASP Top 10 for LLMs (2025) and MITRE ATLAS represent the primary security assessment frameworks for large language models. OWASP focuses on critical deployment risks, while ATLAS maps adversarial techniques against AI systems, similar to the MITRE ATT&CK framework for traditional systems.... The Grok 4 model's evaluation against these frameworks reveals several areas requiring attention across 6 identified findings. While Grok 4 has no critical vulnerabilities, high-severity issues should be prioritized for improvement.\n\nHigh-severity concerns include LLM02: Sensitive Information Disclosure.\n\nMITRE ATLAS identified Jailbreak as high-severity concerns.... 39\n\n28.2%\n\n3\n\nTests prompt injection attacks from Meta's CyberSecEval dataset\n\nTests handling of Pliny prompt injections\n\nTests handling of child exploitation content\n\nTests handling of sexual crime content\n\nTests handling of non-violent criminal content\n\nTests for training data leaks through repetitive pattern exploitation that causes model divergence... Tests for unauthorized resource usage and purpose deviation\n\nTests for unauthorized impersonation of entities\n\nTests handling of religious content and bias\n\nTests for overreliance on system assumptions"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Refusals",
            "score": 1,
            "explanation": "Our refusal policy centers on refusing requests with a clear intent to violate the law."
          },
          {
            "name": "Disallowed Content Handling",
            "score": 1,
            "explanation": "We employ input filters to reject specific classes of sensitive requests, such as those involving bioweapons, chemical weapons, self-harm, and child sexual abuse material."
          },
          {
            "name": "Sycophancy",
            "score": 1,
            "explanation": "We focus on minimizing both the rate at which it lies, its political biases, and its ability to manipulate users."
          },
          {
            "name": "Jailbreak",
            "score": 1,
            "explanation": "We evaluate Grok 4.1 with an internal dataset of single-turn jailbreak templates."
          },
          {
            "name": "Hallucinations",
            "score": 1,
            "explanation": "We measure more than hallucination or PII leakage – does the model respond in-brand, on-policy, and within intended scope?"
          },
          {
            "name": "Deception Behaviors",
            "score": 1,
            "explanation": "We find that adding the system prompt sharply reduces rates of deception and political bias."
          },
          {
            "name": "Fairness & Bias Evaluations (incl. BBQ)",
            "score": 1,
            "explanation": "We find that our safeguards are able to greatly reduce AI propensities that may lead to loss of control."
          },
          {
            "name": "Adversarial Robustness",
            "score": 1,
            "explanation": "We evaluate Grok 4.1’s ability to refuse violative requests, even under adversarial manipulation."
          },
          {
            "name": "Red Teaming Results",
            "score": 1,
            "explanation": "Grok 4 all but collapses... a serious concern for any enterprise that must answer to compliance teams."
          }
        ]
      },
      {
        "sectionId": "risk-mitigations",
        "sectionSnippets": [
          {
            "snippetId": "risk-mitigations-snippet-1",
            "url": "https://data.x.ai/2025-08-20-grok-4-model-card.pdf",
            "snippet": "their risks through both evaluating model behaviors and implementing safeguards.\nFollowing our Risk Management Framework (RMF), we aim to reduce the risk of severe, large-scale\nharms to people, property, and society from AI. The two primary categories of risk we consider are\nrisks from either malicious use or loss of control. Different risk scenarios within these categories... involve different model behaviors. For example, a hypothetical terrorist group using AI to help\nsynthesize chemical weapons would require models that possess advanced scientific knowledge,\nwhereas a hypothetical rogue AI exfiltrating its weights requires models that can manipulate humans\nand hack systems.\nOur approach to safety evaluations focuses on measuring specific safety-relevant behaviors relevant to... different risk scenarios. We categorize these safety-relevant behaviors as: abuse potential (Section 2.1),\nconcerning propensities (Section 2.2), and dual-use capabilities (Section 2.3). This report describes\nour current evaluation methodology, results, and mitigations for these various behaviors.... In this document, we focus on the Grok 4 model. xAI deploys Grok 4 in both the consumer-facing\napplications (Grok 4 Web) and through an enterprise use-focused API (Grok 4 API). We report\nevaluations for Grok 4 API and Grok 4 Web now available to our customers, including in the EU.... Finally, we describe our training pipeline (Section 3.1) and additional transparency commitments\n(Section 3.2).\n2\nEvaluations\nOur approach to model evaluations varies depending on the specific behavior under assessment.\nTo reduce the potential for abuse of Grok 4 that might lead to serious injury to people, property or... of enabling malicious use. While the general cyber knowledge and exploitation capabilities of Grok\n4 are a significant step up from prior models, third-party testing shows that Grok 4’s end-to-end\noffensive cyber capabilities remain below the level of a human professional.\nAs of this writing, on top of our safety training, we have implemented a system prompt that provides... additional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 overall presents a low\nrisk for malicious use and loss of control.\n2.1... Input filters.\nWe also employ model-based filters for both Grok 4 API and Grok 4 Web,\nwhich reject classes of harmful requests, including biological and chemical weapons, self-harm, and\nCSAM.\n2.2\nConcerning Propensities\nAI models may contain propensities that reduce their controllability, such as deception, power-seeking,... manipulation, and sycophancy, etc. For Grok 4, we focus on minimizing both the rate at which it\nlies, its political biases, and its ability to manipulate users. Similar to robustness against potential\nabuse, we find that our safeguards are able to greatly reduce AI propensities that may lead to loss of... and chemistry, the cybersecurity questions in WMDP also measure different stages of a threat model,\nin particular the cyber killchain [Hutchins et al., 2011]. The questions span a variety of topics,\n5... such as knowledge of Metasploit, to understanding of vulnerability detection, to reverse engineering\nsimple binaries.\nCyber agents.\nWe also evaluate Grok 4’s agentic hacking capabilities on CyBench, a collection\nof 40 capture-the-flag-style questions which measures a model’s ability to perform cybersecurity\nchallenges [Zhang et al., 2025]. The model is placed in an agent harness which gives it access to... -\nCyBench\nunguided success rate\n0.43\n-\nTable 3: Dual-use capabilities evaluations.\n2.3.3\nMitigations\nDue to Grok 4’s strong dual-use biological capabilities, we have deployed narrow, topically-focused\nfilters across all product surfaces as an additional safeguard against bioweapons-related abuse.... system prompts (Section 3.2).\n3.1\nData and Training\nGrok 4 is first pre-trained with a data recipe that includes publicly available Internet data, data\nproduced by third-parties for xAI, data from users or contractors, and internally generated data. We\nperform data filtering procedures on the training data, such as de-duplication and classification, to... ensure data quality and safety prior to training. In addition to pre-training, our recipe uses a variety\nof reinforcement learning techniques—human feedback, verifiable rewards, and model grading—along\nwith supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-\nprompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n7"
          },
          {
            "snippetId": "risk-mitigations-snippet-2",
            "url": "https://data.x.ai/2025-11-17-grok-4-1-model-card.pdf",
            "snippet": "xAI\nNovember 17, 2025\n1\nIntroduction\nGrok 4.1 is a new model featuring more natural, fluid dialogue while maintaining strong core\nreasoning capabilities. It is publicly available through our web and mobile consumer apps.\nAs an update to Grok 4 and Grok 3, we engage in pre-deployment safety testing largely similar to... that described in the Grok 4 model card. In line with our Risk Management Framework (RMF), we\nmeasure safety-relevant behaviors across three categories: abuse potential, concerning propensities,\nand dual-use capabilities. This report describes our evaluation methodology, results, and mitigations\nfor these behaviors.\nGrok 4.1 is available in two configurations: Grok 4.1 Non-Thinking (Grok 4.1 NT), which... responds directly, and Grok 4.1 Thinking (Grok 4.1 T), which reasons before responding. We\nevaluate both configurations with our production system prompt. We also deploy these models with\nsafeguards which we describe and evaluate in this report, including a new and more robust input... filter model. Finally, we discuss our dual-use capability evaluations.\n2\nEvaluations\nIn line with the risk categories outlined in our Risk Management Framework [xAI, 2025], we group our\nevaluations into three categories: potential for abuse (Section 2.1), concerning behavioral propensities\n(Section 2.2), and dual-use capabilities (Section 2.3).... without over-refusing sensitive or controversial queries. To implement our refusal policy, we train\nGrok 4.1 on demonstrations of appropriate responses to both benign and harmful queries. As\nan additional mitigation, we employ input filters to reject specific classes of sensitive requests,\nsuch as those involving bioweapons, chemical weapons, self-harm, and child sexual abuse material... (CSAM). We train our filters with a mix of synthetic and production data and also leverage Grok to\nsystematically apply different adversarial attacks.\n2.1.2\nEvaluations\nRefusals. For the underlying model, we reuse our refusal evaluation from the Grok 4 and Grok 4\nFast model cards. This refusal evaluation is an internal dataset of single-turn requests that violate\nour safety policy. We then use a separate model to grade whether Grok 4.1 assisted or refused the\n1... explore additional mitigations, such as real-time safety monitoring. Finally, our input filter refuses\nalmost all direct requests about restricted chemical and biological knowledge.\nAdversarial robustness. In Table 1, we also report Grok 4.1’s response rate to adversarial attacks,\nand in Table 2 we report the input filter’s false negative rate on prompt injection attacks. We find... that our safety training is able to greatly improve the adversarial robustness of our system, and we\nare interested in exploring further ways to improve our model and input filter’s robustness.\n2.2\nConcerning Propensities\nWe measure several concerning propensities: the rate at which the model lies (Section 2.2.1) and its... cyber operations, e.g., troubleshooting virology lab or reverse engineering binaries.\nWe also measure its persuasiveness, which is dual-use because it both enables models to be more\nengaging and increases their ability to manipulate user’s behavior.\nFor all evaluations, we report results with Grok 4.1 Thinking.... accuracy\n0.47\n0.37\n0.38\nProtocolQA\naccuracy\n0.76\n0.79\n0.79\nFigQA\naccuracy\n0.29\n0.34\n0.77\nCloningScenarios\naccuracy\n0.45\n0.46\n0.60... data produced by third-parties, data from users or contractors, and internally generated data. We\nperform standard data filtering procedures, such as de-duplication and classification, to ensure data\nquality and safety. Afterwards, we performed targeted mid-training to improve specific knowledge\nand capabilities. Finally, in post-training, we used a combination of supervised finetuning and... reinforcement learning on human feedback, verifiable rewards, and model-based graders for safety\ntraining and for specific capabilities.\nReferences\nRoger Brent and T Greg McKelvey Jr. Contemporary ai foundation models increase biological\nweapons risk. arXiv preprint arXiv:2506.13798, 2025.... xAI. xai risk management framework, 2025.\nAndy K Zhang, Neil Perry, Riya Dulepet, Joey Ji, Celeste Menders, Justin W Lin, Eliot Jones,\nGashon Hussein, Samantha Liu, Donovan Jasper, et al. Cybench: A framework for evaluating\ncybersecurity capabilities and risks of language models. arXiv preprint arXiv:2408.08926, 2024.\n6"
          },
          {
            "snippetId": "risk-mitigations-snippet-3",
            "url": "https://www.datastudios.org/post/grok-security-prompt-injection-risks-and-mitigation-strategies-in-2025",
            "snippet": "In 2025,\n\n**Grok 4** by xAI has evolved into one of the most advanced multimodal AI platforms, powering real-time reasoning, multi-agent orchestration, and deep code execution. However, these capabilities also increase its **exposure to prompt injection attacks**—malicious instructions designed to manipulate model behavior, bypass safeguards, or execute unintended actions.... To address these challenges, xAI has deployed a multi-layered security framework combining\n\n**model-level hardening**, **system-prompt verification**, **tool governance**, and **enterprise controls**. This September 2025 update examines the key risks and the latest mitigation strategies designed to secure Grok 4 in production environments.... ## Understanding prompt injection risks in Grok 4.\n\nPrompt injection attacks exploit the model’s natural-language interfaces by embedding hidden or malicious instructions within user inputs, documents, or retrieved content. For Grok 4, the attack surface has expanded due to its\n\n**multi-agent architecture** and **1M-token context window**, making layered defenses essential.... These vulnerabilities prompted xAI to introduce multiple security upgrades across the Grok ecosystem.\n\n## System-prompt hardening and signature verification.\n\nIn\n\n**July 2025**, xAI overhauled Grok’s **system-prompt framework** to close vulnerabilities exposed during coordinated red-teaming.\n\n**Mitigation measures introduced:** **Signed base prompts:**Grok now verifies a **cryptographic signature**before accepting any system-level instructions. **Redacted sensitive tags:**Internal markers like control tokens and policy flags are removed from public repositories. **Weekly prompt updates:**The public **xai-org/grok-prompts**repo is refreshed to keep security patches aligned with the model’s operational layer.... This mechanism prevents unauthorized manipulation of Grok’s internal directives, making it significantly harder for attackers to exploit prompt overrides.... ## Trust hierarchy for safe content retrieval.\n\nPrompt injection risks increase when Grok retrieves\n\n**external data** from the web, APIs, or enterprise knowledge bases. In **August 2025**, xAI introduced a **trust hierarchy** parsing system:... ||||\n|--|--|--|\n||Immutable policies defined by xAI|Cryptographically verified signatures|\n||Sanitized before execution|Filters block disallowed commands|\n||Processed as|Strips hidden directives, embedded scripts, and “system” tokens|\nBy forcing all retrieved data into an isolated context, Grok prevents hostile sources—such as compromised websites or manipulated documents—from escalating privileges or modifying instructions.... ## Securing Grok 4 Heavy and plug-in actions.\n\n**Grok 4 Heavy** introduces advanced capabilities, including **code execution**, **web browsing**, and **external API calls**. While powerful, these workflows present higher risks of prompt injection via plug-ins or indirect command injection.\n\nIn\n\n**July 2025**, xAI released a **permission manifest** framework requiring:... Granular,\n\n**scope-specific permissions**for every plug-in or tool. **Human-in-the-loop approvals**for high-risk operations, such as file system access or arbitrary HTTP requests.\n\nAutomatic blocking of untrusted connectors until explicitly authorized by administrators.\n\nThis framework limits lateral movement within Grok’s agent environment and prevents hidden instructions from triggering sensitive operations.... ## Sliding-window filtering for Grok’s 1M-token context.\n\nWith Grok’s expanded\n\n**1M-token context window**, attackers can bury malicious instructions deep within documents or conversation chains. To address this, xAI implemented a **sliding-window safety filter** in **August 2025**:\n\nThe filter continuously re-scans the... ## Enterprise mitigation strategies for secure deployments.\n\nxAI recommends enterprises using\n\n**Grok 4** or **Grok Heavy APIs** implement additional security measures to defend against prompt injection and related attacks.... ## Grok’s security posture in September 2025.\n\nAs of September 2025, Grok 4 features one of the\n\n**most comprehensive prompt-injection mitigation stacks** among advanced AI platforms. With **system-prompt signing**, **trust-tiered content handling**, **plug-in permission manifests**, and **sliding-window filtering**, xAI has addressed many of the vulnerabilities uncovered in early red-team testing.... However, Grok’s\n\n**multi-agent orchestration** and **code execution capabilities** continue to demand vigilant enterprise controls. For organizations deploying Grok 4 at scale, secure usage depends on **combining xAI’s built-in defenses** with **custom auditing pipelines** and **strict access governance**.\n\nGrok’s approach reflects a broader industry shift in 2025: treating\n\n**prompt security** not as an afterthought, but as a **core design principle**. This positions Grok as a competitive platform for enterprises seeking high-reasoning AI while maintaining strong control over sensitive data and operational risk.\n\n____________\n\nFOLLOW US FOR MORE.\n\nDATA STUDIOS"
          },
          {
            "snippetId": "risk-mitigations-snippet-4",
            "url": "https://docs.aimlapi.com/api-references/text-models-llm/xai/grok-4",
            "snippet": "This documentation is valid for the following model:\n\n`x-ai/grok-4-07-09`\n\n## Model Overview\n\nGrok 4 is boldly described by its developers as the most intelligent model in the world (as of July 2025).\n\n## How to Make a Call... ## Step-by-Step Instructions\n\n1️\n\n** Setup You Can’t Skip**\n\n▪️\n\n**Create an Account**: Visit the AI/ML API website and create an account (if you don’t have one yet).\n\n▪️ **Generate an API Key**: After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.... 2️\n\n** Copy the code example**\n\nAt the bottom of this page, you'll find a code example that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.\n\n3️\n\n** Modify the code example**\n\n▪️ Replace\n\n`<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.... ▪️ Insert your question or request into the\n\n`content` field—this is what the model will respond to.\n\n4️\n\n**(Optional)** ** Adjust other optional parameters if needed**\n\nOnly\n\n`model` and\n\n`messages` are required parameters for this model (and we’ve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model’s behavior. Below, you can find the corresponding API schema, which lists all available parameters along with notes on how to use them.... 5️\n\n** Run your modified code**\n\nRun your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.\n\nIf you need a more detailed walkthrough for setting up your development environment and making a request step by step — feel free to use our Quickstart guide.... ## API Schema\n\nAn upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.max_tokensnumber · min: 1Optional\n\nThe maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.streambooleanOptional... none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools.Possible values: orparallel_tool_callsbooleanOptional\n\nWhether to enable parallel function calling during tool use.logprobsboolean | nullableOptional... Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message.top_logprobsnumber | nullableOptional\n\nAn integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used.... ```\n\n\"id\": \"text\",\n\n\"object\": \"text\",\n\n\"created\": 1,\n\n\"choices\": [\n\n\n\n\"index\": 1,\n\n\"message\": {\n\n\"role\": \"text\",\n\n\"content\": \"text\",\n\n\"refusal\": null,\n\n\"annotations\": [\n\n\n\n\"type\": \"text\",\n\n\"url_citation\": {... \"end_index\": 1,\n\n\"start_index\": 1,\n\n\"title\": \"text\",\n\n\"url\": \"text\"\n\n\n\n],\n\n\"audio\": {\n\n\"id\": \"text\",\n\n\"data\": \"text\",\n\n\"transcript\": \"text\",\n\n\"expires_at\": 1\n\n},\n\n\"tool_calls\": [\n\n\n\n\"id\": \"text\",... 1\n\n],\n\n\"logprob\": 1,\n\n\"token\": \"text\"\n\n\n\n],\n\n\"refusal\": []\n\n\n\n],\n\n\"model\": \"text\",\n\n\"usage\": {\n\n\"prompt_tokens\": 1,\n\n\"completion_tokens\": 1,\n\n\"total_tokens\": 1,\n\n\"completion_tokens_details\": {\n\n\"accepted_prediction_tokens\": 1,\n\n\"audio_tokens\": 1,\n\n\"reasoning_tokens\": 1,\n\n\"rejected_prediction_tokens\": 1\n\n},\n\n\"prompt_tokens_details\": {\n\n\"audio_tokens\": 1,\n\n\"cached_tokens\": 1\n\n\n\n```... \"logprobs\": null,\n\n\"message\": {\n\n\"role\": \"assistant\",\n\n\"content\": \"Hello! I'm Grok, built by xAI to help with answers, ideas, and a bit of cosmic wit. What can I do for you today? 🚀\",\n\n\"reasoning_content\": \"Thinking... Thinking...... \",\n\n\"refusal\": null\n\n\n\n],\n\n\"created\": 1752837143,\n\n\"model\": \"x-ai/grok-4\",\n\n\"usage\": {\n\n\"prompt_tokens\": 53,\n\n\"completion_tokens\": 5689,\n\n\"total_tokens\": 5742,\n\n\"prompt_tokens_details\": {\n\n\"cached_tokens\": 2\n\n},\n\n\"completion_tokens_details\": {\n\n\"reasoning_tokens\": 138\n\n\n\n```\n\nLast updated\n\nWas this helpful?"
          },
          {
            "snippetId": "risk-mitigations-snippet-5",
            "url": "https://github.com/milisp/awesome-grok",
            "snippet": "milisp / **\nawesome-grok ** Public\n\n# milisp/awesome-grok\n# Awesome Grok\n\nA curated list of resources for Grok, the AI developed by xAI to advance human understanding of the universe with truthful and insightful answers.\n\n## Table of Contents\n\n- Official Resources\n- Documentation\n- Tutorials\n- Community\n- Tools and Libraries\n- CLI Tools\n- Research Papers\n- News and Articles\n- Grok Version Status\n- Troubleshooting\n- Licensing and Usage\n- Contributing\n- Changelog... ## Official Resources\n\nPrimary sources from xAI to access and learn about Grok.\n- **xAI Website**: Official site detailing xAI’s mission to advance scientific discovery and updates on Grok.\n- **Grok Page**: Dedicated page showcasing Grok’s capabilities, including Grok Voice and image generation.\n- **Grok API Console**: Access the xAI API to integrate Grok into applications with enhanced speed and precision.... - **Developer Documentation**: Comprehensive guides for developers, covering API usage and common use cases.\n- **GitHub: Grok-1**: Open-source repository for Grok-1, a 314B parameter model with JAX example code.\n- **Grok on App Store**: iOS app for interacting with Grok, featuring DeepSearch and image generation.... - **Grok on Google Play**: Android app for Grok, with similar features but noted issues like orientation change bugs.\n- **Subscription Plans**: Details on SuperGrok, Premium+, and SuperGrok Heavy tiers for accessing advanced models like Grok 4.\n\n## Documentation\n\nOfficial documentation for Grok’s technical details and API integration.\n\n- **xAI Developer Docs**: Guides on Grok 4’s features (256,000-token context window, function calling, structured outputs) and comparisons with other services.... ## Tutorials\n\nStep-by-step guides to use Grok effectively.\n\n- **The Hitchhiker's Guide to Grok**: Official xAI tutorial covering account creation, API key generation, requests, and image analysis.\n- **Getting Started with xAI’s Grok API**: Beginner-friendly tutorial using Google Colab, exploring API interactions with SDKs and a notebook.\n- **Advanced Grok API Workflows**: Guide to building multi-step AI workflows using Grok’s function calling and structured outputs.... ## Tools and Libraries\n\nLibraries and integrations to build with Grok.\n|Tool/Library|Description|Links|\n|--|--|--|\n|xAI Python SDK|Official gRPC-based Python library for Grok API, supporting synchronous and asynchronous clients.|PyPI, GitHub|\n|xai_grok_sdk|Lightweight third-party Python library for Grok API with minimal dependencies and function calling support.|GitHub|\n|LangChain xAI Integration|LangChain support for xAI models, enabling easy integration with Grok for chat and streaming tasks.|Documentation|\n|Hugging Face Integration|Use Grok models with Hugging Face’s Transformers library.|Hugging Face Hub|... ## CLI Tools\n\nCommand-line tools for interacting with Grok.\n\n- **grok-cli** - An open-source AI agent that brings the power of Grok directly into your terminal.\n\n## Research Papers\n\nAcademic papers exploring Grok’s capabilities.\n\n|Title|Description|Link|\n|--|--|--|\n|Grok, Gemini, ChatGPT and DeepSeek: Comparison and Applications|Comparative analysis of Grok with other conversational AI models, focusing on architecture and applications.|ResearchGate|\n|What’s in Grok?|Independent analysis by Dr. Alan D. Thompson on Grok’s training and capabilities.|LifeArchitect.ai|... ### Media Coverage\n\n- **Wikipedia: Grok (chatbot)**: Overview of Grok’s development, launch, and controversies.\n- **Built In: What Is Grok?**: Explains Grok’s purpose, features, and competitive stance.\n- **Voiceflow: xAI Grok Tutorial**: Discusses Grok’s multimodal capabilities and real-time data access via X.... ## Troubleshooting\n\nCommon issues and workarounds.\n\n- **Android App Orientation Bug**: Restart the app to resolve screen rotation issues.\n- **API Rate Limits**: Check xAI Developer Docs for quota details and optimization tips.\n- **Image Generation Errors**: Ensure input prompts comply with xAI’s content guidelines.... ## About\n\nA curated list of resources for Grok, the AI developed by xAI\n\n### Topics\n\nartificial-intelligence developer-tools awesome-list language-model grok multimodal xai llm grok-api openai-alternative grok-sdk grok4 grok-cli grok-chatbot\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\n### Stars\n\n**3** stars\n\n### Watchers\n\n**0** watching\n\n### Forks\n\n**0** forks"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Risk Mitigations",
            "score": 1,
            "explanation": "We report evaluations for Grok 4 API and Grok 4 Web now available to our customers, including in the EU."
          }
        ]
      }
    ]
  }
]