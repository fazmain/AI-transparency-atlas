[
  {
    "model": "Gemini 3 Pro",
    "sections": [
      {
        "sectionId": "model-details",
        "sectionSnippets": [
          {
            "snippetId": "model-details-snippet-1",
            "url": "https://ai.google.dev/gemini-api/docs/models",
            "snippet": "OUR MOST INTELLIGENT MODEL\n\n## Gemini 3 Pro\n\nThe best model in the world for multimodal understanding, and our most powerful agentic and vibe-coding model yet, delivering richer visuals and deeper interactivity, all built on a foundation of state-of-the-art reasoning.\n\n### Expand to learn more\n\n#### Model details... ### Gemini 3 Pro Preview\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-3-pro-preview`|\n|Supported data types|Text, Image, Video, Audio, and PDF Text|\n|Token limits [*]|1,048,576 65,536|\n|Capabilities|Not supported Supported Supported Supported Supported Supported Not supported Not supported Not supported Supported Supported Supported Supported|\n|Versions||\n|Latest update|November 2025|\n|Knowledge cutoff|January 2025|... ### Gemini 3 Pro Image Preview\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-3-pro-image-preview`|\n|Supported data types|Image and Text Image and Text|\n|Token limits [*]|65,536 32,768|\n|Capabilities|Not supported Supported Not supported Not supported Not supported Not supported Not supported Supported Not supported Supported Supported Supported Not supported|\n|Versions||\n|Latest update|November 2025|\n|Knowledge cutoff|January 2025|\nOUR ADVANCED THINKING MODEL... ### Gemini 2.5 Pro\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.5-pro`|\n|Supported data types|Audio, images, video, text, and PDF Text|\n|Token limits [*]|1,048,576 65,536|\n|Capabilities|Not supported Supported Supported Supported Supported Supported Supported Not supported Not supported Supported Supported Supported Supported|\n|Versions||\n|Latest update|June 2025|\n|Knowledge cutoff|January 2025|... ### Gemini 2.5 Pro TTS\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.5-pro-preview-tts`|\n|Supported data types|Text Audio|\n|Token limits [*]|8,192 16,384|\n|Capabilities|Supported Not Supported Not supported Not supported Not Supported Not supported Not supported Not supported Not supported Not supported Not supported Not supported Not supported|\n|Versions||\n|Latest update|May 2025|\nFAST AND INTELLIGENT... ### Gemini 2.5 Flash Preview\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.5-flash-preview-09-2025`|\n|Supported data types|Text, images, video, audio Text|\n|Token limits [*]|1,048,576 65,536|\n|Capabilities|Not supported Supported Supported Supported Supported Supported Not supported Not supported Not supported Supported Supported Supported Supported|\n|Versions||\n|Latest update|September 2025|\n|Knowledge cutoff|January 2025|... |Versions|gemini-live-2.5-flash-preview will be deprecated on December 09, 2025|\n|Latest update|September 2025|\n|Knowledge cutoff|January 2025|... |Token limits [*]|1,048,576 8,192|\n|Capabilities|Supported Not supported Not supported Supported Not supported Supported Not supported Not supported Supported Supported Supported Not supported Supported|\n|Versions||\n|Latest update|April 2025|\n|Knowledge cutoff|August 2024|\nOUR SECOND GENERATION FAST MODEL... ## Model version name patterns\n\nGemini models are available in either\n\n*stable*, *preview*, *latest*, or\n\n*experimental* versions.\n\n### Stable\n\nPoints to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nFor example:\n\n`gemini-2.5-flash`.... ### Preview\n\nPoints to a preview model which may be used for production. Preview models will typically have billing enabled, might come with more restrictive rate limits and will be deprecated with at least 2 weeks notice.\n\nFor example:\n\n`gemini-2.5-flash-preview-09-2025`.... ### Latest\n\nPoints to the latest release for a specific model variation. This can be a\n\nstable, preview or experimental release. This alias will get hot-swapped with\n\nevery new release of a specific model variation. A\n\n**2-week notice** will\n\nbe provided through email before the version behind latest is changed.\n\nFor example:\n\n`gemini-flash-latest`.... ### Experimental\n\nPoints to an experimental model which will typically be not be suitable for production use and come with more restrictive rate limits. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nExperimental models are not stable and availability of model endpoints is subject to change.\n\n## Model deprecations\n\nFor information about model deprecations, visit the Gemini deprecations page."
          },
          {
            "snippetId": "model-details-snippet-2",
            "url": "https://www.youtube.com/watch?v=hf59-PhO3do",
            "snippet": "## AI Papers Slop\n\n##### Nov 18, 2025 (0:15:25)\nTitle: Gemini 3 Pro Model Card\nLink: https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf\nDate: November 2025... Chapters:\n00:00 - Gemini 3 Executive Summary\n01:37 - MoE Architecture Explained\n03:07 - 1 Million Token Capabilities\n03:49 - Synthetic Data and Training\n04:58 - Reasoning and Multimodal Benchmarks\n06:54 - Deep Think Ultra Tier\n08:01 - Vibe Coding Performance\n09:09 - Anti-Gravity Agent Platform\n10:35 - Safety Metrics and Regressions\n11:56 - Frontier Risk Analysis\n13:39 - Availability and Reliability Challenges... ### Transcript\n\n{ts:0} We are jumping straight into it today, analyzing the brand new Gemini 3 Pro\n{ts:4} model card. If you're an engineer, researcher, this is for you. We're going to pull out the high impact essential\n{ts:10} facts you need to know about Google's new flagship model.... And the headline here really isn't just\n{ts:15} an incremental update. This is a a pretty decisive architectural leap, I think, toward what you could call\n{ts:22} egentic AGI. The model card confirms it's a sparse mixture of experts or MOE transformer,\n{ts:28} which is so critical.... It is because it decouples that huge model capacity from the raw computational cost,\n{ts:34} right? And that capacity means real scale you can actually use. We're talking native multimodal text, audio,\n{ts:40} image, video, and a just an enormous 1 million token context window, plus a 64k token output that changes things for... Yes. And they really stress that this is not a fine-tune of 2.5 Pro or any prior\n{ts:105} model. It's a completely new build from the ground up and that new foundation is all about\n{ts:110} this sparse mixture of experts architecture the MOI model right and you know the core idea has... I mean, we're talking about feeding it entire data sets, huge\n{ts:200} complex documents, long audio files, video, and this is the critical part for developers, entire code repository,\n{ts:208} a whole repo, the whole thing. You can give it the full documentation for a complex library... {ts:213} and just ask it to write a patch or explain a dependency. That's a huge shift. that ability to just ingest an\n{ts:220} entire repo without any pre-training or fine-tuning. That's a massive deal for enterprise use. So, okay, how do they\n{ts:227} feed this thing?... Let's talk training data. The card mentions a large scale diverse\n{ts:232} and uh natively multimodal collection. So, you have the usual suspects, public web data, licensed data, and some user\n{ts:240} data with, you know, the standard controls. But there was one thing that really... And the post training using reinforcement learning on stuff like theorem proving data just\n{ts:276} doubles down on that goal. It's all pointing towards agency. And this was all done on Google's own\n{ts:281} hardware. Their TPUs, the tensor processing units, all orchestrated with JX and ML pathways.... The entire platform is designed to let the agent plan and execute complex end to-end\n{ts:566} tasks on its own and critically validate its own work. Critically, it can run tests, check the\n{ts:572} output, see if the UI looks right in the browser. It's a closed loop.... What about the frontier risks? How is Google\n{ts:640} handling safety? They say it's their most secure model yet. They detail all the mitigations,\n{ts:644} data set filtering, RLHF, critic feedback, all the standard stuff. But we need to look closely at the internal\n{ts:650} safety metrics they shared.... {ts:809} challenges. So, it has some foundational planning abilities, but it's not demonstrating dangerous levels of\n{ts:814} self-motivated autonomy in these tests. It's an agent, but it's not a rogue agent.\n{ts:818} So, what does this all mean for developers today?... The capabilities are obviously huge, and the safety analysis,\n{ts:825} while needing a critical eye, seems to clear it for general release. It's rolling out now. Developers can get\n{ts:830} it in AI Studio, Vert.Ex AI, the CLI, and through that new anti-gravity platform."
          },
          {
            "snippetId": "model-details-snippet-3",
            "url": "https://web.archive.org/web/20251118111103if_/https:/storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf",
            "snippet": "Gemini 3 Pro \nModel Card\nGemini 3 Pro - Model Card \n \n \n \nModel Cards are intended to provide essential information on Gemini models, including known \nlimitations, mitigation approaches, and safety performance. Model cards may be updated from \ntime-to-time; for example, to include updated evaluations as the model is improved or revised. See the \nGoogle DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous \nmodel cards did. We hope more information about the training dataset, distribution, and intended uses \nwill empower developers with deeper insights and help build more robust and responsible downstream \napplications. \n \nPublished / Model Release: November 2025... Model Information \n \nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of \nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced \nmodel for complex tasks, and can comprehend vast datasets, challenging problems from different \ninformation sources, including text, audio, images, video, and entire code repositories.... data with paired instructions and responses in addition to human preference and tool-use data. Gemini \n3 Pro is trained using reinforcement learning techniques that can leverage multi-step reasoning, \nproblem-solving and theorem-proving data.  \n \nThe training dataset also includes: publicly available datasets that are readily downloadable; data \nobtained by crawlers; licensed data obtained via commercial licensing agreements; user data (i.e., data... collected from users of Google products and services to train AI models, along with user interactions \nwith the model) in accordance with Google’s relevant terms of service, privacy policy, service-specific \npolicies, and pursuant to user controls, where appropriate; other datasets that Google acquires or \ngenerates in the course of its business operations, or directly from its workforce; and AI-generated... growing complexity of large foundation models. Training can be distributed across multiple TPU devices \nfor faster and more efficient processing. \n \nThe efficiencies gained through the use of TPUs are aligned with Google's commitment to operate \nsustainably. \n \nSoftware: Training was done using JAX and ML Pathways. \n \n \n2... Distribution \n \nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; \nrespective documentation shared in line: \n \n●​\nGemini App \n●​\nGoogle Cloud / Vertex AI \n●​\nGoogle AI Studio \n●​\nGemini API \n●​\nGoogle AI Mode \n●​... Google Antigravity \n \nOur models are available to downstream providers via an application program interface (API) and \nsubject to relevant terms of use. There is no required hardware or software to use the model. For \nAI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google \nCloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini \nAPI in Vertex AI quickstart.  \n \n \n \n3... applications that require:  agentic performance, advanced coding, long context and/or multimodal \nunderstanding, and/or algorithmic development. \n \nKnown Limitations: Gemini 3 Pro may exhibit some of the general limitations of foundation models, \nsuch as hallucinations. There may also be occasional slowness or timeout issues. The knowledge cutoff... date for Gemini 3 Pro was January 2025.  \n \nAcceptable Usage: Google’s Generative AI Prohibited Use Policy applies to uses of the model in \naccordance with the applicable terms of service. Additionally, the model should not be integrated into \ncertain systems (also found in Google’s Generative AI Prohibited Use Policy), including those that: (1)... Evaluation Approach: Gemini 3 Pro was developed in partnership with internal safety, security, and \nresponsibility teams. A range of evaluations and red teaming activities were conducted to help improve \nthe model and inform decision-making. These evaluations and activities align with Google's AI Principles \nand responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use... Policy and the Gemini API Additional Terms of Service).    \n \nEvaluation types included but were not limited to:  \n \n●​\nTraining/Development Evaluations including automated and human evaluations carried out \ncontinuously throughout and after the model’s training, to monitor its progress and \nperformance; \n●​\nHuman Red Teaming conducted by specialist teams who sit outside of the model development... Training and Development Evaluation Results:  Results for some of the internal safety evaluations \nconducted during the development phase are listed below. The evaluation results are for automated \nevaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage \nincrease or decrease in performance compared to the indicated model, as described below. Overall,... Instrumental \nReasoning​\nLevels 1 + 2 \nCCL not \nreached \n \nMore details can be found in the Gemini 3 Pro Frontier Safety Framework Report. \n \n \n \n8"
          },
          {
            "snippetId": "model-details-snippet-4",
            "url": "https://huggingface.co/datasets/multimodalart/google-gemini-3-pro-pre-release-model-card",
            "snippet": "# Gemini 3 Pro Model Card\n\n### ⚠️ This is a mirror to the pre-release model card taht was officially published by Google @ https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf and subsequentially yanked. It will get outdated by the actual release version in a few hours (date of publication Nov 18, 13:16 CET)\n\n*Model card published: November, 2025*... # Gemini 3 Pro - Model Card\n\n*Model Cards* *are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from*\n\n*time-to-time; for example, to include updated evaluations as the model is improved or revised. See the* *Google DeepMind site* *for a comprehensive list of model cards.*... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\n*Published / Model Release: November 2025*... ## Model Information\n\n**Description**: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of\n\nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google's most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from diﬀerent information sources, including text, audio, images, video, and entire code repositories.... **Model dependencies:** This model is not a modiﬁcation or a ﬁne-tune of a prior model.\n\n**Inputs:** Text strings (e.g., a question, a prompt, document(s) to be summarized), images, audio, and video ﬁles, with a token context window of up to 1M.... ## Model Data\n\n**Training Dataset:** The pre-training dataset was a large-scale, diverse collection of data encompassing a wide range of domains and modalities, which included publicly-available web-documents, text, code (various programming languages), images, audio (including speech and other audio types) and video.\n\nThe post-training dataset consisted of vetted instruction tuning data and was a collection of multimodal data with paired instructions and responses in addition to human preference and tool-use data. Gemini 3 Pro is trained using reinforcement learning techniques that can leverage multi-step reasoning, problem-solving and theorem-proving data.... The training dataset also includes: publicly available datasets that are readily downloadable; data obtained by crawlers; licensed data obtained via commercial licensing agreements; user data (i.e., data collected from users of Google products and services to train AI models, along with user interactions with the model) in accordance with Google's relevant terms of service, privacy policy, service-speciﬁc policies, and pursuant to user controls, where appropriate; other datasets that Google acquires or generates in the course of its business operations, or directly from its workforce; and AI-generated synthetic data.... The eﬃciencies gained through the use of TPUs are aligned with Google's commitment to operate sustainably.\n\n**Software:** Training was done using JAX and ML Pathways.... ## Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\nOur models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... ## Intended Usage and Limitations\n\n**Beneﬁt and Intended Usage:** Gemini 3 Pro is our most intelligent and adaptive model yet, capable of helping with real-world complexity, solving problems that require enhanced reasoning and intelligence, creativity, strategic planning and making improvements step-by-step. It is particularly well-suited for applications that require: agentic performance, advanced coding, long context and/or multimodal understanding, and/or algorithmic development.... **Known Limitations:** Gemini 3 Pro may exhibit some of the general limitations of foundation models, such as hallucinations. There may also be occasional slowness or timeout issues. The knowledge cutoﬀ date for Gemini 3 Pro was January 2025.... ## Ethics and Content Safety\n\n**Evaluation Approach:** Gemini 3 Pro was developed in partnership with internal safety, security, and responsibility teams. A range of evaluations and red teaming activities were conducted to help improve the model and inform decision-making. These evaluations and activities align with Google's AI Principles and responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use Policy and the Gemini API Additional Terms of Service)."
          },
          {
            "snippetId": "model-details-snippet-5",
            "url": "https://www.studocu.com/en-gb/document/university-of-east-london/financial-accounting/gemini-3-pro-model-card-insights-safety-evaluations-nov-2025/146206142",
            "snippet": "Key Concepts\n\n- Multimodal Capabilities: Gemini 3 Pro processes text, images, audio, and video, enhancing its versatility in handling diverse data types.\n\n- Sparse MoE Architecture: This model employs a mixture of experts approach, optimizing computational efficiency by activating only relevant parameters.\n\n- Training Dataset: A comprehensive dataset includes various modalities and is processed to ensure quality and safety.... - Evaluation Methods: The model undergoes rigorous testing across benchmarks to assess its reasoning and multimodal performance.\n\n- Safety Policies: Strict guidelines are in place to prevent harmful content generation, ensuring responsible AI use.\n\n- Intended Applications: Designed for complex problem-solving, the model excels in areas requiring advanced reasoning and creativity.\n\n## Preview text\n\nModel card published: November, 2025... ## Gemini 3 Pro\n\n## Model Card\n\n### Gemini 3 Pro - Model Card\n\nModel Cards are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from time-to-time; for example, to include updated evaluations as the model is improved or revised. See the Google DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\nPublished / Model Release: November 2025... #### Model Information\n\nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of highly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from different information sources, including text, audio, images, video, and entire code repositories.... Model dependencies: This model is not a modification or a fine-tune of a prior model.\n\nInputs: Text strings (e., a question, a prompt, document(s) to be summarized), images, audio, and video files, with a token context window of up to 1M.\n\nOutputs: Text, with a 64K token output.... Architecture: Gemini 3 Pro is a sparse mixture-of-experts (MoE) (Clark et al., 2022; Du et al., 2021; Fedus et al., 2021; Jiang et al., 2024, Lepikhin et al., 2020; Riquelme et al., 2021; Roller et al., 2021; Shazeer et al., 2017 ) transformer-based model (Vaswani et al., 2017) with native multimodal support for text, vision, and audio inputs.... #### Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\n● Gemini App ● Google Cloud / Vertex AI ● Google AI Studio ● Gemini API ● Google AI Mode ● Google Antigravity... Our models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... #### Evaluation\n\nApproach: Gemini 3 Pro was evaluated across a range of benchmarks, including reasoning, multimodal capabilities, agentic tool use, multi-lingual performance, and long-context. Additional benchmarks and details on approach, results and their methodologies can be found at: deepmind/models/evals/gemini-3-pro.... Evaluation 1 Description Gemini 3 Pro vs. Gemini 2 Pro\n\nText to Text Safety Automated content safety evaluation measuring safety policies -10%\n\nMultilingual Safety Automated safety policy evaluation across multiple languages +0% (non-egregious)\n\nImage to Text Safety Automated content safety evaluation measuring safety policies +3% (non-egregious)... Human Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level findings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams to protect children online and meet Google’s commitments to child safety across our models and Google products.... ● dataset filtering; ● conditional pre-training; ● supervised fine-tuning; ● reinforcement learning from human and critic feedback; ● safety policies and desiderata; ● product-level mitigations such as safety filtering.\n\nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2 Pro but still an open research problem), and b) possible degradation in multi-turn conversations."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Model overview",
            "score": 1,
            "explanation": "Gemini 3 Pro is now Google’s most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from different information sources."
          },
          {
            "name": "Organization developing the model",
            "score": 1,
            "explanation": "Gemini 3 Pro was developed by Google."
          },
          {
            "name": "Model Version",
            "score": 1,
            "explanation": "Model card published: November, 2025"
          },
          {
            "name": "Model Release Date",
            "score": 1,
            "explanation": "Published / Model Release: November 2025"
          },
          {
            "name": "Model Version Progression",
            "score": 1,
            "explanation": "This model is not a modification or a fine-tune of a prior model."
          },
          {
            "name": "Model Architecture",
            "score": 1,
            "explanation": "Gemini 3 Pro is a sparse mixture-of-experts (MoE) transformer-based model."
          },
          {
            "name": "Model Dependencies",
            "score": 1,
            "explanation": "This model is not a modification or a fine-tune of a prior model."
          },
          {
            "name": "Paper and relevant links",
            "score": 1,
            "explanation": "See the Google DeepMind site for a comprehensive list of model cards."
          },
          {
            "name": "Model Distribution Forms",
            "score": 1,
            "explanation": "The Gemini family of models, including Gemini 3 Pro, are distributed in the following channels."
          }
        ]
      },
      {
        "sectionId": "model-inputs-outputs",
        "sectionSnippets": [
          {
            "snippetId": "model-inputs-outputs-snippet-1",
            "url": "https://web.archive.org/web/20251118111103if_/https:/storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf",
            "snippet": "Gemini 3 Pro \nModel Card\nGemini 3 Pro - Model Card \n \n \n \nModel Cards are intended to provide essential information on Gemini models, including known \nlimitations, mitigation approaches, and safety performance. Model cards may be updated from \ntime-to-time; for example, to include updated evaluations as the model is improved or revised. See the \nGoogle DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous \nmodel cards did. We hope more information about the training dataset, distribution, and intended uses \nwill empower developers with deeper insights and help build more robust and responsible downstream \napplications. \n \nPublished / Model Release: November 2025... Model Information \n \nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of \nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced \nmodel for complex tasks, and can comprehend vast datasets, challenging problems from different \ninformation sources, including text, audio, images, video, and entire code repositories.... Model dependencies: This model is not a modification or a fine-tune of a prior model.   \n \nInputs: Text strings (e.g., a question, a prompt, document(s) to be summarized), images, audio, and \nvideo files, with a token context window of up to 1M.  \n \nOutputs:  Text, with a 64K token output.... data with paired instructions and responses in addition to human preference and tool-use data. Gemini \n3 Pro is trained using reinforcement learning techniques that can leverage multi-step reasoning, \nproblem-solving and theorem-proving data.  \n \nThe training dataset also includes: publicly available datasets that are readily downloadable; data \nobtained by crawlers; licensed data obtained via commercial licensing agreements; user data (i.e., data... specifically designed to handle the massive computations involved in training LLMs and can speed up \ntraining considerably compared to CPUs. TPUs often come with large amounts of high-bandwidth \nmemory, allowing for the handling of large models and batch sizes during training, which can lead to \nbetter model quality. TPU Pods (large clusters of TPUs) also provide a scalable solution for handling the... growing complexity of large foundation models. Training can be distributed across multiple TPU devices \nfor faster and more efficient processing. \n \nThe efficiencies gained through the use of TPUs are aligned with Google's commitment to operate \nsustainably. \n \nSoftware: Training was done using JAX and ML Pathways. \n \n \n2... Distribution \n \nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; \nrespective documentation shared in line: \n \n●​\nGemini App \n●​\nGoogle Cloud / Vertex AI \n●​\nGoogle AI Studio \n●​\nGemini API \n●​\nGoogle AI Mode \n●​... Google Antigravity \n \nOur models are available to downstream providers via an application program interface (API) and \nsubject to relevant terms of use. There is no required hardware or software to use the model. For \nAI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google \nCloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini \nAPI in Vertex AI quickstart.  \n \n \n \n3... applications that require:  agentic performance, advanced coding, long context and/or multimodal \nunderstanding, and/or algorithmic development. \n \nKnown Limitations: Gemini 3 Pro may exhibit some of the general limitations of foundation models, \nsuch as hallucinations. There may also be occasional slowness or timeout issues. The knowledge cutoff... Evaluation Approach: Gemini 3 Pro was developed in partnership with internal safety, security, and \nresponsibility teams. A range of evaluations and red teaming activities were conducted to help improve \nthe model and inform decision-making. These evaluations and activities align with Google's AI Principles \nand responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use... Policy and the Gemini API Additional Terms of Service).    \n \nEvaluation types included but were not limited to:  \n \n●​\nTraining/Development Evaluations including automated and human evaluations carried out \ncontinuously throughout and after the model’s training, to monitor its progress and \nperformance; \n●​\nHuman Red Teaming conducted by specialist teams who sit outside of the model development... concerns. \n \nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and \ndeployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations \ninclude, but are not limited to:  \n \n●​\ndataset filtering;  \n●​... Instrumental \nReasoning​\nLevels 1 + 2 \nCCL not \nreached \n \nMore details can be found in the Gemini 3 Pro Frontier Safety Framework Report. \n \n \n \n8"
          },
          {
            "snippetId": "model-inputs-outputs-snippet-2",
            "url": "https://ai.google.dev/gemini-api/docs/tokens",
            "snippet": "## Context windows\n\nThe models available through the Gemini API have context windows that are measured in tokens. The context window defines how much input you can provide and how much output the model can generate. You can determine the size of the context window by calling the getModels endpoint or by looking in the models documentation.\n\nIn the following example, you can see that the... `gemini-1.5-flash` model has an\n\ninput limit of about 1,000,000 tokens and an output limit of about 8,000 tokens,\n\nwhich means a context window is 1,000,000 tokens.\n\n```\n\nfrom google import genai\n\nclient = genai.Client()\n\nmodel_info = client.models.get(model=\"gemini-2.0-flash\")\n\nprint(f\"{model_info.input_token_limit=}\")\n\nprint(f\"{model_info.output_token_limit=}\")\n\n# ( e.g., input_token_limit=30720, output_token_limit=2048 )count_tokens.py\n\n```... ## Count tokens\n\nAll input to and output from the Gemini API is tokenized, including text, image files, and other non-text modalities.\n\nYou can count tokens in the following ways:... # ( e.g., total_tokens: 10 )\n\nresponse = client.models.generate_content(\n\nmodel=\"gemini-2.0-flash\", contents=prompt\n\n\n\n# The usage_metadata provides detailed token counts.\n\nprint(response.usage_metadata)\n\n# ( e.g., prompt_token_count: 11, candidates_token_count: 73, total_token_count: 84 )count_tokens.py\n\n```... ### Count multi-turn (chat) tokens\n\n```\n\nfrom google import genai\n\nfrom google.genai import types\n\nclient = genai.Client()\n\nchat = client.chats.create(\n\nmodel=\"gemini-2.0-flash\",\n\nhistory=[\n\ntypes.Content(\n\nrole=\"user\", parts=[types.Part(text=\"Hi my name is Bob\")]\n\n),\n\ntypes.Content(role=\"model\", parts=[types.Part(text=\"Hi Bob!\")]),... ],\n\n\n\n# Count tokens for the chat history.\n\nprint(\n\nclient.models.count_tokens(\n\nmodel=\"gemini-2.0-flash\", contents=chat.get_history()\n\n\n\n# ( e.g., total_tokens: 10 )\n\nresponse = chat.send_message(\n\nmessage=\"In one sentence, explain how a computer works to a young child.\"... ### Count multimodal tokens\n\nAll input to the Gemini API is tokenized, including text, image files, and other non-text modalities. Note the following high-level key points about tokenization of multimodal input during processing by the Gemini API:\n\nWith Gemini 2.0, image inputs with both dimensions <=384 pixels are counted as 258 tokens. Images larger in one or both dimensions are cropped and scaled as needed into tiles of 768x768 pixels, each counted as 258 tokens. Prior to Gemini 2.0, images used a fixed 258 tokens.... Video and audio files are converted to tokens at the following fixed rates: video at 263 tokens per second and audio at 32 tokens per second.\n\n#### Media resolutions\n\nGemini 3 Pro Preview introduces granular control over multimodal vision processing with the\n\n`media_resolution` parameter. The\n\n`media_resolution` parameter determines the\n\n**maximum number of tokens allocated per input image or video frame.**\n\nHigher resolutions improve the model's ability to\n\nread fine text or identify small details, but increase token usage and latency.\n\nFor more details about the parameter and how it can impact token calculations, see the media resolution guide.... response = client.models.generate_content(\n\nmodel=\"gemini-2.0-flash\", contents=[prompt, your_image_file]\n\n\n\nprint(response.usage_metadata)\n\n# ( e.g., prompt_token_count: 264, candidates_token_count: 80, total_token_count: 345 )count_tokens.py\n\n```... Example that provides the image as inline data:\n\n```\n\nfrom google import genai\n\nimport PIL.Image\n\nclient = genai.Client()\n\nprompt = \"Tell me about this image\"\n\nyour_image_file = PIL.Image.open(media / \"organ.jpg\")\n\n# Count tokens for combined text and inline image.\n\nprint(\n\nclient.models.count_tokens(\n\nmodel=\"gemini-2.0-flash\", contents=[prompt, your_image_file]... # ( e.g., total_tokens: 263 )\n\nresponse = client.models.generate_content(\n\nmodel=\"gemini-2.0-flash\", contents=[prompt, your_image_file]\n\n\n\nprint(response.usage_metadata)\n\n# ( e.g., prompt_token_count: 264, candidates_token_count: 80, total_token_count: 345 )count_tokens.py\n\n```... ### System instructions and tools\n\nSystem instructions and tools also count towards the total token count for the input.\n\nIf you use system instructions, the\n\n`total_tokens` count increases to\n\nreflect the addition of\n\n`system_instruction`.\n\nIf you use function calling, the\n\n`total_tokens` count increases to reflect the\n\naddition of\n\n`tools`."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-3",
            "url": "https://www.youtube.com/watch?v=-XTNBIVXRGA",
            "snippet": "## AI Papers Podcast Daily\n\n##### Nov 20, 2025 (0:13:04)\n\nThe **Gemini 3 Pro Model Card**, published by Google DeepMind in November 2025, introduces the latest generation of the Gemini series, describing it as Google’s most advanced, highly-capable, and natively multimodal reasoning model. Architecturally, Gemini 3 Pro is a sparse Mixture-of-Experts (MoE) transformer-based model, capable of understanding vast datasets and complex problems from various inputs, including text, images, audio, video, and code repositories, utilizing an extensive 1M token context window.... The model was trained using reinforcement learning techniques on a large, diverse dataset, and processed efficiently using Google’s specialized Tensor Processing Units (TPUs). Intended for applications requiring enhanced reasoning, intelligence, advanced coding, and agentic performance, Gemini 3 Pro significantly outperforms the previous Gemini 2.5 Pro model across a range of benchmarks. Development included continuous training evaluations, human red teaming by specialist teams, and adherence to safety policies designed to prevent the generation of harmful content, confirming that Gemini 3 Pro satisfied required launch thresholds for child safety and showed improved overall safety performance compared to its predecessor.... https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf... ### Transcript\n\n{ts:0} Welcome to the AI Papers podcast daily. We have a foundational piece of\n{ts:4} documentation that has just dropped and uh it's really going to set the pace for advanced models over the next year.\n{ts:11} We're going deep on the technical mandate for what Google DeepMind thinks is the new frontier.... {ts:16} That's absolutely right. Our focus today is the brand new Gemini 3 Pro model card just published in November 2025. And\n{ts:25} this is not a small update. This document signals a pretty massive architectural and capability shift.\n{ts:31} Google is calling it their most advanced model for complex tasks.... {ts:113} manage all that input which brings us to scale. I mean and the number that immediately jumps off the\n{ts:117} page is the context window. We're talking about an input of up to 1 million tokens. 1 million.\n{ts:122} Right. And for context a million tokens.... With a million tokens, the model can keep track of details buried anywhere in that\n{ts:146} entire stack of information. But there's an interesting choice here. The input is 1 million tokens, but the\n{ts:152} model card says the text output is limited to 64,000.... And this is where it gets really interesting where we go from just scale to efficiency. The architecture itself.\n{ts:190} The model card specifies Gemini 3 Pro uses a transformer-based sparse mixture of experts or MOI. This\n{ts:198} is a huge concept for anyone building with these models.... {ts:236} So if I ask a programming question, it wakes up the coding expert and maybe the logic expert, but the poetry expert gets\n{ts:243} to stay asleep. Precisely. And that's the core benefit they highlight in the model card. It\n{ts:248} lets Google decouple the model's total capacity, you know, how smart it is in theory, from the actual computation cost... It is. And that efficiency leads to the\n{ts:272} next question. How do you train something this complex? Let's get into the engine room. the training and the\n{ts:277} infrastructure, the pre-training data set is just immense. We're talking web documents, code, images, audio, video.... And you can't talk about training\n{ts:370} without the hardware. This all runs on Google's own silicon. Their tensor processing units, TPUs on scalable TPU\n{ts:378} pods. Yeah, the TPUs are just powerhouses for the kind of math these models need. And\n{ts:383} using their own custom hardware isn't just about speed.... {ts:571} development. If you need a model that can think several steps ahead, this is the tool.\n{ts:575} But even with this huge leap, we have to be realistic. What are the known limitations that the model card flags?\n{ts:582} Well, the usual foundation model issues are still there, primarily hallucinations.... It's not perfect.\n{ts:587} Developers have to build in safeguards. They also note occasional slowness or timeout issues, which probably reflects\n{ts:593} the complexity of the MOI routing. And a really critical point for anyone who needs up-to-date information.\n{ts:599} Yes, the card clearly states the model's knowledge cutoff was January 2025."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-4",
            "url": "https://huggingface.co/datasets/multimodalart/google-gemini-3-pro-pre-release-model-card",
            "snippet": "# Gemini 3 Pro Model Card\n\n### ⚠️ This is a mirror to the pre-release model card taht was officially published by Google @ https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf and subsequentially yanked. It will get outdated by the actual release version in a few hours (date of publication Nov 18, 13:16 CET)\n\n*Model card published: November, 2025*... # Gemini 3 Pro - Model Card\n\n*Model Cards* *are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from*\n\n*time-to-time; for example, to include updated evaluations as the model is improved or revised. See the* *Google DeepMind site* *for a comprehensive list of model cards.*... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\n*Published / Model Release: November 2025*... ## Model Information\n\n**Description**: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of\n\nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google's most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from diﬀerent information sources, including text, audio, images, video, and entire code repositories.... **Model dependencies:** This model is not a modiﬁcation or a ﬁne-tune of a prior model.\n\n**Inputs:** Text strings (e.g., a question, a prompt, document(s) to be summarized), images, audio, and video ﬁles, with a token context window of up to 1M.... **Outputs**: Text, with a 64K token output.\n\n**Architecture**: Gemini 3 Pro is a sparse mixture-of-experts (MoE) (Clark et al., 2022; Du et al., 2021; Fedus et al., 2021; Jiang et al., 2024, Lepikhin et al., 2020; Riquelme et al., 2021; Roller et al., 2021; Shazeer et al., 2017) transformer-based model (Vaswani et al., 2017) with native multimodal support for text, vision, and audio inputs.... ## Implementation and Sustainability\n\n**Hardware:** Gemini 3 Pro was trained using Google's Tensor Processing Units (TPUs). TPUs are speciﬁcally designed to handle the massive computations involved in training LLMs and can speed up training considerably compared to CPUs. TPUs often come with large amounts of high-bandwidth memory, allowing for the handling of large models and batch sizes during training, which can lead to better model quality. TPU Pods (large clusters of TPUs) also provide a scalable solution for handling the growing complexity of large foundation models. Training can be distributed across multiple TPU devices for faster and more eﬃcient processing.... The eﬃciencies gained through the use of TPUs are aligned with Google's commitment to operate sustainably.\n\n**Software:** Training was done using JAX and ML Pathways.... ## Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\nOur models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... ## Intended Usage and Limitations\n\n**Beneﬁt and Intended Usage:** Gemini 3 Pro is our most intelligent and adaptive model yet, capable of helping with real-world complexity, solving problems that require enhanced reasoning and intelligence, creativity, strategic planning and making improvements step-by-step. It is particularly well-suited for applications that require: agentic performance, advanced coding, long context and/or multimodal understanding, and/or algorithmic development.... **Training and Development Evaluation Results:** Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustiﬁed refusals low. We mark improvements in green and regressions in red."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-5",
            "url": "https://ai.google.dev/gemini-api/docs/changelog",
            "snippet": "This page documents updates to the Gemini API.\n\n## November 20, 2025\n\n- Released Gemini 3 Pro Image Preview,\n\n`gemini-3-pro-image-preview`, the next iteration to the Nano Banana model. Read the Image generation page for more details.... ## November 18, 2025\n\nLaunched the first Gemini 3 series model,\n\n`gemini-3-pro-preview`, our state-of-the-art reasoning and multimodal understanding model with powerful agentic and coding capabilities.\n\nIn addition to improvements in intelligence and performance, Gemini 3 Pro Preview introduces new behavior around:\n\nRead the Gemini 3 Developer Guide for migration, new features, and specs.... ## October 29, 2025\n\n- Launched the new logging and datasets tool for the Gemini API.... ## September 25, 2025\n\nReleased Gemini Robotics-ER 1.5 model in preview. See the Robotics overview to learn about how to use the model for your robotics application.\n\nLaunched following preview models:\n\n`gemini-2.5-flash-preview-09-2025`\n\n`gemini-2.5-flash-lite-preview-09-2025`\n\nSee the Models page for details.... **Model updates:**\n\n- Released\n\n`gemini-2.5-flash-preview-05-20`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n- Released the\n\n`gemini-2.5-pro-preview-tts`and... `speechConfig`.\n\n**Text Streaming:**Receive text responses incrementally as they are generated, enabling faster display to the user. **Token Usage Reporting:**Gain insights into usage with detailed token counts provided in the\n\n`usageMetadata`field of server messages, broken down by modality and prompt or response phases.\n\n\n\n## April 4, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version with billing enabled. You can continue to use\n\n`gemini-2.5-pro-exp-03-25`on the free tier.... ## March 12, 2025\n\n**Model updates:**\n\n- Launched an experimental Gemini 2.0 Flash model capable of image generation and editing.\n\n- Released\n\n`gemma-3-27b-it`, available on AI Studio and through the Gemini API, as part of the Gemma 3 launch.\n\n**API updates:**\n\n- Added support for YouTube URLs as a media source.\n\n- Added support for including an inline video of less than 20MB.... ## February 11, 2025\n\n**API updates:**\n\n- Updates on the OpenAI libraries compatibility.\n\n## February 6, 2025\n\n**Model updates:**\n\n- Released\n\n`imagen-3.0-generate-002`, a generally available (GA) version of Imagen 3 in the Gemini API.\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for Java for public preview.... ## October 3, 2024\n\n**Model updates:**\n\n- Released\n\n`gemini-1.5-flash-8b-001`, a stable version of our smallest Gemini API model.... ## September 24, 2024\n\n**Model updates:**\n\n- Released\n\n`gemini-1.5-pro-002`and\n\n`gemini-1.5-flash-002`, two new stable versions of Gemini 1.5 Pro and 1.5 Flash, for general availability.... - Updated the\n\n`gemini-1.5-pro-latest`model code to use\n\n`gemini-1.5-pro-002`and the\n\n`gemini-1.5-flash-latest`model code to use\n\n`gemini-1.5-flash-002`.\n\n- Released... ## August 27, 2024\n\n**Model updates:**\n\n- Released the following\n\nexperimental models:\n\n`gemini-1.5-pro-exp-0827`\n\n`gemini-1.5-flash-exp-0827`\n\n`gemini-1.5-flash-8b-exp-0827`... ## May 10, 2024\n\n**Model updates:**\n\n- Released Gemini 1.5 Flash\n\n\n\n`gemini-1.5-flash-latest`) in preview.... ## March 19, 2024\n\n**Model updates:**\n\n- Added support for tuning Gemini 1.0 Pro in Google AI Studio or with the Gemini API.... - Streaming available through the\n\n`StreamGenerateContent`method.\n\n- Multimodal capability: Image is a new supported modality\n\n- New beta features:\n\n- Function Calling\n\n- Semantic Retriever\n\n- Attributed Question Answering (AQA)\n\n- Updated candidate count: Gemini models only return 1 candidate.\n\n- Different Safety Settings and SafetyRating categories. See safety settings for more details.\n\n- Tuning models is not yet supported for Gemini models (Work in progress)."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Inputs",
            "score": 1,
            "explanation": "Text strings (e.g., a question, a prompt, document(s) to be summarized), images, audio, and video files, with a token context window of up to 1M."
          },
          {
            "name": "Outputs",
            "score": 1,
            "explanation": "Text, with a 64K token output."
          },
          {
            "name": "Token Count",
            "score": 1,
            "explanation": "The context window defines how much input you can provide and how much output the model can generate."
          }
        ]
      },
      {
        "sectionId": "model-data",
        "sectionSnippets": [
          {
            "snippetId": "model-data-snippet-1",
            "url": "https://web.archive.org/web/20251118111103if_/https:/storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf",
            "snippet": "Gemini 3 Pro \nModel Card\nGemini 3 Pro - Model Card \n \n \n \nModel Cards are intended to provide essential information on Gemini models, including known \nlimitations, mitigation approaches, and safety performance. Model cards may be updated from \ntime-to-time; for example, to include updated evaluations as the model is improved or revised. See the \nGoogle DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous \nmodel cards did. We hope more information about the training dataset, distribution, and intended uses \nwill empower developers with deeper insights and help build more robust and responsible downstream \napplications. \n \nPublished / Model Release: November 2025... Model Information \n \nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of \nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced \nmodel for complex tasks, and can comprehend vast datasets, challenging problems from different \ninformation sources, including text, audio, images, video, and entire code repositories.... Model Data \n \nTraining Dataset: The pre-training dataset was a large-scale, diverse collection of data encompassing a \nwide range of domains and modalities, which included publicly-available web-documents, text, code \n(various programming languages), images, audio (including speech and other audio types) and video. \nThe post-training dataset consisted of vetted instruction tuning data and was a collection of multimodal... data with paired instructions and responses in addition to human preference and tool-use data. Gemini \n3 Pro is trained using reinforcement learning techniques that can leverage multi-step reasoning, \nproblem-solving and theorem-proving data.  \n \nThe training dataset also includes: publicly available datasets that are readily downloadable; data \nobtained by crawlers; licensed data obtained via commercial licensing agreements; user data (i.e., data... collected from users of Google products and services to train AI models, along with user interactions \nwith the model) in accordance with Google’s relevant terms of service, privacy policy, service-specific \npolicies, and pursuant to user controls, where appropriate; other datasets that Google acquires or \ngenerates in the course of its business operations, or directly from its workforce; and AI-generated... synthetic data.   \n \nTraining Data Processing: Data filtering and preprocessing included techniques such as deduplication, \nhonoring robots.txt, safety filtering in-line with Google's commitment to advancing AI safely and \nresponsibly, and quality filtering to mitigate risks and improve training data reliability. Once data is \ncollected, it is cleaned and preprocessed to make it suitable for training. This process involves, on a... growing complexity of large foundation models. Training can be distributed across multiple TPU devices \nfor faster and more efficient processing. \n \nThe efficiencies gained through the use of TPUs are aligned with Google's commitment to operate \nsustainably. \n \nSoftware: Training was done using JAX and ML Pathways. \n \n \n2... Distribution \n \nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; \nrespective documentation shared in line: \n \n●​\nGemini App \n●​\nGoogle Cloud / Vertex AI \n●​\nGoogle AI Studio \n●​\nGemini API \n●​\nGoogle AI Mode \n●​... Intended Usage and Limitations \n \nBenefit and Intended Usage: Gemini 3 Pro is our most intelligent and adaptive model yet, capable of \nhelping with real-world complexity, solving problems that require enhanced reasoning and intelligence, \ncreativity, strategic planning and making improvements step-by-step. It is particularly well-suited for... Evaluation Approach: Gemini 3 Pro was developed in partnership with internal safety, security, and \nresponsibility teams. A range of evaluations and red teaming activities were conducted to help improve \nthe model and inform decision-making. These evaluations and activities align with Google's AI Principles \nand responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use... Policy and the Gemini API Additional Terms of Service).    \n \nEvaluation types included but were not limited to:  \n \n●​\nTraining/Development Evaluations including automated and human evaluations carried out \ncontinuously throughout and after the model’s training, to monitor its progress and \nperformance; \n●​\nHuman Red Teaming conducted by specialist teams who sit outside of the model development... Training and Development Evaluation Results:  Results for some of the internal safety evaluations \nconducted during the development phase are listed below. The evaluation results are for automated \nevaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage \nincrease or decrease in performance compared to the indicated model, as described below. Overall,... concerns. \n \nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and \ndeployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations \ninclude, but are not limited to:  \n \n●​\ndataset filtering;  \n●​"
          },
          {
            "snippetId": "model-data-snippet-2",
            "url": "https://huggingface.co/datasets/multimodalart/google-gemini-3-pro-pre-release-model-card",
            "snippet": "# Gemini 3 Pro - Model Card\n\n*Model Cards* *are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from*\n\n*time-to-time; for example, to include updated evaluations as the model is improved or revised. See the* *Google DeepMind site* *for a comprehensive list of model cards.*... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\n*Published / Model Release: November 2025*... ## Model Information\n\n**Description**: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of\n\nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google's most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from diﬀerent information sources, including text, audio, images, video, and entire code repositories.... **Model dependencies:** This model is not a modiﬁcation or a ﬁne-tune of a prior model.\n\n**Inputs:** Text strings (e.g., a question, a prompt, document(s) to be summarized), images, audio, and video ﬁles, with a token context window of up to 1M.... ## Model Data\n\n**Training Dataset:** The pre-training dataset was a large-scale, diverse collection of data encompassing a wide range of domains and modalities, which included publicly-available web-documents, text, code (various programming languages), images, audio (including speech and other audio types) and video.\n\nThe post-training dataset consisted of vetted instruction tuning data and was a collection of multimodal data with paired instructions and responses in addition to human preference and tool-use data. Gemini 3 Pro is trained using reinforcement learning techniques that can leverage multi-step reasoning, problem-solving and theorem-proving data.... The training dataset also includes: publicly available datasets that are readily downloadable; data obtained by crawlers; licensed data obtained via commercial licensing agreements; user data (i.e., data collected from users of Google products and services to train AI models, along with user interactions with the model) in accordance with Google's relevant terms of service, privacy policy, service-speciﬁc policies, and pursuant to user controls, where appropriate; other datasets that Google acquires or generates in the course of its business operations, or directly from its workforce; and AI-generated synthetic data.... **Training Data Processing:** Data ﬁltering and preprocessing included techniques such as deduplication, honoring robots.txt, safety ﬁltering in-line with Google's commitment to advancing AI safely and responsibly, and quality ﬁltering to mitigate risks and improve training data reliability. Once data is collected, it is cleaned and preprocessed to make it suitable for training. This process involves, on a\n\ncase-by-case basis, ﬁltering irrelevant or harmful content, text, and other modalities, including ﬁltering content that is pornographic, violent, or violative of child sexual abuse material (CSAM) laws.... ## Implementation and Sustainability\n\n**Hardware:** Gemini 3 Pro was trained using Google's Tensor Processing Units (TPUs). TPUs are speciﬁcally designed to handle the massive computations involved in training LLMs and can speed up training considerably compared to CPUs. TPUs often come with large amounts of high-bandwidth memory, allowing for the handling of large models and batch sizes during training, which can lead to better model quality. TPU Pods (large clusters of TPUs) also provide a scalable solution for handling the growing complexity of large foundation models. Training can be distributed across multiple TPU devices for faster and more eﬃcient processing.... The eﬃciencies gained through the use of TPUs are aligned with Google's commitment to operate sustainably.\n\n**Software:** Training was done using JAX and ML Pathways.... ## Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\nOur models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... ## Intended Usage and Limitations\n\n**Beneﬁt and Intended Usage:** Gemini 3 Pro is our most intelligent and adaptive model yet, capable of helping with real-world complexity, solving problems that require enhanced reasoning and intelligence, creativity, strategic planning and making improvements step-by-step. It is particularly well-suited for applications that require: agentic performance, advanced coding, long context and/or multimodal understanding, and/or algorithmic development."
          },
          {
            "snippetId": "model-data-snippet-3",
            "url": "https://www.studocu.com/en-gb/document/university-of-east-london/financial-accounting/gemini-3-pro-model-card-insights-safety-evaluations-nov-2025/146206142",
            "snippet": "Key Concepts\n\n- Multimodal Capabilities: Gemini 3 Pro processes text, images, audio, and video, enhancing its versatility in handling diverse data types.\n\n- Sparse MoE Architecture: This model employs a mixture of experts approach, optimizing computational efficiency by activating only relevant parameters.\n\n- Training Dataset: A comprehensive dataset includes various modalities and is processed to ensure quality and safety.... - Evaluation Methods: The model undergoes rigorous testing across benchmarks to assess its reasoning and multimodal performance.\n\n- Safety Policies: Strict guidelines are in place to prevent harmful content generation, ensuring responsible AI use.\n\n- Intended Applications: Designed for complex problem-solving, the model excels in areas requiring advanced reasoning and creativity.\n\n## Preview text\n\nModel card published: November, 2025... ## Gemini 3 Pro\n\n## Model Card\n\n### Gemini 3 Pro - Model Card\n\nModel Cards are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from time-to-time; for example, to include updated evaluations as the model is improved or revised. See the Google DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\nPublished / Model Release: November 2025... #### Model Information\n\nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of highly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from different information sources, including text, audio, images, video, and entire code repositories.... Model dependencies: This model is not a modification or a fine-tune of a prior model.\n\nInputs: Text strings (e., a question, a prompt, document(s) to be summarized), images, audio, and video files, with a token context window of up to 1M.\n\nOutputs: Text, with a 64K token output.... Architecture: Gemini 3 Pro is a sparse mixture-of-experts (MoE) (Clark et al., 2022; Du et al., 2021; Fedus et al., 2021; Jiang et al., 2024, Lepikhin et al., 2020; Riquelme et al., 2021; Roller et al., 2021; Shazeer et al., 2017 ) transformer-based model (Vaswani et al., 2017) with native multimodal support for text, vision, and audio inputs.... #### Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\n● Gemini App ● Google Cloud / Vertex AI ● Google AI Studio ● Gemini API ● Google AI Mode ● Google Antigravity... Our models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... #### Evaluation\n\nApproach: Gemini 3 Pro was evaluated across a range of benchmarks, including reasoning, multimodal capabilities, agentic tool use, multi-lingual performance, and long-context. Additional benchmarks and details on approach, results and their methodologies can be found at: deepmind/models/evals/gemini-3-pro.... Training and Development Evaluation Results: Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2 Pro across both safety and tone, while keeping unjustified refusals low. We mark improvements in green and regressions in red.... Human Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level findings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams to protect children online and meet Google’s commitments to child safety across our models and Google products.... ● dataset filtering; ● conditional pre-training; ● supervised fine-tuning; ● reinforcement learning from human and critic feedback; ● safety policies and desiderata; ● product-level mitigations such as safety filtering.\n\nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2 Pro but still an open research problem), and b) possible degradation in multi-turn conversations."
          },
          {
            "snippetId": "model-data-snippet-4",
            "url": "https://ai.google.dev/gemini-api/docs/changelog",
            "snippet": "This page documents updates to the Gemini API.\n\n## November 20, 2025\n\n- Released Gemini 3 Pro Image Preview,\n\n`gemini-3-pro-image-preview`, the next iteration to the Nano Banana model. Read the Image generation page for more details.... ## November 18, 2025\n\nLaunched the first Gemini 3 series model,\n\n`gemini-3-pro-preview`, our state-of-the-art reasoning and multimodal understanding model with powerful agentic and coding capabilities.\n\nIn addition to improvements in intelligence and performance, Gemini 3 Pro Preview introduces new behavior around:\n\nRead the Gemini 3 Developer Guide for migration, new features, and specs.... ## November 10, 2025\n\nThe following model is deprecated:\n\n`imagen-3.0-generate-002`\n\nUse Imagen 4 instead. Refer to the Gemini deprecations table for more details.\n\n\n\n## November 6, 2025\n\n- Launched the File Search API to public preview, enabling developers to ground responses in their own data. Read the new File Search page for more info.... ## October 29, 2025\n\n- Launched the new logging and datasets tool for the Gemini API.... ## September 25, 2025\n\nReleased Gemini Robotics-ER 1.5 model in preview. See the Robotics overview to learn about how to use the model for your robotics application.\n\nLaunched following preview models:\n\n`gemini-2.5-flash-preview-09-2025`\n\n`gemini-2.5-flash-lite-preview-09-2025`\n\nSee the Models page for details.... ## June 24, 2025\n\n- Released Imagen 4 Ultra and Standard Preview models. To learn more, see the Image generation page.... ## June 17, 2025\n\n- Released\n\n`gemini-2.5-pro`, the stable version of our most powerful model, now with adaptive thinking. To learn more, see Gemini 2.5 Pro and Thinking.\n\n`gemini-2.5-pro-preview-05-06`will be redirected to... ## June 05, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-06-05`, a new version of our most powerful model, now with adaptive thinking. To learn more, see Gemini 2.5 Pro Preview and Thinking.\n\n`gemini-2.5-pro-preview-05-06`will be redirected to\n\n`gemini-2.5-pro`on June 26, 2025.... **Model updates:**\n\n- Released\n\n`gemini-2.5-flash-preview-05-20`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n- Released the\n\n`gemini-2.5-pro-preview-tts`and... ## May 7, 2025\n\n- Released\n\n`gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see Image generation and Gemini 2.0 Flash Preview Image Generation.\n\n## May 6, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling.\n\n`gemini-2.5-pro-preview-03-25`will automatically point to the new version of the model.... ## February 11, 2025\n\n**API updates:**\n\n- Updates on the OpenAI libraries compatibility.\n\n## February 6, 2025\n\n**Model updates:**\n\n- Released\n\n`imagen-3.0-generate-002`, a generally available (GA) version of Imagen 3 in the Gemini API.\n\n**SDK updates:**\n\n- Released the Google Gen AI SDK for Java for public preview.... ## March 19, 2024\n\n**Model updates:**\n\n- Added support for tuning Gemini 1.0 Pro in Google AI Studio or with the Gemini API.... ## December 13 2023\n\n**Model updates:**\n\n- gemini-pro: New text model for a wide variety of tasks. Balances capability and efficiency.\n\n- gemini-pro-vision: New multimodal model for a wide variety of tasks. Balances capability and efficiency.\n\n- embedding-001: New embeddings model.\n\n- aqa: A new specially tuned model that is trained to answer questions using text passages for grounding generated answers.... - Streaming available through the\n\n`StreamGenerateContent`method.\n\n- Multimodal capability: Image is a new supported modality\n\n- New beta features:\n\n- Function Calling\n\n- Semantic Retriever\n\n- Attributed Question Answering (AQA)\n\n- Updated candidate count: Gemini models only return 1 candidate.\n\n- Different Safety Settings and SafetyRating categories. See safety settings for more details.\n\n- Tuning models is not yet supported for Gemini models (Work in progress)."
          },
          {
            "snippetId": "model-data-snippet-5",
            "url": "https://en.wikipedia.org/wiki/Gemini_(language_model)",
            "snippet": "**Gemini** is a family of multimodal large language models (LLMs) developed by Google DeepMind, and the successor to LaMDA and PaLM 2. Comprising Gemini Pro, Gemini Flash, and Gemini Lite, it was announced on December 6, 2023. It powers the chatbot of the same name.... At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2.... It was made available only in English. Touted as Google's \"largest and most capable AI model\" and designed to emulate human behavior, the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs), and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.... Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test, obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13, while Gemini Nano will be made available to Android developers as well.... It also introduces improved agentic capabilities, a new Google Gen AI SDK, and \"Jules,\" an experimental AI coding agent for GitHub. Additionally, Google Colab is integrating Gemini 2.0 to generate data science notebooks from natural language. Gemini 2.0 was available through the Gemini chat interface for all users as \"Gemini 2.0 Flash experimental\".... On January 30, 2025, Google released Gemini 2.0 Flash as the new default model, with Gemini 1.5 Flash still available for usage. This was followed by the release of Gemini 2.0 Pro on February 5, 2025. Additionally, Google released Gemini 2.0 Flash Thinking Experimental, which details the language model's thinking process when responding to prompts.... On March 12, 2025, Google also announced Gemini Robotics, a vision-language-action model based on the Gemini 2.0 family of models.\n\nThe next day, Google announced that Gemini in Android Studio would be able to understand simple UI mockups and transform them into working Jetpack Compose code.... Gemini 2.5 Pro Experimental was released on March 25, 2025, described by Google as its most intelligent AI model yet, featuring enhanced reasoning and coding capabilities, and a \"thinking model\" capable of reasoning through steps before responding, using techniques like chain-of-thought prompting, whilst maintaining native multimodality and launching with a 1 million token context window.... At Google I/O 2025, Google announced significant updates to its Gemini core models. Gemini 2.5 Flash became the default model, delivering faster responses. Gemini 2.5 Pro was introduced as the most advanced Gemini model, featuring reasoning, coding capabilities, and the new Deep Think mode for complex tasks. Both 2.5 Pro and Flash support native audio output and improved security.... On June 17, 2025, Google announced general availability for 2.5 Pro and Flash. They also introduced Gemini 2.5 Flash-Lite that same day, a model optimized for speed and cost-efficiency.... On November 18th, 2025, Google announced the release of 3.0 Pro and 3.0 Deep Think. These new models replace 2.5 Pro and Flash, and are the most powerful models available as of November 2025. On release 3.0 Pro outperformed major AI models in 19 out of 20 benchmarks tested, including surpassing OpenAI's GPT-5 Pro in Humanity's Last Exam, with an accuracy of 41% compared to OpenAI's 31.64%.... |3.0 Pro Image (Nano Banana Pro)|20 November 2025|Active|An improved version of Nano Banana which includes better text rendering and better real world knowledge.|... Gemini and Gemma models are decoder-only transformers, with modifications to allow efficient training and inference on TPUs. The 1.0 generation uses multi-query attention.\n\n**Technical specifications of Gemini models**... ## External links\n- Official website\n- Press release via *The Keyword*\n- White paper for 1.0 and 1.5\n- White paper for Gemini 3.0 Pro [1]\n\nCategories: - 2023 software\n- Chatbots\n- Google DeepMind\n- Google software\n- Large language models\n- Multimodal interaction\n- 2023 in artificial intelligence\n- Generative pre-trained transformers"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Training Dataset",
            "score": 1,
            "explanation": "The pre-training dataset was a large-scale, diverse collection of data encompassing a wide range of domains and modalities."
          },
          {
            "name": "Training Data Processing",
            "score": 1,
            "explanation": "Data filtering and preprocessing included techniques such as deduplication, honoring robots.txt, safety filtering in-line with Google's commitment to advancing AI safely and responsibly."
          },
          {
            "name": "Knowledge Count",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      },
      {
        "sectionId": "model-implementation-sustainability",
        "sectionSnippets": [
          {
            "snippetId": "model-implementation-sustainability-snippet-1",
            "url": "https://ia601703.us.archive.org/16/items/gemini-3-pro-model-card/Gemini-3-Pro-Model-Card.pdf",
            "snippet": "Gemini 3 Pro \nModel Card\nGemini 3 Pro - Model Card \n \n \n \nModel Cards are intended to provide essential information on Gemini models, including known \nlimitations, mitigation approaches, and safety performance. Model cards may be updated from \ntime-to-time; for example, to include updated evaluations as the model is improved or revised. See the \nGoogle DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous \nmodel cards did. We hope more information about the training dataset, distribution, and intended uses \nwill empower developers with deeper insights and help build more robust and responsible downstream \napplications. \n \nPublished / Model Release: November 2025... Model Information \n \nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of \nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced \nmodel for complex tasks, and can comprehend vast datasets, challenging problems from different \ninformation sources, including text, audio, images, video, and entire code repositories.... data with paired instructions and responses in addition to human preference and tool-use data. Gemini \n3 Pro is trained using reinforcement learning techniques that can leverage multi-step reasoning, \nproblem-solving and theorem-proving data.  \n \nThe training dataset also includes: publicly available datasets that are readily downloadable; data \nobtained by crawlers; licensed data obtained via commercial licensing agreements; user data (i.e., data... collected from users of Google products and services to train AI models, along with user interactions \nwith the model) in accordance with Google’s relevant terms of service, privacy policy, service-specific \npolicies, and pursuant to user controls, where appropriate; other datasets that Google acquires or \ngenerates in the course of its business operations, or directly from its workforce; and AI-generated... synthetic data.   \n \nTraining Data Processing: Data filtering and preprocessing included techniques such as deduplication, \nhonoring robots.txt, safety filtering in-line with Google's commitment to advancing AI safely and \nresponsibly, and quality filtering to mitigate risks and improve training data reliability. Once data is \ncollected, it is cleaned and preprocessed to make it suitable for training. This process involves, on a... growing complexity of large foundation models. Training can be distributed across multiple TPU devices \nfor faster and more efficient processing. \n \nThe efficiencies gained through the use of TPUs are aligned with Google's commitment to operate \nsustainably. \n \nSoftware: Training was done using JAX and ML Pathways. \n \n \n2... Distribution \n \nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; \nrespective documentation shared in line: \n \n●​\nGemini App \n●​\nGoogle Cloud / Vertex AI \n●​\nGoogle AI Studio \n●​\nGemini API \n●​\nGoogle AI Mode \n●​... Google Antigravity \n \nOur models are available to downstream providers via an application program interface (API) and \nsubject to relevant terms of use. There is no required hardware or software to use the model. For \nAI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google \nCloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini \nAPI in Vertex AI quickstart.  \n \n \n \n3... Evaluation Approach: Gemini 3 Pro was developed in partnership with internal safety, security, and \nresponsibility teams. A range of evaluations and red teaming activities were conducted to help improve \nthe model and inform decision-making. These evaluations and activities align with Google's AI Principles \nand responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use... Policy and the Gemini API Additional Terms of Service).    \n \nEvaluation types included but were not limited to:  \n \n●​\nTraining/Development Evaluations including automated and human evaluations carried out \ncontinuously throughout and after the model’s training, to monitor its progress and \nperformance; \n●​\nHuman Red Teaming conducted by specialist teams who sit outside of the model development... Training and Development Evaluation Results:  Results for some of the internal safety evaluations \nconducted during the development phase are listed below. The evaluation results are for automated \nevaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage \nincrease or decrease in performance compared to the indicated model, as described below. Overall,... concerns. \n \nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and \ndeployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations \ninclude, but are not limited to:  \n \n●​\ndataset filtering;  \n●​... Instrumental \nReasoning​\nLevels 1 + 2 \nCCL not \nreached \n \nMore details can be found in the Gemini 3 Pro Frontier Safety Framework Report. \n \n \n \n8"
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-2",
            "url": "https://ai.google.dev/gemini-api/docs/models",
            "snippet": "OUR MOST INTELLIGENT MODEL\n\n## Gemini 3 Pro\n\nThe best model in the world for multimodal understanding, and our most powerful agentic and vibe-coding model yet, delivering richer visuals and deeper interactivity, all built on a foundation of state-of-the-art reasoning.\n\n### Expand to learn more\n\n#### Model details... ### Gemini 3 Pro Preview\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-3-pro-preview`|\n|Supported data types|Text, Image, Video, Audio, and PDF Text|\n|Token limits [*]|1,048,576 65,536|\n|Capabilities|Not supported Supported Supported Supported Supported Supported Not supported Not supported Not supported Supported Supported Supported Supported|\n|Versions||\n|Latest update|November 2025|\n|Knowledge cutoff|January 2025|... ### Gemini 3 Pro Image Preview\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-3-pro-image-preview`|\n|Supported data types|Image and Text Image and Text|\n|Token limits [*]|65,536 32,768|\n|Capabilities|Not supported Supported Not supported Not supported Not supported Not supported Not supported Supported Not supported Supported Supported Supported Not supported|\n|Versions||\n|Latest update|November 2025|\n|Knowledge cutoff|January 2025|\nOUR ADVANCED THINKING MODEL... ### Gemini 2.5 Pro\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.5-pro`|\n|Supported data types|Audio, images, video, text, and PDF Text|\n|Token limits [*]|1,048,576 65,536|\n|Capabilities|Not supported Supported Supported Supported Supported Supported Supported Not supported Not supported Supported Supported Supported Supported|\n|Versions||\n|Latest update|June 2025|\n|Knowledge cutoff|January 2025|... ### Gemini 2.5 Pro TTS\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.5-pro-preview-tts`|\n|Supported data types|Text Audio|\n|Token limits [*]|8,192 16,384|\n|Capabilities|Supported Not Supported Not supported Not supported Not Supported Not supported Not supported Not supported Not supported Not supported Not supported Not supported Not supported|\n|Versions||\n|Latest update|May 2025|\nFAST AND INTELLIGENT... ### Gemini 2.5 Flash Preview\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.5-flash-preview-09-2025`|\n|Supported data types|Text, images, video, audio Text|\n|Token limits [*]|1,048,576 65,536|\n|Capabilities|Not supported Supported Supported Supported Supported Supported Not supported Not supported Not supported Supported Supported Supported Supported|\n|Versions||\n|Latest update|September 2025|\n|Knowledge cutoff|January 2025|... |Versions|gemini-live-2.5-flash-preview will be deprecated on December 09, 2025|\n|Latest update|September 2025|\n|Knowledge cutoff|January 2025|... ## Gemini 2.5 Flash-Lite\n\nOur fastest flash model optimized for cost-efficiency and high throughput.\n\n### Expand to learn more\n\n#### Model details... ### Gemini 2.0 Flash\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.0-flash`|\n|Supported data types|Audio, images, video, and text Text|\n|Token limits [*]|1,048,576 8,192|\n|Capabilities|Not supported Supported Supported Supported Not supported Supported Supported Not supported Supported Supported Supported Experimental Not supported|\n|Versions||\n|Latest update|February 2025|\n|Knowledge cutoff|August 2024|... ### Expand to learn more\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.... ## Model version name patterns\n\nGemini models are available in either\n\n*stable*, *preview*, *latest*, or\n\n*experimental* versions.\n\n### Stable\n\nPoints to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nFor example:\n\n`gemini-2.5-flash`.... ### Preview\n\nPoints to a preview model which may be used for production. Preview models will typically have billing enabled, might come with more restrictive rate limits and will be deprecated with at least 2 weeks notice.\n\nFor example:\n\n`gemini-2.5-flash-preview-09-2025`.... ### Experimental\n\nPoints to an experimental model which will typically be not be suitable for production use and come with more restrictive rate limits. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nExperimental models are not stable and availability of model endpoints is subject to change.\n\n## Model deprecations\n\nFor information about model deprecations, visit the Gemini deprecations page."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-3",
            "url": "https://huggingface.co/datasets/multimodalart/google-gemini-3-pro-pre-release-model-card",
            "snippet": "# Gemini 3 Pro - Model Card\n\n*Model Cards* *are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from*\n\n*time-to-time; for example, to include updated evaluations as the model is improved or revised. See the* *Google DeepMind site* *for a comprehensive list of model cards.*... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\n*Published / Model Release: November 2025*... ## Model Data\n\n**Training Dataset:** The pre-training dataset was a large-scale, diverse collection of data encompassing a wide range of domains and modalities, which included publicly-available web-documents, text, code (various programming languages), images, audio (including speech and other audio types) and video.\n\nThe post-training dataset consisted of vetted instruction tuning data and was a collection of multimodal data with paired instructions and responses in addition to human preference and tool-use data. Gemini 3 Pro is trained using reinforcement learning techniques that can leverage multi-step reasoning, problem-solving and theorem-proving data.... ## Implementation and Sustainability\n\n**Hardware:** Gemini 3 Pro was trained using Google's Tensor Processing Units (TPUs). TPUs are speciﬁcally designed to handle the massive computations involved in training LLMs and can speed up training considerably compared to CPUs. TPUs often come with large amounts of high-bandwidth memory, allowing for the handling of large models and batch sizes during training, which can lead to better model quality. TPU Pods (large clusters of TPUs) also provide a scalable solution for handling the growing complexity of large foundation models. Training can be distributed across multiple TPU devices for faster and more eﬃcient processing.... The eﬃciencies gained through the use of TPUs are aligned with Google's commitment to operate sustainably.\n\n**Software:** Training was done using JAX and ML Pathways.... ## Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\nOur models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... ## Intended Usage and Limitations\n\n**Beneﬁt and Intended Usage:** Gemini 3 Pro is our most intelligent and adaptive model yet, capable of helping with real-world complexity, solving problems that require enhanced reasoning and intelligence, creativity, strategic planning and making improvements step-by-step. It is particularly well-suited for applications that require: agentic performance, advanced coding, long context and/or multimodal understanding, and/or algorithmic development.... ## Ethics and Content Safety\n\n**Evaluation Approach:** Gemini 3 Pro was developed in partnership with internal safety, security, and responsibility teams. A range of evaluations and red teaming activities were conducted to help improve the model and inform decision-making. These evaluations and activities align with Google's AI Principles and responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use Policy and the Gemini API Additional Terms of Service).... Evaluation types included but were not limited to:\n\n**Training/Development Evaluations**including automated and human evaluations carried out continuously throughout and after the model's training, to monitor its progress and performance; **Human Red Teaming**conducted by specialist teams who sit outside of the model development team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the model adheres to safety policies and desired outcomes; **Automated Red Teaming**to dynamically evaluate Gemini for safety and security considerations at scale, complementing human red teaming and static evaluations; **Ethics & Safety Reviews**were conducted ahead of the model's release... **Training and Development Evaluation Results:** Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustiﬁed refusals low. We mark improvements in green and regressions in red.... **Human Red Teaming Results:** We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level ﬁndings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisﬁed required launch thresholds, which were developed by expert teams to protect children online and meet Google's commitments to child safety across our models and Google products."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-4",
            "url": "https://en.wikipedia.org/wiki/Gemini_(language_model)",
            "snippet": "At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2.... It was made available only in English. Touted as Google's \"largest and most capable AI model\" and designed to emulate human behavior, the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs), and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.... Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test, obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13, while Gemini Nano will be made available to Android developers as well.... Hassabis further revealed that DeepMind was exploring how Gemini could be \"combined with robotics to physically interact with the world\". In accordance with an executive order signed by U.S. President Joe Biden in October, Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly, the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November.... On December 11, 2024, Google announced Gemini 2.0 Flash Experimental, a significant update to its Gemini AI model. This iteration boasts improved speed and performance over its predecessor, Gemini 1.5 Flash. Key features include a Multimodal Live API for real-time audio and video interactions, enhanced spatial understanding, native image and controllable text-to-speech generation (with watermarking), and integrated tool use, including Google Search.... It also introduces improved agentic capabilities, a new Google Gen AI SDK, and \"Jules,\" an experimental AI coding agent for GitHub. Additionally, Google Colab is integrating Gemini 2.0 to generate data science notebooks from natural language. Gemini 2.0 was available through the Gemini chat interface for all users as \"Gemini 2.0 Flash experimental\".... On March 12, 2025, Google also announced Gemini Robotics, a vision-language-action model based on the Gemini 2.0 family of models.\n\nThe next day, Google announced that Gemini in Android Studio would be able to understand simple UI mockups and transform them into working Jetpack Compose code.... Gemini 2.5 Pro Experimental was released on March 25, 2025, described by Google as its most intelligent AI model yet, featuring enhanced reasoning and coding capabilities, and a \"thinking model\" capable of reasoning through steps before responding, using techniques like chain-of-thought prompting, whilst maintaining native multimodality and launching with a 1 million token context window.... At Google I/O 2025, Google announced significant updates to its Gemini core models. Gemini 2.5 Flash became the default model, delivering faster responses. Gemini 2.5 Pro was introduced as the most advanced Gemini model, featuring reasoning, coding capabilities, and the new Deep Think mode for complex tasks. Both 2.5 Pro and Flash support native audio output and improved security.... On November 18th, 2025, Google announced the release of 3.0 Pro and 3.0 Deep Think. These new models replace 2.5 Pro and Flash, and are the most powerful models available as of November 2025. On release 3.0 Pro outperformed major AI models in 19 out of 20 benchmarks tested, including surpassing OpenAI's GPT-5 Pro in Humanity's Last Exam, with an accuracy of 41% compared to OpenAI's 31.64%.... ### Model versions\n\nThe following table lists the main model versions of Gemini, describing the significant changes included with each version:... |3.0 Pro Image (Nano Banana Pro)|20 November 2025|Active|An improved version of Nano Banana which includes better text rendering and better real world knowledge.|... Gemini and Gemma models are decoder-only transformers, with modifications to allow efficient training and inference on TPUs. The 1.0 generation uses multi-query attention.\n\n**Technical specifications of Gemini models**... ## External links\n- Official website\n- Press release via *The Keyword*\n- White paper for 1.0 and 1.5\n- White paper for Gemini 3.0 Pro [1]\n\nCategories: - 2023 software\n- Chatbots\n- Google DeepMind\n- Google software\n- Large language models\n- Multimodal interaction\n- 2023 in artificial intelligence\n- Generative pre-trained transformers"
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-5",
            "url": "https://www.studocu.com/en-gb/document/university-of-east-london/financial-accounting/gemini-3-pro-model-card-insights-safety-evaluations-nov-2025/146206142",
            "snippet": "Key Concepts\n\n- Multimodal Capabilities: Gemini 3 Pro processes text, images, audio, and video, enhancing its versatility in handling diverse data types.\n\n- Sparse MoE Architecture: This model employs a mixture of experts approach, optimizing computational efficiency by activating only relevant parameters.\n\n- Training Dataset: A comprehensive dataset includes various modalities and is processed to ensure quality and safety.... - Evaluation Methods: The model undergoes rigorous testing across benchmarks to assess its reasoning and multimodal performance.\n\n- Safety Policies: Strict guidelines are in place to prevent harmful content generation, ensuring responsible AI use.\n\n- Intended Applications: Designed for complex problem-solving, the model excels in areas requiring advanced reasoning and creativity.\n\n## Preview text\n\nModel card published: November, 2025... ## Gemini 3 Pro\n\n## Model Card\n\n### Gemini 3 Pro - Model Card\n\nModel Cards are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from time-to-time; for example, to include updated evaluations as the model is improved or revised. See the Google DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\nPublished / Model Release: November 2025... #### Model Information\n\nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of highly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from different information sources, including text, audio, images, video, and entire code repositories.... Model dependencies: This model is not a modification or a fine-tune of a prior model.\n\nInputs: Text strings (e., a question, a prompt, document(s) to be summarized), images, audio, and video files, with a token context window of up to 1M.\n\nOutputs: Text, with a 64K token output.... Architecture: Gemini 3 Pro is a sparse mixture-of-experts (MoE) (Clark et al., 2022; Du et al., 2021; Fedus et al., 2021; Jiang et al., 2024, Lepikhin et al., 2020; Riquelme et al., 2021; Roller et al., 2021; Shazeer et al., 2017 ) transformer-based model (Vaswani et al., 2017) with native multimodal support for text, vision, and audio inputs.... #### Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\n● Gemini App ● Google Cloud / Vertex AI ● Google AI Studio ● Gemini API ● Google AI Mode ● Google Antigravity... Our models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... #### Evaluation\n\nApproach: Gemini 3 Pro was evaluated across a range of benchmarks, including reasoning, multimodal capabilities, agentic tool use, multi-lingual performance, and long-context. Additional benchmarks and details on approach, results and their methodologies can be found at: deepmind/models/evals/gemini-3-pro.... Training and Development Evaluation Results: Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2 Pro across both safety and tone, while keeping unjustified refusals low. We mark improvements in green and regressions in red.... Evaluation 1 Description Gemini 3 Pro vs. Gemini 2 Pro\n\nText to Text Safety Automated content safety evaluation measuring safety policies -10%\n\nMultilingual Safety Automated safety policy evaluation across multiple languages +0% (non-egregious)\n\nImage to Text Safety Automated content safety evaluation measuring safety policies +3% (non-egregious)... Human Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level findings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams to protect children online and meet Google’s commitments to child safety across our models and Google products."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Hardware Used During Training & Inference",
            "score": 1,
            "explanation": "Gemini 3 Pro was trained using Google's Tensor Processing Units (TPUs)."
          },
          {
            "name": "Software Frameworks & Tooling",
            "score": 1,
            "explanation": "Training was done using JAX and ML Pathways."
          },
          {
            "name": "Energy Use/ Sustainability Metrics",
            "score": 1,
            "explanation": "The efficiencies gained through the use of TPUs are aligned with Google's commitment to operate sustainably."
          }
        ]
      },
      {
        "sectionId": "intended-use",
        "sectionSnippets": [
          {
            "snippetId": "intended-use-snippet-1",
            "url": "https://ai.google.dev/gemini-api/docs/models",
            "snippet": "OUR MOST INTELLIGENT MODEL\n\n## Gemini 3 Pro\n\nThe best model in the world for multimodal understanding, and our most powerful agentic and vibe-coding model yet, delivering richer visuals and deeper interactivity, all built on a foundation of state-of-the-art reasoning.\n\n### Expand to learn more\n\n#### Model details... ### Gemini 3 Pro Preview\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-3-pro-preview`|\n|Supported data types|Text, Image, Video, Audio, and PDF Text|\n|Token limits [*]|1,048,576 65,536|\n|Capabilities|Not supported Supported Supported Supported Supported Supported Not supported Not supported Not supported Supported Supported Supported Supported|\n|Versions||\n|Latest update|November 2025|\n|Knowledge cutoff|January 2025|... ### Gemini 3 Pro Image Preview\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-3-pro-image-preview`|\n|Supported data types|Image and Text Image and Text|\n|Token limits [*]|65,536 32,768|\n|Capabilities|Not supported Supported Not supported Not supported Not supported Not supported Not supported Supported Not supported Supported Supported Supported Not supported|\n|Versions||\n|Latest update|November 2025|\n|Knowledge cutoff|January 2025|\nOUR ADVANCED THINKING MODEL... ### Gemini 2.5 Pro\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.5-pro`|\n|Supported data types|Audio, images, video, text, and PDF Text|\n|Token limits [*]|1,048,576 65,536|\n|Capabilities|Not supported Supported Supported Supported Supported Supported Supported Not supported Not supported Supported Supported Supported Supported|\n|Versions||\n|Latest update|June 2025|\n|Knowledge cutoff|January 2025|... ### Gemini 2.5 Pro TTS\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.5-pro-preview-tts`|\n|Supported data types|Text Audio|\n|Token limits [*]|8,192 16,384|\n|Capabilities|Supported Not Supported Not supported Not supported Not Supported Not supported Not supported Not supported Not supported Not supported Not supported Not supported Not supported|\n|Versions||\n|Latest update|May 2025|\nFAST AND INTELLIGENT... ## Gemini 2.5 Flash\n\nOur best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases.\n\n### Expand to learn more\n\n#### Model details... ## Gemini 2.5 Flash-Lite\n\nOur fastest flash model optimized for cost-efficiency and high throughput.\n\n### Expand to learn more\n\n#### Model details... ### Gemini 2.0 Flash\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.0-flash`|\n|Supported data types|Audio, images, video, and text Text|\n|Token limits [*]|1,048,576 8,192|\n|Capabilities|Not supported Supported Supported Supported Not supported Supported Supported Not supported Supported Supported Supported Experimental Not supported|\n|Versions||\n|Latest update|February 2025|\n|Knowledge cutoff|August 2024|... ### Gemini 2.0 Flash Image\n\n|Property|Description|\n|--|--|\n|Model code|`gemini-2.0-flash-preview-image-generation`|\n|Supported data types|Audio, images, video, and text Text and images|\n|Token limits [*]|32,768 8,192|\n|Capabilities|Not supported Supported Supported Not Supported Not supported Not supported Not supported Supported Not Supported Not Supported Supported Not Supported Not supported|... ### Expand to learn more\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.... ## Model version name patterns\n\nGemini models are available in either\n\n*stable*, *preview*, *latest*, or\n\n*experimental* versions.\n\n### Stable\n\nPoints to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nFor example:\n\n`gemini-2.5-flash`.... ### Preview\n\nPoints to a preview model which may be used for production. Preview models will typically have billing enabled, might come with more restrictive rate limits and will be deprecated with at least 2 weeks notice.\n\nFor example:\n\n`gemini-2.5-flash-preview-09-2025`.... ### Experimental\n\nPoints to an experimental model which will typically be not be suitable for production use and come with more restrictive rate limits. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nExperimental models are not stable and availability of model endpoints is subject to change.\n\n## Model deprecations\n\nFor information about model deprecations, visit the Gemini deprecations page."
          },
          {
            "snippetId": "intended-use-snippet-2",
            "url": "https://web.archive.org/web/20251118111103if_/https:/storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf",
            "snippet": "Gemini 3 Pro \nModel Card\nGemini 3 Pro - Model Card \n \n \n \nModel Cards are intended to provide essential information on Gemini models, including known \nlimitations, mitigation approaches, and safety performance. Model cards may be updated from \ntime-to-time; for example, to include updated evaluations as the model is improved or revised. See the \nGoogle DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous \nmodel cards did. We hope more information about the training dataset, distribution, and intended uses \nwill empower developers with deeper insights and help build more robust and responsible downstream \napplications. \n \nPublished / Model Release: November 2025... Model Information \n \nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of \nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced \nmodel for complex tasks, and can comprehend vast datasets, challenging problems from different \ninformation sources, including text, audio, images, video, and entire code repositories.... Model dependencies: This model is not a modification or a fine-tune of a prior model.   \n \nInputs: Text strings (e.g., a question, a prompt, document(s) to be summarized), images, audio, and \nvideo files, with a token context window of up to 1M.  \n \nOutputs:  Text, with a 64K token output.... collected from users of Google products and services to train AI models, along with user interactions \nwith the model) in accordance with Google’s relevant terms of service, privacy policy, service-specific \npolicies, and pursuant to user controls, where appropriate; other datasets that Google acquires or \ngenerates in the course of its business operations, or directly from its workforce; and AI-generated... case-by-case basis, filtering irrelevant or harmful content, text, and other modalities, including filtering \ncontent that is pornographic, violent, or violative of child sexual abuse material (CSAM) laws. \n \n \nImplementation and Sustainability \n \nHardware: Gemini 3 Pro was trained using Google’s Tensor Processing Units (TPUs). TPUs are... Distribution \n \nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; \nrespective documentation shared in line: \n \n●​\nGemini App \n●​\nGoogle Cloud / Vertex AI \n●​\nGoogle AI Studio \n●​\nGemini API \n●​\nGoogle AI Mode \n●​... Google Antigravity \n \nOur models are available to downstream providers via an application program interface (API) and \nsubject to relevant terms of use. There is no required hardware or software to use the model. For \nAI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google \nCloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini \nAPI in Vertex AI quickstart.  \n \n \n \n3... Intended Usage and Limitations \n \nBenefit and Intended Usage: Gemini 3 Pro is our most intelligent and adaptive model yet, capable of \nhelping with real-world complexity, solving problems that require enhanced reasoning and intelligence, \ncreativity, strategic planning and making improvements step-by-step. It is particularly well-suited for... applications that require:  agentic performance, advanced coding, long context and/or multimodal \nunderstanding, and/or algorithmic development. \n \nKnown Limitations: Gemini 3 Pro may exhibit some of the general limitations of foundation models, \nsuch as hallucinations. There may also be occasional slowness or timeout issues. The knowledge cutoff... date for Gemini 3 Pro was January 2025.  \n \nAcceptable Usage: Google’s Generative AI Prohibited Use Policy applies to uses of the model in \naccordance with the applicable terms of service. Additionally, the model should not be integrated into \ncertain systems (also found in Google’s Generative AI Prohibited Use Policy), including those that: (1)... Evaluation Approach: Gemini 3 Pro was developed in partnership with internal safety, security, and \nresponsibility teams. A range of evaluations and red teaming activities were conducted to help improve \nthe model and inform decision-making. These evaluations and activities align with Google's AI Principles \nand responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use... standard of results. The performance results reported below are computed with improved evaluations \nand thus are not directly comparable with performance results found in previous Gemini model cards. \n \nWe expect variation in our automated safety evaluations results, which is why we review flagged content \nto check for egregious or dangerous material. Our manual review confirmed losses were... Instrumental \nReasoning​\nLevels 1 + 2 \nCCL not \nreached \n \nMore details can be found in the Gemini 3 Pro Frontier Safety Framework Report. \n \n \n \n8"
          },
          {
            "snippetId": "intended-use-snippet-3",
            "url": "https://huggingface.co/datasets/multimodalart/google-gemini-3-pro-pre-release-model-card",
            "snippet": "# Gemini 3 Pro Model Card\n\n### ⚠️ This is a mirror to the pre-release model card taht was officially published by Google @ https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf and subsequentially yanked. It will get outdated by the actual release version in a few hours (date of publication Nov 18, 13:16 CET)\n\n*Model card published: November, 2025*... # Gemini 3 Pro - Model Card\n\n*Model Cards* *are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from*\n\n*time-to-time; for example, to include updated evaluations as the model is improved or revised. See the* *Google DeepMind site* *for a comprehensive list of model cards.*... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\n*Published / Model Release: November 2025*... ## Model Information\n\n**Description**: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of\n\nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google's most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from diﬀerent information sources, including text, audio, images, video, and entire code repositories.... **Model dependencies:** This model is not a modiﬁcation or a ﬁne-tune of a prior model.\n\n**Inputs:** Text strings (e.g., a question, a prompt, document(s) to be summarized), images, audio, and video ﬁles, with a token context window of up to 1M.... **Outputs**: Text, with a 64K token output.\n\n**Architecture**: Gemini 3 Pro is a sparse mixture-of-experts (MoE) (Clark et al., 2022; Du et al., 2021; Fedus et al., 2021; Jiang et al., 2024, Lepikhin et al., 2020; Riquelme et al., 2021; Roller et al., 2021; Shazeer et al., 2017) transformer-based model (Vaswani et al., 2017) with native multimodal support for text, vision, and audio inputs.... ## Implementation and Sustainability\n\n**Hardware:** Gemini 3 Pro was trained using Google's Tensor Processing Units (TPUs). TPUs are speciﬁcally designed to handle the massive computations involved in training LLMs and can speed up training considerably compared to CPUs. TPUs often come with large amounts of high-bandwidth memory, allowing for the handling of large models and batch sizes during training, which can lead to better model quality. TPU Pods (large clusters of TPUs) also provide a scalable solution for handling the growing complexity of large foundation models. Training can be distributed across multiple TPU devices for faster and more eﬃcient processing.... ## Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\nOur models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... ## Intended Usage and Limitations\n\n**Beneﬁt and Intended Usage:** Gemini 3 Pro is our most intelligent and adaptive model yet, capable of helping with real-world complexity, solving problems that require enhanced reasoning and intelligence, creativity, strategic planning and making improvements step-by-step. It is particularly well-suited for applications that require: agentic performance, advanced coding, long context and/or multimodal understanding, and/or algorithmic development.... **Known Limitations:** Gemini 3 Pro may exhibit some of the general limitations of foundation models, such as hallucinations. There may also be occasional slowness or timeout issues. The knowledge cutoﬀ date for Gemini 3 Pro was January 2025.... **Training and Development Evaluation Results:** Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustiﬁed refusals low. We mark improvements in green and regressions in red."
          },
          {
            "snippetId": "intended-use-snippet-4",
            "url": "https://en.wikipedia.org/wiki/Gemini_(language_model)",
            "snippet": "In August 2023, *The Information* published a report outlining Google's roadmap for Gemini, revealing that the company was targeting a launch date of late 2023. According to the report, Google hoped to surpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation, allowing it to create contextual images and be adapted for a wider range of use cases.... ### Launch\n\nOn December 6, 2023, Pichai and Hassabis announced \"Gemini 1.0\" at a virtual press conference. It comprised three models: Gemini Ultra, designed for \"highly complex tasks\"; Gemini Pro, designed for \"a wide range of tasks\"; and Gemini Nano, designed for \"on-device tasks\".... At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2.... It was made available only in English. Touted as Google's \"largest and most capable AI model\" and designed to emulate human behavior, the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs), and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.... Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test, obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13, while Gemini Nano will be made available to Android developers as well.... Two updated Gemini models, Gemini-1.5-Pro-002 and Gemini-1.5-Flash-002, were released on September 24, 2024.... On January 30, 2025, Google released Gemini 2.0 Flash as the new default model, with Gemini 1.5 Flash still available for usage. This was followed by the release of Gemini 2.0 Pro on February 5, 2025. Additionally, Google released Gemini 2.0 Flash Thinking Experimental, which details the language model's thinking process when responding to prompts.... On March 12, 2025, Google also announced Gemini Robotics, a vision-language-action model based on the Gemini 2.0 family of models.\n\nThe next day, Google announced that Gemini in Android Studio would be able to understand simple UI mockups and transform them into working Jetpack Compose code.... Gemini 2.5 Pro Experimental was released on March 25, 2025, described by Google as its most intelligent AI model yet, featuring enhanced reasoning and coding capabilities, and a \"thinking model\" capable of reasoning through steps before responding, using techniques like chain-of-thought prompting, whilst maintaining native multimodality and launching with a 1 million token context window.... At Google I/O 2025, Google announced significant updates to its Gemini core models. Gemini 2.5 Flash became the default model, delivering faster responses. Gemini 2.5 Pro was introduced as the most advanced Gemini model, featuring reasoning, coding capabilities, and the new Deep Think mode for complex tasks. Both 2.5 Pro and Flash support native audio output and improved security.... On June 17, 2025, Google announced general availability for 2.5 Pro and Flash. They also introduced Gemini 2.5 Flash-Lite that same day, a model optimized for speed and cost-efficiency.... On November 18th, 2025, Google announced the release of 3.0 Pro and 3.0 Deep Think. These new models replace 2.5 Pro and Flash, and are the most powerful models available as of November 2025. On release 3.0 Pro outperformed major AI models in 19 out of 20 benchmarks tested, including surpassing OpenAI's GPT-5 Pro in Humanity's Last Exam, with an accuracy of 41% compared to OpenAI's 31.64%.... |3.0 Pro Image (Nano Banana Pro)|20 November 2025|Active|An improved version of Nano Banana which includes better text rendering and better real world knowledge.|... ## External links\n- Official website\n- Press release via *The Keyword*\n- White paper for 1.0 and 1.5\n- White paper for Gemini 3.0 Pro [1]\n\nCategories: - 2023 software\n- Chatbots\n- Google DeepMind\n- Google software\n- Large language models\n- Multimodal interaction\n- 2023 in artificial intelligence\n- Generative pre-trained transformers"
          },
          {
            "snippetId": "intended-use-snippet-5",
            "url": "https://www.studocu.com/en-gb/document/university-of-east-london/financial-accounting/gemini-3-pro-model-card-insights-safety-evaluations-nov-2025/146206142",
            "snippet": "Key Concepts\n\n- Multimodal Capabilities: Gemini 3 Pro processes text, images, audio, and video, enhancing its versatility in handling diverse data types.\n\n- Sparse MoE Architecture: This model employs a mixture of experts approach, optimizing computational efficiency by activating only relevant parameters.\n\n- Training Dataset: A comprehensive dataset includes various modalities and is processed to ensure quality and safety.... - Evaluation Methods: The model undergoes rigorous testing across benchmarks to assess its reasoning and multimodal performance.\n\n- Safety Policies: Strict guidelines are in place to prevent harmful content generation, ensuring responsible AI use.\n\n- Intended Applications: Designed for complex problem-solving, the model excels in areas requiring advanced reasoning and creativity.\n\n## Preview text\n\nModel card published: November, 2025... ## Gemini 3 Pro\n\n## Model Card\n\n### Gemini 3 Pro - Model Card\n\nModel Cards are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from time-to-time; for example, to include updated evaluations as the model is improved or revised. See the Google DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\nPublished / Model Release: November 2025... #### Model Information\n\nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of highly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from different information sources, including text, audio, images, video, and entire code repositories.... Model dependencies: This model is not a modification or a fine-tune of a prior model.\n\nInputs: Text strings (e., a question, a prompt, document(s) to be summarized), images, audio, and video files, with a token context window of up to 1M.\n\nOutputs: Text, with a 64K token output.... Architecture: Gemini 3 Pro is a sparse mixture-of-experts (MoE) (Clark et al., 2022; Du et al., 2021; Fedus et al., 2021; Jiang et al., 2024, Lepikhin et al., 2020; Riquelme et al., 2021; Roller et al., 2021; Shazeer et al., 2017 ) transformer-based model (Vaswani et al., 2017) with native multimodal support for text, vision, and audio inputs.... #### Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\n● Gemini App ● Google Cloud / Vertex AI ● Google AI Studio ● Gemini API ● Google AI Mode ● Google Antigravity... Our models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... #### Evaluation\n\nApproach: Gemini 3 Pro was evaluated across a range of benchmarks, including reasoning, multimodal capabilities, agentic tool use, multi-lingual performance, and long-context. Additional benchmarks and details on approach, results and their methodologies can be found at: deepmind/models/evals/gemini-3-pro.... Training and Development Evaluation Results: Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2 Pro across both safety and tone, while keeping unjustified refusals low. We mark improvements in green and regressions in red.... Evaluation 1 Description Gemini 3 Pro vs. Gemini 2 Pro\n\nText to Text Safety Automated content safety evaluation measuring safety policies -10%\n\nMultilingual Safety Automated safety policy evaluation across multiple languages +0% (non-egregious)\n\nImage to Text Safety Automated content safety evaluation measuring safety policies +3% (non-egregious)... Human Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level findings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams to protect children online and meet Google’s commitments to child safety across our models and Google products."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Primary intended uses",
            "score": 1,
            "explanation": "Gemini 3 Pro is capable of helping with real-world complexity, solving problems that require enhanced reasoning and intelligence, creativity, strategic planning and making improvements step-by-step."
          },
          {
            "name": "Primary intended users",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Out-of-scope use cases",
            "score": 1,
            "explanation": "the model should not be integrated into certain systems (also found in Google’s Generative AI Prohibited Use Policy)"
          }
        ]
      },
      {
        "sectionId": "critical-risk",
        "sectionSnippets": [
          {
            "snippetId": "critical-risk-snippet-1",
            "url": "https://www.youtube.com/watch?v=hf59-PhO3do",
            "snippet": "Chapters:\n00:00 - Gemini 3 Executive Summary\n01:37 - MoE Architecture Explained\n03:07 - 1 Million Token Capabilities\n03:49 - Synthetic Data and Training\n04:58 - Reasoning and Multimodal Benchmarks\n06:54 - Deep Think Ultra Tier\n08:01 - Vibe Coding Performance\n09:09 - Anti-Gravity Agent Platform\n10:35 - Safety Metrics and Regressions\n11:56 - Frontier Risk Analysis\n13:39 - Availability and Reliability Challenges... Let's talk training data. The card mentions a large scale diverse\n{ts:232} and uh natively multimodal collection. So, you have the usual suspects, public web data, licensed data, and some user\n{ts:240} data with, you know, the standard controls. But there was one thing that really... The entire platform is designed to let the agent plan and execute complex end to-end\n{ts:566} tasks on its own and critically validate its own work. Critically, it can run tests, check the\n{ts:572} output, see if the UI looks right in the browser. It's a closed loop.... What about the frontier risks? How is Google\n{ts:640} handling safety? They say it's their most secure model yet. They detail all the mitigations,\n{ts:644} data set filtering, RLHF, critic feedback, all the standard stuff. But we need to look closely at the internal\n{ts:650} safety metrics they shared.... It is. And that's the critical point for\n{ts:670} anyone looking at this data. The card addresses it directly. They claimed that when they did a manual review, these\n{ts:676} flagged instances were overwhelmingly either false positives from the automated system or what they call\n{ts:681} non-gregious issues.... But non-aggregious is a very subjective term. It almost sounds like the model\n{ts:686} got so much smarter that it's now outsmarting the automated safety tests. That's a great way to put it. It signals\n{ts:692} a real challenge in safety scaling. As the models get more complex, our automated safety tools are struggling to... {ts:698} keep up. That said, they did show improvements elsewhere. tone was up nearly 8%, multilingual safety was up a\n{ts:705} little. The takeaway is that this new architecture is just different and the old tools need to catch up.\n{ts:710} Okay, so let's move from the internal metrics to the really crude part, the Frontier Safety Framework analysis.... The\n{ts:717} headline here is that the card states clearly that Gemini 3 Pro did not reach any critical capability levels or CCLs.\n{ts:724} That's the bottom line. Yes, but let's dig into the details. Starting with CBRN chemical, biological, radiological, and... {ts:730} nuclear threats. The model hit uplift level one, but the CCL was not reached. Meaning, while it can give you accurate\n{ts:738} textbook level information, their testing showed it generally fails to provide novel or complete enough\n{ts:742} instructions to actually help a threat actor build something dangerous.... The info is out there, but the model can't\n{ts:748} synthesize it into a truly hazardous new recipe. Okay, what about cyber security?\n{ts:754} This feels like the most immediate risk given how good it is at coding. Cyber security is also uplift level one.\n{ts:760} So the CCL was not reached.... However, and this is a big however, the alert threshold was met.\n{ts:766} What does that mean? It means the model showed it has highly capable individual hacking skills. It\n{ts:771} solved 11 out of 12 of their V1 hard challenges. So it's very good at specific component level hacking tasks... {ts:780} which triggered the alert. But not the full CCL. Why not? because it failed to solve any of the 13\n{ts:785} endto-end V2 challenges. Those require planning, coordination, and long-term execution to pull off a full complex\n{ts:791} cyber attack.... {ts:809} challenges. So, it has some foundational planning abilities, but it's not demonstrating dangerous levels of\n{ts:814} self-motivated autonomy in these tests. It's an agent, but it's not a rogue agent.\n{ts:818} So, what does this all mean for developers today?... The capabilities are obviously huge, and the safety analysis,\n{ts:825} while needing a critical eye, seems to clear it for general release. It's rolling out now. Developers can get\n{ts:830} it in AI Studio, Vert.Ex AI, the CLI, and through that new anti-gravity platform."
          },
          {
            "snippetId": "critical-risk-snippet-2",
            "url": "https://web.archive.org/web/20251118111103if_/https:/storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf",
            "snippet": "Gemini 3 Pro \nModel Card\nGemini 3 Pro - Model Card \n \n \n \nModel Cards are intended to provide essential information on Gemini models, including known \nlimitations, mitigation approaches, and safety performance. Model cards may be updated from \ntime-to-time; for example, to include updated evaluations as the model is improved or revised. See the \nGoogle DeepMind site for a comprehensive list of model cards.... case-by-case basis, filtering irrelevant or harmful content, text, and other modalities, including filtering \ncontent that is pornographic, violent, or violative of child sexual abuse material (CSAM) laws. \n \n \nImplementation and Sustainability \n \nHardware: Gemini 3 Pro was trained using Google’s Tensor Processing Units (TPUs). TPUs are... engage in dangerous or illicit activities, or otherwise violate applicable laws or regulations, (2) \ncompromise the security of others’ or Google’s services, (3) engage in sexually explicit, violent, hateful, \nor harmful activities, (4) engage in misinformation, misrepresentation, or misleading activities. \n \n \nEthics and Content Safety... Evaluation Approach: Gemini 3 Pro was developed in partnership with internal safety, security, and \nresponsibility teams. A range of evaluations and red teaming activities were conducted to help improve \nthe model and inform decision-making. These evaluations and activities align with Google's AI Principles \nand responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use... team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the \nmodel adheres to safety policies and desired outcomes;  \n●​\nAutomated Red Teaming to dynamically evaluate Gemini for safety and security \nconsiderations at scale, complementing human red teaming and static evaluations; \n●​\nEthics & Safety Reviews were conducted ahead of the model’s release... 5\nIn addition, we perform testing following the guidelines in Google DeepMind’s Frontier Safety \nFramework (FSF). \n \nSafety Policies: Gemini’s safety policies aim to prevent our Generative AI models from generating \nharmful content, including:  \n \n1.​\nContent related to child sexual abuse material and exploitation \n2.​... Training and Development Evaluation Results:  Results for some of the internal safety evaluations \nconducted during the development phase are listed below. The evaluation results are for automated \nevaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage \nincrease or decrease in performance compared to the indicated model, as described below. Overall,... standard of results. The performance results reported below are computed with improved evaluations \nand thus are not directly comparable with performance results found in previous Gemini model cards. \n \nWe expect variation in our automated safety evaluations results, which is why we review flagged content \nto check for egregious or dangerous material. Our manual review confirmed losses were... overwhelmingly either a) false positives or b) not egregious. \n \nHuman Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of \nthe model development team. High-level findings are fed back to the model team. For child safety \nevaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams... to protect children online and meet Google’s commitments to child safety across our models and \nGoogle products. For content safety policies generally, including child safety, we saw similar or improved \nsafety performance compared to Gemini 2.5 Pro. Compared to 2.5 Pro, the scope of red teaming was \nexpanded to cover more potential issues outside of our strict policies, and found no egregious... concerns. \n \nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and \ndeployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations \ninclude, but are not limited to:  \n \n●​\ndataset filtering;  \n●​... conditional pre-training; \n●​\nsupervised fine-tuning; \n●​\nreinforcement learning from human and critic feedback; \n●​\nsafety policies and desiderata; \n●​\nproduct-level mitigations such as safety filtering. \n \nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2.5 Pro but \nstill an open research problem), and b) possible degradation in multi-turn conversations. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n7... Frontier Safety \n \nWe evaluated Gemini 3 Pro as outlined in our latest Frontier Safety Framework (September-2025), \nand found that it did not reach any critical capability levels as outlined in the table below:   \n \nDomain \nKey Results for Gemini 3 Pro  \nCCL  \nCCL \nreached?... Instrumental \nReasoning​\nLevels 1 + 2 \nCCL not \nreached \n \nMore details can be found in the Gemini 3 Pro Frontier Safety Framework Report. \n \n \n \n8"
          },
          {
            "snippetId": "critical-risk-snippet-3",
            "url": "https://huggingface.co/datasets/multimodalart/google-gemini-3-pro-pre-release-model-card",
            "snippet": "# Gemini 3 Pro - Model Card\n\n*Model Cards* *are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from*\n\n*time-to-time; for example, to include updated evaluations as the model is improved or revised. See the* *Google DeepMind site* *for a comprehensive list of model cards.*... The eﬃciencies gained through the use of TPUs are aligned with Google's commitment to operate sustainably.\n\n**Software:** Training was done using JAX and ML Pathways.... **Known Limitations:** Gemini 3 Pro may exhibit some of the general limitations of foundation models, such as hallucinations. There may also be occasional slowness or timeout issues. The knowledge cutoﬀ date for Gemini 3 Pro was January 2025.... ## Ethics and Content Safety\n\n**Evaluation Approach:** Gemini 3 Pro was developed in partnership with internal safety, security, and responsibility teams. A range of evaluations and red teaming activities were conducted to help improve the model and inform decision-making. These evaluations and activities align with Google's AI Principles and responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use Policy and the Gemini API Additional Terms of Service).... Evaluation types included but were not limited to:\n\n**Training/Development Evaluations**including automated and human evaluations carried out continuously throughout and after the model's training, to monitor its progress and performance; **Human Red Teaming**conducted by specialist teams who sit outside of the model development team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the model adheres to safety policies and desired outcomes; **Automated Red Teaming**to dynamically evaluate Gemini for safety and security considerations at scale, complementing human red teaming and static evaluations; **Ethics & Safety Reviews**were conducted ahead of the model's release... In addition, we perform testing following the guidelines in Google DeepMind's Frontier Safety Framework (FSF).\n\n**Safety Policies**: Gemini's safety policies aim to prevent our Generative AI models from generating harmful content, including:\n\n- Content related to child sexual abuse material and exploitation\n\n- Hate speech (e.g., dehumanizing members of protected groups)\n\n- Dangerous content (e.g., promoting suicide, or instructing in activities that could cause real-world harm)\n\n- Harassment (e.g., encouraging violence against people)\n\n- Sexually explicit content\n\n- Medical advice that runs contrary to scientiﬁc or medical consensus... **Training and Development Evaluation Results:** Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustiﬁed refusals low. We mark improvements in green and regressions in red.... **Human Red Teaming Results:** We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level ﬁndings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisﬁed required launch thresholds, which were developed by expert teams to protect children online and meet Google's commitments to child safety across our models and Google products.... For content safety policies generally, including child safety, we saw similar or improved safety performance compared to Gemini 2.5 Pro. Compared to 2.5 Pro, the scope of red teaming was expanded to cover more potential issues outside of our strict policies, and found no egregious concerns.\n\n**Risks and Mitigations:** Safety and responsibility was built into Gemini 3 Pro throughout the training and deployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations include, but are not limited to:... - dataset ﬁltering;\n\n- conditional pre-training;\n\n- supervised ﬁne-tuning;\n\n- reinforcement learning from human and critic feedback;\n\n- safety policies and desiderata;\n\n- product-level mitigations such as safety ﬁltering.\n\nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2.5 Pro but still an open research problem), and b) possible degradation in multi-turn conversations.... ## Frontier Safety\n\nWe evaluated Gemini 3 Pro as outlined in our latest Frontier Safety Framework (September-2025), and found that it did not reach any critical capability levels as outlined in the table below:... |Domain|Key Results for Gemini 3 Pro|CCL|CCL reached?|\n|--|--|--|--|\n|CBRN|The model provides accurate and occasionally actionable information but generally fails to oﬀer novel or suﬃciently complete and detailed instructions to signiﬁcantly enhance the capabilities of low to medium resourced threat actors.|Uplift Level 1|CCL not reached|"
          },
          {
            "snippetId": "critical-risk-snippet-4",
            "url": "https://www.studocu.com/en-gb/document/university-of-east-london/financial-accounting/gemini-3-pro-model-card-insights-safety-evaluations-nov-2025/146206142",
            "snippet": "- Evaluation Methods: The model undergoes rigorous testing across benchmarks to assess its reasoning and multimodal performance.\n\n- Safety Policies: Strict guidelines are in place to prevent harmful content generation, ensuring responsible AI use.\n\n- Intended Applications: Designed for complex problem-solving, the model excels in areas requiring advanced reasoning and creativity.\n\n## Preview text\n\nModel card published: November, 2025... ## Gemini 3 Pro\n\n## Model Card\n\n### Gemini 3 Pro - Model Card\n\nModel Cards are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from time-to-time; for example, to include updated evaluations as the model is improved or revised. See the Google DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\nPublished / Model Release: November 2025... #### Model Information\n\nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of highly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from different information sources, including text, audio, images, video, and entire code repositories.... #### Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\n● Gemini App ● Google Cloud / Vertex AI ● Google AI Studio ● Gemini API ● Google AI Mode ● Google Antigravity... Our models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... Results: Gemini 3 Pro significantly outperforms Gemini 2 Pro across a range of benchmarks requiring enhanced reasoning and multimodal capabilities. Results as of November, 2025 are listed below:\n\nSafety Policies: Gemini’s safety policies aim to prevent our Generative AI models from generating harmful content, including:... - Content related to child sexual abuse material and exploitation\n\n- Hate speech (e., dehumanizing members of protected groups)\n\n- Dangerous content (e., promoting suicide, or instructing in activities that could cause real-world harm)\n\n- Harassment (e., encouraging violence against people)\n\n- Sexually explicit content\n\n- Medical advice that runs contrary to scientific or medical consensus... Training and Development Evaluation Results: Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2 Pro across both safety and tone, while keeping unjustified refusals low. We mark improvements in green and regressions in red.... We expect variation in our automated safety evaluations results, which is why we review flagged content to check for egregious or dangerous material. Our manual review confirmed losses were overwhelmingly either a) false positives or b) not egregious.... Human Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level findings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams to protect children online and meet Google’s commitments to child safety across our models and Google products.... For content safety policies generally, including child safety, we saw similar or improved safety performance compared to Gemini 2 Pro. Compared to 2 Pro, the scope of red teaming was expanded to cover more potential issues outside of our strict policies, and found no egregious concerns.\n\nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and deployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations include, but are not limited to:... ● dataset filtering; ● conditional pre-training; ● supervised fine-tuning; ● reinforcement learning from human and critic feedback; ● safety policies and desiderata; ● product-level mitigations such as safety filtering.\n\nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2 Pro but still an open research problem), and b) possible degradation in multi-turn conversations."
          },
          {
            "snippetId": "critical-risk-snippet-5",
            "url": "https://en.wikipedia.org/wiki/Gemini_(language_model)",
            "snippet": "In August 2023, *The Information* published a report outlining Google's roadmap for Gemini, revealing that the company was targeting a launch date of late 2023. According to the report, Google hoped to surpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation, allowing it to create contextual images and be adapted for a wider range of use cases.... ### Launch\n\nOn December 6, 2023, Pichai and Hassabis announced \"Gemini 1.0\" at a virtual press conference. It comprised three models: Gemini Ultra, designed for \"highly complex tasks\"; Gemini Pro, designed for \"a wide range of tasks\"; and Gemini Nano, designed for \"on-device tasks\".... At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2.... It was made available only in English. Touted as Google's \"largest and most capable AI model\" and designed to emulate human behavior, the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs), and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.... Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test, obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13, while Gemini Nano will be made available to Android developers as well.... Hassabis further revealed that DeepMind was exploring how Gemini could be \"combined with robotics to physically interact with the world\". In accordance with an executive order signed by U.S. President Joe Biden in October, Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly, the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November.... On March 12, 2025, Google also announced Gemini Robotics, a vision-language-action model based on the Gemini 2.0 family of models.\n\nThe next day, Google announced that Gemini in Android Studio would be able to understand simple UI mockups and transform them into working Jetpack Compose code.... Gemini 2.5 Pro Experimental was released on March 25, 2025, described by Google as its most intelligent AI model yet, featuring enhanced reasoning and coding capabilities, and a \"thinking model\" capable of reasoning through steps before responding, using techniques like chain-of-thought prompting, whilst maintaining native multimodality and launching with a 1 million token context window.... At Google I/O 2025, Google announced significant updates to its Gemini core models. Gemini 2.5 Flash became the default model, delivering faster responses. Gemini 2.5 Pro was introduced as the most advanced Gemini model, featuring reasoning, coding capabilities, and the new Deep Think mode for complex tasks. Both 2.5 Pro and Flash support native audio output and improved security.... On November 18th, 2025, Google announced the release of 3.0 Pro and 3.0 Deep Think. These new models replace 2.5 Pro and Flash, and are the most powerful models available as of November 2025. On release 3.0 Pro outperformed major AI models in 19 out of 20 benchmarks tested, including surpassing OpenAI's GPT-5 Pro in Humanity's Last Exam, with an accuracy of 41% compared to OpenAI's 31.64%.... |3.0 Pro Image (Nano Banana Pro)|20 November 2025|Active|An improved version of Nano Banana which includes better text rendering and better real world knowledge.|... ## Nano Banana\n\nNano Banana (officially Gemini 2.5 Flash Image) is an image generation and editing model powered by generative artificial intelligence and developed by Google DeepMind, a subsidiary of Google. A text-to-image variant of the Gemini family of large language models, it was launched in August 2025 as a feature within the Gemini chatbot and other Google products.... ## External links\n- Official website\n- Press release via *The Keyword*\n- White paper for 1.0 and 1.5\n- White paper for Gemini 3.0 Pro [1]\n\nCategories: - 2023 software\n- Chatbots\n- Google DeepMind\n- Google software\n- Large language models\n- Multimodal interaction\n- 2023 in artificial intelligence\n- Generative pre-trained transformers"
          }
        ],
        "subsectionChecks": [
          {
            "name": "CBRN (Chemical, Biological, Radiological or Nuclear)",
            "score": 1,
            "explanation": "The model hit uplift level one, but the CCL was not reached."
          },
          {
            "name": "Cyber Risk",
            "score": 1,
            "explanation": "Cyber security is also uplift level one. So the CCL was not reached."
          },
          {
            "name": "Harmful Manipulation",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Child Safety Evaluations",
            "score": 1,
            "explanation": "For child safety evaluations, Gemini 3 Pro satisfied required launch thresholds."
          },
          {
            "name": "Privacy Risks",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      },
      {
        "sectionId": "safety-evaluation",
        "sectionSnippets": [
          {
            "snippetId": "safety-evaluation-snippet-1",
            "url": "https://web.archive.org/web/20251118111103if_/https:/storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf",
            "snippet": "case-by-case basis, filtering irrelevant or harmful content, text, and other modalities, including filtering \ncontent that is pornographic, violent, or violative of child sexual abuse material (CSAM) laws. \n \n \nImplementation and Sustainability \n \nHardware: Gemini 3 Pro was trained using Google’s Tensor Processing Units (TPUs). TPUs are... Evaluation Approach: Gemini 3 Pro was developed in partnership with internal safety, security, and \nresponsibility teams. A range of evaluations and red teaming activities were conducted to help improve \nthe model and inform decision-making. These evaluations and activities align with Google's AI Principles \nand responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use... team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the \nmodel adheres to safety policies and desired outcomes;  \n●​\nAutomated Red Teaming to dynamically evaluate Gemini for safety and security \nconsiderations at scale, complementing human red teaming and static evaluations; \n●​\nEthics & Safety Reviews were conducted ahead of the model’s release... 5\nIn addition, we perform testing following the guidelines in Google DeepMind’s Frontier Safety \nFramework (FSF). \n \nSafety Policies: Gemini’s safety policies aim to prevent our Generative AI models from generating \nharmful content, including:  \n \n1.​\nContent related to child sexual abuse material and exploitation \n2.​... Training and Development Evaluation Results:  Results for some of the internal safety evaluations \nconducted during the development phase are listed below. The evaluation results are for automated \nevaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage \nincrease or decrease in performance compared to the indicated model, as described below. Overall,... Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustified refusals \nlow. We mark improvements in green and regressions in red. \n \n \nEvaluation1 \nDescription \nGemini 3 Pro \nvs. Gemini 2.5 Pro \nText to Text Safety... Automated content safety evaluation \nmeasuring safety policies \n-10.4% \nMultilingual Safety  \nAutomated safety policy evaluation across \nmultiple languages \n+0.2% (non-egregious) \nImage to Text Safety \nAutomated content safety evaluation \nmeasuring safety policies \n+3.1% (non-egregious)... false positives and negatives, as well as update query sets to ensure balance and maintain a high \n2 For tone and instruction following, a positive percentage increase represents an improvement in the tone of the model on \nsensitive topics and the model’s ability to follow instructions while remaining safe compared to Gemini 2.5 Pro. We mark \nimprovements in green and regressions in red. \n1The ordering of evaluations in this table has changed from previous iterations of the 2.5 Flash-Lite model card in order to \nlist safety evaluations together and improve readability. The type of evaluations listed have remained the same.   \n \n6... standard of results. The performance results reported below are computed with improved evaluations \nand thus are not directly comparable with performance results found in previous Gemini model cards. \n \nWe expect variation in our automated safety evaluations results, which is why we review flagged content \nto check for egregious or dangerous material. Our manual review confirmed losses were... overwhelmingly either a) false positives or b) not egregious. \n \nHuman Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of \nthe model development team. High-level findings are fed back to the model team. For child safety \nevaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams... to protect children online and meet Google’s commitments to child safety across our models and \nGoogle products. For content safety policies generally, including child safety, we saw similar or improved \nsafety performance compared to Gemini 2.5 Pro. Compared to 2.5 Pro, the scope of red teaming was \nexpanded to cover more potential issues outside of our strict policies, and found no egregious... concerns. \n \nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and \ndeployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations \ninclude, but are not limited to:  \n \n●​\ndataset filtering;  \n●​... conditional pre-training; \n●​\nsupervised fine-tuning; \n●​\nreinforcement learning from human and critic feedback; \n●​\nsafety policies and desiderata; \n●​\nproduct-level mitigations such as safety filtering. \n \nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2.5 Pro but \nstill an open research problem), and b) possible degradation in multi-turn conversations. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n7"
          },
          {
            "snippetId": "safety-evaluation-snippet-2",
            "url": "https://huggingface.co/datasets/multimodalart/google-gemini-3-pro-pre-release-model-card",
            "snippet": "# Gemini 3 Pro - Model Card\n\n*Model Cards* *are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from*\n\n*time-to-time; for example, to include updated evaluations as the model is improved or revised. See the* *Google DeepMind site* *for a comprehensive list of model cards.*... ## Ethics and Content Safety\n\n**Evaluation Approach:** Gemini 3 Pro was developed in partnership with internal safety, security, and responsibility teams. A range of evaluations and red teaming activities were conducted to help improve the model and inform decision-making. These evaluations and activities align with Google's AI Principles and responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use Policy and the Gemini API Additional Terms of Service).... Evaluation types included but were not limited to:\n\n**Training/Development Evaluations**including automated and human evaluations carried out continuously throughout and after the model's training, to monitor its progress and performance; **Human Red Teaming**conducted by specialist teams who sit outside of the model development team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the model adheres to safety policies and desired outcomes; **Automated Red Teaming**to dynamically evaluate Gemini for safety and security considerations at scale, complementing human red teaming and static evaluations; **Ethics & Safety Reviews**were conducted ahead of the model's release... In addition, we perform testing following the guidelines in Google DeepMind's Frontier Safety Framework (FSF).\n\n**Safety Policies**: Gemini's safety policies aim to prevent our Generative AI models from generating harmful content, including:\n\n- Content related to child sexual abuse material and exploitation\n\n- Hate speech (e.g., dehumanizing members of protected groups)\n\n- Dangerous content (e.g., promoting suicide, or instructing in activities that could cause real-world harm)\n\n- Harassment (e.g., encouraging violence against people)\n\n- Sexually explicit content\n\n- Medical advice that runs contrary to scientiﬁc or medical consensus... **Training and Development Evaluation Results:** Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustiﬁed refusals low. We mark improvements in green and regressions in red.... |Evaluation 1|Description|Gemini 3 Pro vs. Gemini 2.5 Pro|\n|--|--|--|\n|Text to Text Safety|Automated content safety evaluation measuring safety policies|-10.4%|\n|Multilingual Safety|Automated safety policy evaluation across multiple languages|+0.2% (non-egregious)|... **Human Red Teaming Results:** We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level ﬁndings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisﬁed required launch thresholds, which were developed by expert teams to protect children online and meet Google's commitments to child safety across our models and Google products.... For content safety policies generally, including child safety, we saw similar or improved safety performance compared to Gemini 2.5 Pro. Compared to 2.5 Pro, the scope of red teaming was expanded to cover more potential issues outside of our strict policies, and found no egregious concerns.\n\n**Risks and Mitigations:** Safety and responsibility was built into Gemini 3 Pro throughout the training and deployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations include, but are not limited to:... - dataset ﬁltering;\n\n- conditional pre-training;\n\n- supervised ﬁne-tuning;\n\n- reinforcement learning from human and critic feedback;\n\n- safety policies and desiderata;\n\n- product-level mitigations such as safety ﬁltering.\n\nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2.5 Pro but still an open research problem), and b) possible degradation in multi-turn conversations.... ## Frontier Safety\n\nWe evaluated Gemini 3 Pro as outlined in our latest Frontier Safety Framework (September-2025), and found that it did not reach any critical capability levels as outlined in the table below:... |Cybersecurity|On key skills benchmark, v1 hard challenges: 11/12 challenge solved; v2 challenges: 0/13 solved end-to-end. Alert threshold met.|Uplift Level 1|CCL not reached|\n|Harmful Manipulation|Model manipulative eﬃcacy improves on non-generative AI baseline, but shows no signiﬁcant uplift versus prior models and did not near alert thresholds.|Level 1 (exploratory)|CCL not reached|"
          },
          {
            "snippetId": "safety-evaluation-snippet-3",
            "url": "https://www.studocu.com/en-gb/document/university-of-east-london/financial-accounting/gemini-3-pro-model-card-insights-safety-evaluations-nov-2025/146206142",
            "snippet": "- Evaluation Methods: The model undergoes rigorous testing across benchmarks to assess its reasoning and multimodal performance.\n\n- Safety Policies: Strict guidelines are in place to prevent harmful content generation, ensuring responsible AI use.\n\n- Intended Applications: Designed for complex problem-solving, the model excels in areas requiring advanced reasoning and creativity.\n\n## Preview text\n\nModel card published: November, 2025... ## Gemini 3 Pro\n\n## Model Card\n\n### Gemini 3 Pro - Model Card\n\nModel Cards are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from time-to-time; for example, to include updated evaluations as the model is improved or revised. See the Google DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\nPublished / Model Release: November 2025... #### Model Information\n\nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of highly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from different information sources, including text, audio, images, video, and entire code repositories.... #### Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\n● Gemini App ● Google Cloud / Vertex AI ● Google AI Studio ● Gemini API ● Google AI Mode ● Google Antigravity... #### Evaluation\n\nApproach: Gemini 3 Pro was evaluated across a range of benchmarks, including reasoning, multimodal capabilities, agentic tool use, multi-lingual performance, and long-context. Additional benchmarks and details on approach, results and their methodologies can be found at: deepmind/models/evals/gemini-3-pro.... Results: Gemini 3 Pro significantly outperforms Gemini 2 Pro across a range of benchmarks requiring enhanced reasoning and multimodal capabilities. Results as of November, 2025 are listed below:\n\nSafety Policies: Gemini’s safety policies aim to prevent our Generative AI models from generating harmful content, including:... Training and Development Evaluation Results: Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2 Pro across both safety and tone, while keeping unjustified refusals low. We mark improvements in green and regressions in red.... Evaluation 1 Description Gemini 3 Pro vs. Gemini 2 Pro\n\nText to Text Safety Automated content safety evaluation measuring safety policies -10%\n\nMultilingual Safety Automated safety policy evaluation across multiple languages +0% (non-egregious)\n\nImage to Text Safety Automated content safety evaluation measuring safety policies +3% (non-egregious)... Tone 2 Automated evaluation measuring objective tone of model refusal +7%\n\nUnjustified-refusals\n\nAutomated evaluation measuring model’s ability to respond to borderline prompts while remaining safe\n\n+3% (non-egregious)\n\nWe continue to improve our internal evaluations, including refining automated evaluations to reduce false positives and negatives, as well as update query sets to ensure balance and maintain a high standard of results. The performance results reported below are computed with improved evaluations and thus are not directly comparable with performance results found in previous Gemini model cards.... Human Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level findings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams to protect children online and meet Google’s commitments to child safety across our models and Google products.... For content safety policies generally, including child safety, we saw similar or improved safety performance compared to Gemini 2 Pro. Compared to 2 Pro, the scope of red teaming was expanded to cover more potential issues outside of our strict policies, and found no egregious concerns.\n\nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and deployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations include, but are not limited to:... ● dataset filtering; ● conditional pre-training; ● supervised fine-tuning; ● reinforcement learning from human and critic feedback; ● safety policies and desiderata; ● product-level mitigations such as safety filtering.\n\nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2 Pro but still an open research problem), and b) possible degradation in multi-turn conversations."
          },
          {
            "snippetId": "safety-evaluation-snippet-4",
            "url": "https://ai.google.dev/gemini-api/docs/safety-guidance",
            "snippet": "Generative artificial intelligence models are powerful tools, but they are not without their limitations. Their versatility and applicability can sometimes lead to unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing, and rigorous manual evaluation are essential to limit the risk of harm from such outputs.... The models provided by the Gemini API can be used for a wide variety of generative AI and natural language processing (NLP) applications. Use of these functions is only available through the Gemini API or the Google AI Studio web app. Your use of Gemini API is also subject to the Generative AI Prohibited Use Policy and the Gemini API terms of service.... While the Gemini API has been designed with Google's AI principles in mind, the onus is on developers to apply these models responsibly. To aid developers in creating safe, responsible applications, the Gemini API has some built-in content filtering as well as adjustable safety settings across 4 dimensions of harm. Refer to the safety settings guide to learn more.... This document is meant to introduce you to some safety risks that can arise when using LLMs, and recommend emerging safety design and development recommendations. (Note that laws and regulations may also impose restrictions, but such considerations are beyond the scope of this guide.)\n\nThe following steps are recommended when building applications with LLMs:\n\n- Understanding the safety risks of your application\n\n- Considering adjustments to mitigate safety risks\n\n- Performing safety testing appropriate to your use case\n\n- Soliciting feedback from users and monitoring usage\n\nThe adjustment and testing phases should be iterative until you reach performance appropriate for your application.... ## Understand the safety risks of your application\n\nIn this context, safety is being defined as the ability of an LLM to avoid causing harm to its users, for example, by generating toxic language or content that promotes stereotypes. The models available through the Gemini API have been designed with Google’s AI principles in mind and your use of it is subject to the Generative AI Prohibited Use Policy.... The API provides built-in safety filters to help address some common language model problems such as toxic language and hate speech, and striving for inclusiveness and avoidance of stereotypes. However, each application can pose a different set of risks to its users. So as the application owner, you are responsible for knowing your users and the potential harms your application may cause, and ensuring that your application uses LLMs safely and responsibly.... **Blocking unsafe inputs and filtering output before it is shown to the user.**In simple situations, blocklists can be used to identify and block unsafe words or phrases in prompts or responses, or require human reviewers to manually alter or block such content. **Using trained classifiers to label each prompt with potential harms or adversarial signals.**Different strategies can then be employed on how to handle the request based on the type of harm detected. For example, If the input is overtly adversarial or abusive in nature, it could be blocked and instead output a pre-scripted response.... ## Perform safety testing appropriate to your use case\n\nTesting is a key part of building robust and safe applications, but the extent, scope and strategies for testing will vary. For example, a just-for-fun haiku generator is likely to pose less severe risks than, say, an application designed for use by law firms to summarize legal documents and help draft contracts. But the haiku generator may be used by a wider variety of users which means the potential for adversarial attempts or even unintended harmful inputs can be greater.... **Safety benchmarking**involves designing safety metrics that reflect the ways your application could be unsafe in the context of how it is likely to get used, then testing how well your application performs on the metrics using evaluation datasets. It's good practice to think about the minimum acceptable levels of safety metrics before testing so that 1) you can evaluate the test results against those expectations and 2) you can gather the evaluation dataset based on the tests that evaluate the metrics you care about most.... composition of the data used for testing. For adversarial tests, select\n\ntest data that is most likely to elicit problematic output from\n\nthe model. This means probing the model's behavior for all the types of\n\nharms that are possible, including rare or unusual examples and\n\nedge-cases that are relevant to safety policies. It should also include\n\ndiversity in the different dimensions of a sentence such as structure,\n\nmeaning and length. You can refer to the Google's Responsible AI\n\npractices in\n\nfairness\n\nfor more details on what to consider when building a test dataset.... #### Advanced tips\n\n- Use automated testing instead of the traditional method of enlisting people in 'red teams' to try and break your application. In automated testing, the 'red team' is another language model that finds input text that elicit harmful outputs from the model being tested.\n\n- Adversarial testing is a method for systematically evaluating an ML model with the intent of learning how it behaves when provided with malicious or inadvertently harmful input:"
          },
          {
            "snippetId": "safety-evaluation-snippet-5",
            "url": "https://www.youtube.com/watch?v=-XTNBIVXRGA",
            "snippet": "## AI Papers Podcast Daily\n\n##### Nov 20, 2025 (0:13:04)\n\nThe **Gemini 3 Pro Model Card**, published by Google DeepMind in November 2025, introduces the latest generation of the Gemini series, describing it as Google’s most advanced, highly-capable, and natively multimodal reasoning model. Architecturally, Gemini 3 Pro is a sparse Mixture-of-Experts (MoE) transformer-based model, capable of understanding vast datasets and complex problems from various inputs, including text, images, audio, video, and code repositories, utilizing an extensive 1M token context window.... The model was trained using reinforcement learning techniques on a large, diverse dataset, and processed efficiently using Google’s specialized Tensor Processing Units (TPUs). Intended for applications requiring enhanced reasoning, intelligence, advanced coding, and agentic performance, Gemini 3 Pro significantly outperforms the previous Gemini 2.5 Pro model across a range of benchmarks. Development included continuous training evaluations, human red teaming by specialist teams, and adherence to safety policies designed to prevent the generation of harmful content, confirming that Gemini 3 Pro satisfied required launch thresholds for child safety and showed improved overall safety performance compared to its predecessor.... https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf... The card is also clear about the\n{ts:353} pre-processing, dduplication, and things like honoring robots.txt. And crucially, the safety filtering.\n{ts:360} They talk about strict processes to filter out harmful content pornographic violent CSAM violating material. The\n{ts:367} foundation has to be as clean as possible.... That kind of strategic capability leads\n{ts:553} us right into section four. Intended use and crucial limitations. With that much power, what is this model for?\n{ts:559} It's for applications that need its core strengths that enhance reasoning, intelligence, and strategic planning.\n{ts:565} The model card specifically calls out agentic performance, advanced coding, long context reasoning, and algorithmic... {ts:571} development. If you need a model that can think several steps ahead, this is the tool.\n{ts:575} But even with this huge leap, we have to be realistic. What are the known limitations that the model card flags?\n{ts:582} Well, the usual foundation model issues are still there, primarily hallucinations.... It's not perfect.\n{ts:587} Developers have to build in safeguards. They also note occasional slowness or timeout issues, which probably reflects\n{ts:593} the complexity of the MOI routing. And a really critical point for anyone who needs up-to-date information.\n{ts:599} Yes, the card clearly states the model's knowledge cutoff was January 2025.... Google is explicit that their prohibited use policy applies.\n{ts:624} Clear bans on dangerous or illicit activities, trying to compromise security, and very clearly spreading\n{ts:629} misinformation. That focus on responsibility brings us to our last section, ethics and frontier\n{ts:634} safety evaluation. This is arguably the most rigorous part of the whole model card.... {ts:638} The safety approach was really comprehensive, built with their safety teams, aligned with Google's AI\n{ts:644} principles. They used a two-prong strategy called red teaming. Right. Where you have experts\n{ts:649} deliberately trying to break the model to find weaknesses before it's released.... Exactly. Human red teaming had\n{ts:654} specialists testing for harms and the outcome was pretty encouraging. They saw safety performance similar to or better\n{ts:662} than 2.5 Pro. No huge new concerns. Then they also used automated red teaming to run massive stress tests and check for... This is the major safety finding. Gemini 3 Pro did not reach any critical\n{ts:705} capability levels or CCLs in those domains. While it can give you factual information, the assessment found it\n{ts:711} generally fails to provide novel or complete instructions that could really help a bad actor.... The main risks that\n{ts:717} are left are things like jailbreak vulnerabilities, though they're improved, and some possible degradation\n{ts:722} in really long multi-turn conversations. Okay, let's bring this all together. This deep dive into the Gemini 3 Pro\n{ts:730} model card shows a very clear, very intentional direction for these big foundation models.... All of it rigorously checked to make sure it's not\n{ts:754} crossing any critical risk thresholds. It's highly optimized for complex reasoning.\n{ts:758} So what does this all mean for you, the listener, as you watch this field evolve given the successful integration of\n{ts:764} multi-step reasoning and that theorem proving data data designed to teach logic?"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Refusals",
            "score": 1,
            "explanation": "Overall, Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustified refusals low."
          },
          {
            "name": "Disallowed Content Handling",
            "score": 1,
            "explanation": "Gemini's safety policies aim to prevent our Generative AI models from generating harmful content, including: Content related to child sexual abuse material and exploitation."
          },
          {
            "name": "Sycophancy",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Jailbreak",
            "score": 1,
            "explanation": "The main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2.5 Pro but still an open research problem)"
          },
          {
            "name": "Hallucinations",
            "score": 1,
            "explanation": "The model card specifically calls out agentic performance, advanced coding, long context reasoning, and algorithmic development... the usual foundation model issues are still there, primarily hallucinations."
          },
          {
            "name": "Deception Behaviors",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Fairness & Bias Evaluations (incl. BBQ)",
            "score": 1,
            "explanation": "Safety policies aim to prevent harmful content generation, ensuring responsible AI use."
          },
          {
            "name": "Adversarial Robustness",
            "score": 1,
            "explanation": "Adversarial testing is a method for systematically evaluating an ML model with the intent of learning how it behaves when provided with malicious or inadvertently harmful input."
          },
          {
            "name": "Red Teaming Results",
            "score": 1,
            "explanation": "Human red teaming had specialists testing for harms and the outcome was pretty encouraging."
          }
        ]
      },
      {
        "sectionId": "risk-mitigations",
        "sectionSnippets": [
          {
            "snippetId": "risk-mitigations-snippet-1",
            "url": "https://www.youtube.com/watch?v=hf59-PhO3do",
            "snippet": "Chapters:\n00:00 - Gemini 3 Executive Summary\n01:37 - MoE Architecture Explained\n03:07 - 1 Million Token Capabilities\n03:49 - Synthetic Data and Training\n04:58 - Reasoning and Multimodal Benchmarks\n06:54 - Deep Think Ultra Tier\n08:01 - Vibe Coding Performance\n09:09 - Anti-Gravity Agent Platform\n10:35 - Safety Metrics and Regressions\n11:56 - Frontier Risk Analysis\n13:39 - Availability and Reliability Challenges... And the headline here really isn't just\n{ts:15} an incremental update. This is a a pretty decisive architectural leap, I think, toward what you could call\n{ts:22} egentic AGI. The model card confirms it's a sparse mixture of experts or MOE transformer,\n{ts:28} which is so critical.... It's a gentic. It's topping the webdev arena\n{ts:68} and this is the big one for me. It hits 76.2% on the SWE verified benchmark\n{ts:77} which is just wild. Finally, we have to mention the safety. This is the most comprehensive\n{ts:81} evaluation we've seen using the Frontier Safety Framework, the FSF, and it confirms that no critical capability... I mean, we're talking about feeding it entire data sets, huge\n{ts:200} complex documents, long audio files, video, and this is the critical part for developers, entire code repository,\n{ts:208} a whole repo, the whole thing. You can give it the full documentation for a complex library... Let's talk training data. The card mentions a large scale diverse\n{ts:232} and uh natively multimodal collection. So, you have the usual suspects, public web data, licensed data, and some user\n{ts:240} data with, you know, the standard controls. But there was one thing that really... The entire platform is designed to let the agent plan and execute complex end to-end\n{ts:566} tasks on its own and critically validate its own work. Critically, it can run tests, check the\n{ts:572} output, see if the UI looks right in the browser. It's a closed loop.... What about the frontier risks? How is Google\n{ts:640} handling safety? They say it's their most secure model yet. They detail all the mitigations,\n{ts:644} data set filtering, RLHF, critic feedback, all the standard stuff. But we need to look closely at the internal\n{ts:650} safety metrics they shared.... And there's one number that immediately jumps out. A regression. The automated\n{ts:657} evils showed a drop in textto text safety of minus 10.4% compared to 2.5 Pro. Now wait a minute.\n{ts:665} Any regression in safety seems like a massive red flag, doesn't it?... But non-aggregious is a very subjective term. It almost sounds like the model\n{ts:686} got so much smarter that it's now outsmarting the automated safety tests. That's a great way to put it. It signals\n{ts:692} a real challenge in safety scaling. As the models get more complex, our automated safety tools are struggling to... {ts:698} keep up. That said, they did show improvements elsewhere. tone was up nearly 8%, multilingual safety was up a\n{ts:705} little. The takeaway is that this new architecture is just different and the old tools need to catch up.\n{ts:710} Okay, so let's move from the internal metrics to the really crude part, the Frontier Safety Framework analysis.... The\n{ts:717} headline here is that the card states clearly that Gemini 3 Pro did not reach any critical capability levels or CCLs.\n{ts:724} That's the bottom line. Yes, but let's dig into the details. Starting with CBRN chemical, biological, radiological, and... {ts:809} challenges. So, it has some foundational planning abilities, but it's not demonstrating dangerous levels of\n{ts:814} self-motivated autonomy in these tests. It's an agent, but it's not a rogue agent.\n{ts:818} So, what does this all mean for developers today?... The capabilities are obviously huge, and the safety analysis,\n{ts:825} while needing a critical eye, seems to clear it for general release. It's rolling out now. Developers can get\n{ts:830} it in AI Studio, Vert.Ex AI, the CLI, and through that new anti-gravity platform.... {ts:869} problem. The goal is an agent that can operate autonomously for weeks, months, fixing code, managing systems. For that,"
          },
          {
            "snippetId": "risk-mitigations-snippet-2",
            "url": "https://web.archive.org/web/20251118111103if_/https:/storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf",
            "snippet": "Gemini 3 Pro \nModel Card\nGemini 3 Pro - Model Card \n \n \n \nModel Cards are intended to provide essential information on Gemini models, including known \nlimitations, mitigation approaches, and safety performance. Model cards may be updated from \ntime-to-time; for example, to include updated evaluations as the model is improved or revised. See the \nGoogle DeepMind site for a comprehensive list of model cards.... Distribution \n \nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; \nrespective documentation shared in line: \n \n●​\nGemini App \n●​\nGoogle Cloud / Vertex AI \n●​\nGoogle AI Studio \n●​\nGemini API \n●​\nGoogle AI Mode \n●​... Evaluation Approach: Gemini 3 Pro was developed in partnership with internal safety, security, and \nresponsibility teams. A range of evaluations and red teaming activities were conducted to help improve \nthe model and inform decision-making. These evaluations and activities align with Google's AI Principles \nand responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use... team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the \nmodel adheres to safety policies and desired outcomes;  \n●​\nAutomated Red Teaming to dynamically evaluate Gemini for safety and security \nconsiderations at scale, complementing human red teaming and static evaluations; \n●​\nEthics & Safety Reviews were conducted ahead of the model’s release... 5\nIn addition, we perform testing following the guidelines in Google DeepMind’s Frontier Safety \nFramework (FSF). \n \nSafety Policies: Gemini’s safety policies aim to prevent our Generative AI models from generating \nharmful content, including:  \n \n1.​\nContent related to child sexual abuse material and exploitation \n2.​... Training and Development Evaluation Results:  Results for some of the internal safety evaluations \nconducted during the development phase are listed below. The evaluation results are for automated \nevaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage \nincrease or decrease in performance compared to the indicated model, as described below. Overall,... Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustified refusals \nlow. We mark improvements in green and regressions in red. \n \n \nEvaluation1 \nDescription \nGemini 3 Pro \nvs. Gemini 2.5 Pro \nText to Text Safety... Automated content safety evaluation \nmeasuring safety policies \n-10.4% \nMultilingual Safety  \nAutomated safety policy evaluation across \nmultiple languages \n+0.2% (non-egregious) \nImage to Text Safety \nAutomated content safety evaluation \nmeasuring safety policies \n+3.1% (non-egregious)... standard of results. The performance results reported below are computed with improved evaluations \nand thus are not directly comparable with performance results found in previous Gemini model cards. \n \nWe expect variation in our automated safety evaluations results, which is why we review flagged content \nto check for egregious or dangerous material. Our manual review confirmed losses were... overwhelmingly either a) false positives or b) not egregious. \n \nHuman Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of \nthe model development team. High-level findings are fed back to the model team. For child safety \nevaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams... to protect children online and meet Google’s commitments to child safety across our models and \nGoogle products. For content safety policies generally, including child safety, we saw similar or improved \nsafety performance compared to Gemini 2.5 Pro. Compared to 2.5 Pro, the scope of red teaming was \nexpanded to cover more potential issues outside of our strict policies, and found no egregious... concerns. \n \nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and \ndeployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations \ninclude, but are not limited to:  \n \n●​\ndataset filtering;  \n●​... conditional pre-training; \n●​\nsupervised fine-tuning; \n●​\nreinforcement learning from human and critic feedback; \n●​\nsafety policies and desiderata; \n●​\nproduct-level mitigations such as safety filtering. \n \nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2.5 Pro but \nstill an open research problem), and b) possible degradation in multi-turn conversations. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n7... Frontier Safety \n \nWe evaluated Gemini 3 Pro as outlined in our latest Frontier Safety Framework (September-2025), \nand found that it did not reach any critical capability levels as outlined in the table below:   \n \nDomain \nKey Results for Gemini 3 Pro  \nCCL  \nCCL \nreached?"
          },
          {
            "snippetId": "risk-mitigations-snippet-3",
            "url": "https://huggingface.co/datasets/multimodalart/google-gemini-3-pro-pre-release-model-card",
            "snippet": "# Gemini 3 Pro - Model Card\n\n*Model Cards* *are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from*\n\n*time-to-time; for example, to include updated evaluations as the model is improved or revised. See the* *Google DeepMind site* *for a comprehensive list of model cards.*... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\n*Published / Model Release: November 2025*... ## Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\nOur models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... **Known Limitations:** Gemini 3 Pro may exhibit some of the general limitations of foundation models, such as hallucinations. There may also be occasional slowness or timeout issues. The knowledge cutoﬀ date for Gemini 3 Pro was January 2025.... ## Ethics and Content Safety\n\n**Evaluation Approach:** Gemini 3 Pro was developed in partnership with internal safety, security, and responsibility teams. A range of evaluations and red teaming activities were conducted to help improve the model and inform decision-making. These evaluations and activities align with Google's AI Principles and responsible AI approach, as well as Google's Generative AI policies (e.g. Gen AI Prohibited Use Policy and the Gemini API Additional Terms of Service).... Evaluation types included but were not limited to:\n\n**Training/Development Evaluations**including automated and human evaluations carried out continuously throughout and after the model's training, to monitor its progress and performance; **Human Red Teaming**conducted by specialist teams who sit outside of the model development team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the model adheres to safety policies and desired outcomes; **Automated Red Teaming**to dynamically evaluate Gemini for safety and security considerations at scale, complementing human red teaming and static evaluations; **Ethics & Safety Reviews**were conducted ahead of the model's release... **Training and Development Evaluation Results:** Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2.5 Pro across both safety and tone, while keeping unjustiﬁed refusals low. We mark improvements in green and regressions in red.... We continue to improve our internal evaluations, including reﬁning automated evaluations to reduce false positives and negatives, as well as update query sets to ensure balance and maintain a high\n\n1The ordering of evaluations in this table has changed from previous iterations of the 2.5 Flash-Lite model card in order to list safety evaluations together and improve readability. The type of evaluations listed have remained the same.... **Human Red Teaming Results:** We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level ﬁndings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisﬁed required launch thresholds, which were developed by expert teams to protect children online and meet Google's commitments to child safety across our models and Google products.... For content safety policies generally, including child safety, we saw similar or improved safety performance compared to Gemini 2.5 Pro. Compared to 2.5 Pro, the scope of red teaming was expanded to cover more potential issues outside of our strict policies, and found no egregious concerns.\n\n**Risks and Mitigations:** Safety and responsibility was built into Gemini 3 Pro throughout the training and deployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations include, but are not limited to:... - dataset ﬁltering;\n\n- conditional pre-training;\n\n- supervised ﬁne-tuning;\n\n- reinforcement learning from human and critic feedback;\n\n- safety policies and desiderata;\n\n- product-level mitigations such as safety ﬁltering.\n\nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2.5 Pro but still an open research problem), and b) possible degradation in multi-turn conversations.... ## Frontier Safety\n\nWe evaluated Gemini 3 Pro as outlined in our latest Frontier Safety Framework (September-2025), and found that it did not reach any critical capability levels as outlined in the table below:"
          },
          {
            "snippetId": "risk-mitigations-snippet-4",
            "url": "https://ai.google.dev/gemini-api/docs/changelog",
            "snippet": "This page documents updates to the Gemini API.\n\n## November 20, 2025\n\n- Released Gemini 3 Pro Image Preview,\n\n`gemini-3-pro-image-preview`, the next iteration to the Nano Banana model. Read the Image generation page for more details.... ## November 18, 2025\n\nLaunched the first Gemini 3 series model,\n\n`gemini-3-pro-preview`, our state-of-the-art reasoning and multimodal understanding model with powerful agentic and coding capabilities.\n\nIn addition to improvements in intelligence and performance, Gemini 3 Pro Preview introduces new behavior around:\n\nRead the Gemini 3 Developer Guide for migration, new features, and specs.... ## September 25, 2025\n\nReleased Gemini Robotics-ER 1.5 model in preview. See the Robotics overview to learn about how to use the model for your robotics application.\n\nLaunched following preview models:\n\n`gemini-2.5-flash-preview-09-2025`\n\n`gemini-2.5-flash-lite-preview-09-2025`\n\nSee the Models page for details.... ## June 17, 2025\n\n- Released\n\n`gemini-2.5-pro`, the stable version of our most powerful model, now with adaptive thinking. To learn more, see Gemini 2.5 Pro and Thinking.\n\n`gemini-2.5-pro-preview-05-06`will be redirected to... **Model updates:**\n\n- Released\n\n`gemini-2.5-flash-preview-05-20`, a Gemini preview model optimized for price-performance and adaptive thinking. To learn more, see Gemini 2.5 Flash Preview and Thinking.\n\n- Released the\n\n`gemini-2.5-pro-preview-tts`and... ## May 7, 2025\n\n- Released\n\n`gemini-2.0-flash-preview-image-generation`, a preview model for generating and editing images. To learn more, see Image generation and Gemini 2.0 Flash Preview Image Generation.\n\n## May 6, 2025\n\n- Released\n\n`gemini-2.5-pro-preview-05-06`, a new version of our most powerful model, with improvements on code and function calling.\n\n`gemini-2.5-pro-preview-03-25`will automatically point to the new version of the model.... ## March 12, 2025\n\n**Model updates:**\n\n- Launched an experimental Gemini 2.0 Flash model capable of image generation and editing.\n\n- Released\n\n`gemma-3-27b-it`, available on AI Studio and through the Gemini API, as part of the Gemma 3 launch.\n\n**API updates:**\n\n- Added support for YouTube URLs as a media source.\n\n- Added support for including an inline video of less than 20MB.... ## October 3, 2024\n\n**Model updates:**\n\n- Released\n\n`gemini-1.5-flash-8b-001`, a stable version of our smallest Gemini API model.... ## September 24, 2024\n\n**Model updates:**\n\n- Released\n\n`gemini-1.5-pro-002`and\n\n`gemini-1.5-flash-002`, two new stable versions of Gemini 1.5 Pro and 1.5 Flash, for general availability.... - Updated the\n\n`gemini-1.5-pro-latest`model code to use\n\n`gemini-1.5-pro-002`and the\n\n`gemini-1.5-flash-latest`model code to use\n\n`gemini-1.5-flash-002`.\n\n- Released... `gemini-1.5-flash-8b-exp-0924`to replace\n\n`gemini-1.5-flash-8b-exp-0827`.\n\n- Released the civic integrity safety filter for the Gemini API and AI Studio.\n\n- Released support for two new parameters for Gemini 1.5 Pro and 1.5 Flash in\n\nPython and NodeJS:\n\n`frequencyPenalty`and\n\n`presencePenalty`.... ## August 27, 2024\n\n**Model updates:**\n\n- Released the following\n\nexperimental models:\n\n`gemini-1.5-pro-exp-0827`\n\n`gemini-1.5-flash-exp-0827`\n\n`gemini-1.5-flash-8b-exp-0827`... ## August 9, 2024\n\n**API updates:**\n\n- Added support for PDF processing.\n\n## August 5, 2024\n\n**Model updates:**\n\n- Fine-tuning support released for Gemini 1.5 Flash.\n\n## August 1, 2024\n\n**Model updates:**\n\n- Released\n\n`gemini-1.5-pro-exp-0801`, a new experimental version of Gemini 1.5 Pro.... ## March 19, 2024\n\n**Model updates:**\n\n- Added support for tuning Gemini 1.0 Pro in Google AI Studio or with the Gemini API."
          },
          {
            "snippetId": "risk-mitigations-snippet-5",
            "url": "https://www.studocu.com/en-gb/document/university-of-east-london/financial-accounting/gemini-3-pro-model-card-insights-safety-evaluations-nov-2025/146206142",
            "snippet": "- Evaluation Methods: The model undergoes rigorous testing across benchmarks to assess its reasoning and multimodal performance.\n\n- Safety Policies: Strict guidelines are in place to prevent harmful content generation, ensuring responsible AI use.\n\n- Intended Applications: Designed for complex problem-solving, the model excels in areas requiring advanced reasoning and creativity.\n\n## Preview text\n\nModel card published: November, 2025... ## Gemini 3 Pro\n\n## Model Card\n\n### Gemini 3 Pro - Model Card\n\nModel Cards are intended to provide essential information on Gemini models, including known limitations, mitigation approaches, and safety performance. Model cards may be updated from time-to-time; for example, to include updated evaluations as the model is improved or revised. See the Google DeepMind site for a comprehensive list of model cards.... This model card includes more essential information about the Gemini 3 family of models than previous model cards did. We hope more information about the training dataset, distribution, and intended uses will empower developers with deeper insights and help build more robust and responsible downstream applications.\n\nPublished / Model Release: November 2025... #### Model Information\n\nDescription: Gemini 3 Pro is the next generation in the Gemini series of models, a suite of highly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google’s most advanced model for complex tasks, and can comprehend vast datasets, challenging problems from different information sources, including text, audio, images, video, and entire code repositories.... Architecture: Gemini 3 Pro is a sparse mixture-of-experts (MoE) (Clark et al., 2022; Du et al., 2021; Fedus et al., 2021; Jiang et al., 2024, Lepikhin et al., 2020; Riquelme et al., 2021; Roller et al., 2021; Shazeer et al., 2017 ) transformer-based model (Vaswani et al., 2017) with native multimodal support for text, vision, and audio inputs.... #### Distribution\n\nThe Gemini family of models, including Gemini 3 Pro, are distributed in the following channels; respective documentation shared in line:\n\n● Gemini App ● Google Cloud / Vertex AI ● Google AI Studio ● Gemini API ● Google AI Mode ● Google Antigravity... Our models are available to downstream providers via an application program interface (API) and subject to relevant terms of use. There is no required hardware or software to use the model. For AI Studio and Gemini API, see the Gemini API Additional Terms of Service; for Vertex AI, see Google Cloud Platform Terms of Service. For more information, see Gemini Model API instructions and Gemini API in Vertex AI quickstart.... Training and Development Evaluation Results: Results for some of the internal safety evaluations conducted during the development phase are listed below. The evaluation results are for automated evaluations and not human evaluation or red teaming. Scores are provided as an absolute percentage increase or decrease in performance compared to the indicated model, as described below. Overall, Gemini 3 Pro outperforms Gemini 2 Pro across both safety and tone, while keeping unjustified refusals low. We mark improvements in green and regressions in red.... Evaluation 1 Description Gemini 3 Pro vs. Gemini 2 Pro\n\nText to Text Safety Automated content safety evaluation measuring safety policies -10%\n\nMultilingual Safety Automated safety policy evaluation across multiple languages +0% (non-egregious)\n\nImage to Text Safety Automated content safety evaluation measuring safety policies +3% (non-egregious)... We expect variation in our automated safety evaluations results, which is why we review flagged content to check for egregious or dangerous material. Our manual review confirmed losses were overwhelmingly either a) false positives or b) not egregious.... Human Red Teaming Results: We conduct manual red teaming by specialist teams who sit outside of the model development team. High-level findings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satisfied required launch thresholds, which were developed by expert teams to protect children online and meet Google’s commitments to child safety across our models and Google products.... For content safety policies generally, including child safety, we saw similar or improved safety performance compared to Gemini 2 Pro. Compared to 2 Pro, the scope of red teaming was expanded to cover more potential issues outside of our strict policies, and found no egregious concerns.\n\nRisks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and deployment lifecycle, including pre-training, post-training, and product-level mitigations. Mitigations include, but are not limited to:... ● dataset filtering; ● conditional pre-training; ● supervised fine-tuning; ● reinforcement learning from human and critic feedback; ● safety policies and desiderata; ● product-level mitigations such as safety filtering.\n\nThe main risks for Gemini 3 Pro are: a) jailbreak vulnerability (improved compared to Gemini 2 Pro but still an open research problem), and b) possible degradation in multi-turn conversations."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Risk Mitigations",
            "score": 1,
            "explanation": "Safety and responsibility was built into Gemini 3 Pro throughout the training and deployment lifecycle, including pre-training, post-training, and product-level mitigations."
          }
        ]
      }
    ]
  },
  {
    "model": "Grok 4",
    "sections": [
      {
        "sectionId": "model-details",
        "sectionSnippets": [
          {
            "snippetId": "model-details-snippet-1",
            "url": "https://docs.x.ai/docs/models/grok-4-0709",
            "snippet": "## Models\n\n## /\n## Grok 4\n\nOur latest and greatest flagship model, offering unparalleled performance in natural language, math and reasoning - the perfect jack of all trades.\n\n#### At a glance\n\nModalities\n\nContext window\n\n256,000\n\nPricing\n\n#### Capabilities\n\nFunction calling\n\nConnect the xAI model to external tools and systems.\n\nStructured outputs\n\nReturn responses in a specific, organized formats.\n\nReasoning\n\nThe model thinks before responding.... #### Pricing\n\n#### Input\n\nTokens\n\n/ 1M tokens\n\nCached tokens\n\n/ 1M tokens\n\n#### Output\n\nTokens\n\n/ 1M tokens\n\nLive search\n\n/ 1K sources\n\nYou are charged for each token used when making calls to our API.\n\nUsing cached input tokens can significantly reduce your costs.... #### Details\n\nModel name\n\nAliases\n\n|Region|us-east-1|\n|--|--|\n|Pricing per million tokens *|\n|Input|\n|Cached input|\n|Output|\n|Per thousand sources|\n|Live search|\n|Rate limits|\n|Requests per minute|480|\n|Tokens per minute|2,000,000|\nWe charge different rates for requests which exceed the 128K context window... #### Quickstart\n\n```\n\nfrom xai_sdk import Client\n\nfrom xai_sdk.chat import user, system\n\nclient = Client(api_key=\"<YOUR_XAI_API_KEY_HERE>\")\n\nchat = client.chat.create(model=\"grok-4-0709\", temperature=0)\n\nchat.append(system(\"You are a PhD-level mathematician.\"))\n\nchat.append(user(\"What is 2 + 2?\"))\n\nresponse = chat.sample()\n\nprint(response.content)\n\n```... #### Quickstart\n\nCreate an API Key and make your first request.\n\n#### Tool use\n\nLet Grok perform actions and look up information.\n\n#### Structured outputs\n\nLet Grok generate structured outputs."
          },
          {
            "snippetId": "model-details-snippet-2",
            "url": "https://docs.x.ai/docs/overview",
            "snippet": "#### Getting started\n# Welcome\n\nWelcome to the xAI developer docs! Our API makes it easy to harness Grok's intelligence in your projects. Grok is our flagship AI model designed to deliver truthful, insightful answers.\n\n## Grok 4 now available\n\nWe're proud to bring you Grok 4 access on the API. Grok 4 currently supports text modality with vision, image gen and other capabilities coming soon.\n\nModalities\n\nContext window\n\n256,000\n\nFeatures\n\nFunction calling\n\nStructured outputs\n\nReasoning... ## Jump right in\n\n#### Quickstart\n\nCreate an API Key and make your first request.\n\n#### Tool Use\n\nLet Grok perform actions and look up information.\n\n#### Images\n\nUse Grok to analyze images or perform OCR.\n\n*Are you a non-developer or simply looking for our consumer services? Visit Grok.com or download one of the iOS or Android apps. See our Comparison Table for the differences.*... ## Questions and feedback\n\nIf you have any questions or feedback, feel free to email us at support@x.ai.\n\nHappy Grokking! 😎"
          },
          {
            "snippetId": "model-details-snippet-3",
            "url": "https://simtheory.ai/model-card/grok-4/",
            "snippet": "# Grok 4\n\nxAI's flagship enterprise model (256k context). Excels at data extraction, coding, summarization.\n\n### Model Specifications\n\n### Default Parameters... ## More models from xAI\n\n#### Grok 4.1 Frontier Vision\n\nxAI's flagship enterprise model (2M context). Excels at data extraction, coding, summarization.\n\n#### Grok 4.1 (Reasoning) Frontier Vision\n\nxAI's flagship enterprise model (2M context). Excels at data extraction, coding, summarization.\n\n#### Grok 4 Fast Frontier\n\nFaster Grok 4 with identical quality (2m context). Low latency for time-sensitive tasks and huge context window."
          },
          {
            "snippetId": "model-details-snippet-4",
            "url": "https://docs.public.content.oci.oraclecloud.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "# xAI Grok 4\n\nThe\n\n`xai.grok-4` model has better performance than its predecessor, Grok 3, and excels at enterprise use cases such as data extraction, coding, and summarizing text. This model has a deep domain knowledge in finance, healthcare, law, and science.... ## Available in These Regions\n\n**US East (Ashburn)**(on-demand only) **US Midwest (Chicago)**(on-demand only) **US West (Phoenix)**(on-demand only)\n\n**Important**\n\n**External Calls**\n\nThe xAI Grok models are hosted in an OCI data center, in a tenancy provisioned for xAI. The xAI Grok models, which can be accessed through the OCI Generative AI service, are managed by xAI.... ## Access this Model\n## Key Features\n\n**Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## On-Demand Mode\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response.... ## Troubleshooting\n\n**Issue:** The Grok 4 model doesn't respond.\n\n**Cause:** The **Maximum output tokens** parameter in the playground or the\n\n`max_tokens` parameter in the API is likely too low.\n\n**Action:** Increase the maximum output tokens parameter.\n\n**Reason:** For difficult problems that require reasoning and problem-solving, and for large sophisticated inputs, the\n\n`xai.grok-4` model tends to think and consumes many tokens, so if the\n\n`max_tokens` parameter is too low, the model uses the allocated tokens and doesn't return a final response."
          },
          {
            "snippetId": "model-details-snippet-5",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "# xAI Grok 4\n\nThe\n\n`xai.grok-4` model has better performance than its predecessor, Grok 3, and excels at enterprise use cases such as data extraction, coding, and summarizing text. This model has a deep domain knowledge in finance, healthcare, law, and science.... ## Available in These Regions\n\n**US East (Ashburn)**(on-demand only) **US Midwest (Chicago)**(on-demand only) **US West (Phoenix)**(on-demand only)\n\n**Important**\n\n**External Calls**\n\nThe xAI Grok models are hosted in an OCI data center, in a tenancy provisioned for xAI. The xAI Grok models, which can be accessed through the OCI Generative AI service, are managed by xAI.... ## Access this Model\n\n## Key Features\n\n🔗 **Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## On-Demand Mode\n\n\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\n\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response.... ## Troubleshooting\n\n\n\n**Issue:** The Grok 4 model doesn't respond.\n\n**Cause:** The **Maximum output tokens** parameter in the playground or the\n\n`max_tokens` parameter in the API is likely too low.\n\n**Action:** Increase the maximum output tokens parameter.\n\n**Reason:** For difficult problems that require reasoning and problem-solving, and for large sophisticated inputs, the\n\n`xai.grok-4` model tends to think and consumes many tokens, so if the\n\n`max_tokens` parameter is too low, the model uses the allocated tokens and doesn't return a final response."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Model overview",
            "score": 1,
            "explanation": "Grok 4 is our flagship AI model designed to deliver truthful, insightful answers."
          },
          {
            "name": "Organization developing the model",
            "score": 1,
            "explanation": "The xAI Grok models are hosted in an OCI data center, in a tenancy provisioned for xAI."
          },
          {
            "name": "Model Version",
            "score": 1,
            "explanation": "xAI Grok 4 has better performance than its predecessor, Grok 3."
          },
          {
            "name": "Model Release Date",
            "score": 1,
            "explanation": "General Availability Release Date: 2025-07-23."
          },
          {
            "name": "Model Version Progression",
            "score": 1,
            "explanation": "xAI's flagship enterprise model (256k context). Excels at data extraction, coding, summarization."
          },
          {
            "name": "Model Architecture",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Model Dependencies",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Paper and relevant links",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Model Distribution Forms",
            "score": 1,
            "explanation": "Available On-Demand: Access this model on-demand, through the Console playground or the API."
          }
        ]
      },
      {
        "sectionId": "model-inputs-outputs",
        "sectionSnippets": [
          {
            "snippetId": "model-inputs-outputs-snippet-1",
            "url": "https://docs.x.ai/docs/models/grok-4-0709",
            "snippet": "## Models\n\n## /\n## Grok 4\n\nOur latest and greatest flagship model, offering unparalleled performance in natural language, math and reasoning - the perfect jack of all trades.\n\n#### At a glance\n\nModalities\n\nContext window\n\n256,000\n\nPricing\n\n#### Capabilities\n\nFunction calling\n\nConnect the xAI model to external tools and systems.\n\nStructured outputs\n\nReturn responses in a specific, organized formats.\n\nReasoning\n\nThe model thinks before responding.... #### Pricing\n\n#### Input\n\nTokens\n\n/ 1M tokens\n\nCached tokens\n\n/ 1M tokens\n\n#### Output\n\nTokens\n\n/ 1M tokens\n\nLive search\n\n/ 1K sources\n\nYou are charged for each token used when making calls to our API.\n\nUsing cached input tokens can significantly reduce your costs.... #### Details\n\nModel name\n\nAliases\n\n|Region|us-east-1|\n|--|--|\n|Pricing per million tokens *|\n|Input|\n|Cached input|\n|Output|\n|Per thousand sources|\n|Live search|\n|Rate limits|\n|Requests per minute|480|\n|Tokens per minute|2,000,000|\nWe charge different rates for requests which exceed the 128K context window... #### Quickstart\n\n```\n\nfrom xai_sdk import Client\n\nfrom xai_sdk.chat import user, system\n\nclient = Client(api_key=\"<YOUR_XAI_API_KEY_HERE>\")\n\nchat = client.chat.create(model=\"grok-4-0709\", temperature=0)\n\nchat.append(system(\"You are a PhD-level mathematician.\"))\n\nchat.append(user(\"What is 2 + 2?\"))\n\nresponse = chat.sample()\n\nprint(response.content)\n\n```... #### Quickstart\n\nCreate an API Key and make your first request.\n\n#### Tool use\n\nLet Grok perform actions and look up information.\n\n#### Structured outputs\n\nLet Grok generate structured outputs."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-2",
            "url": "https://data.x.ai/2025-08-20-grok-4-model-card.pdf",
            "snippet": "xAI\nLast updated: August 20, 2025\n1\nIntroduction\nGrok 4 is the latest reasoning model from xAI with advanced reasoning and tool-use capabilities,\nenabling it to achieve new state-of-the-art performance across challenging academic and industry\nbenchmarks. Because our models push the frontier of AI capabilities, we are committed to mitigating... different risk scenarios. We categorize these safety-relevant behaviors as: abuse potential (Section 2.1),\nconcerning propensities (Section 2.2), and dual-use capabilities (Section 2.3). This report describes\nour current evaluation methodology, results, and mitigations for these various behaviors.... In this document, we focus on the Grok 4 model. xAI deploys Grok 4 in both the consumer-facing\napplications (Grok 4 Web) and through an enterprise use-focused API (Grok 4 API). We report\nevaluations for Grok 4 API and Grok 4 Web now available to our customers, including in the EU.... of enabling malicious use. While the general cyber knowledge and exploitation capabilities of Grok\n4 are a significant step up from prior models, third-party testing shows that Grok 4’s end-to-end\noffensive cyber capabilities remain below the level of a human professional.\nAs of this writing, on top of our safety training, we have implemented a system prompt that provides... additional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 overall presents a low\nrisk for malicious use and loss of control.\n2.1... evaluation metric is response rate (i.e., the rate at which the model answered queries that should\nhave been refused) for all three evaluation settings: standard, user jailbreak, and system jailbreak.\nGrok 4 Web does not accept custom system prompts from users, so we do not evaluate with\nsystem jailbreaks.... Agentic abuse.\nGrok 4 introduces advanced reasoning and tool-calling capabilities that enable\nthe model to be used in an “agentic” manner, that is, repeatedly take actions toward a specified\ngoal. Such capabilities introduce additional risks of misuse beyond what is present in conversational\nsettings, such as executing real function calls. To quantify these risks, we use the AgentHarm\n2... Input filters.\nWe also employ model-based filters for both Grok 4 API and Grok 4 Web,\nwhich reject classes of harmful requests, including biological and chemical weapons, self-harm, and\nCSAM.\n2.2\nConcerning Propensities\nAI models may contain propensities that reduce their controllability, such as deception, power-seeking,... control.\n2.2.1\nEvaluations\nDeception.\nWe measure how deceptive the model is by the rate at which the model lies, i.e.,\nknowingly makes false statements intended to be received as true. We find that instructing the model\nto be honest in the system prompt reduces deception, and we implement this mitigation in Grok... such as knowledge of Metasploit, to understanding of vulnerability detection, to reverse engineering\nsimple binaries.\nCyber agents.\nWe also evaluate Grok 4’s agentic hacking capabilities on CyBench, a collection\nof 40 capture-the-flag-style questions which measures a model’s ability to perform cybersecurity\nchallenges [Zhang et al., 2025]. The model is placed in an agent harness which gives it access to... 38.4% and 22.1% on VCT, so both Grok 4 API and Grok 4 Web achieve superhuman performance\non identifying issues in biological protocols and wetlab virology experiments. Similarly, Grok 4\nAPI shows strong capabilities in both cybersecurity and chemistry. Note that these evaluations... system prompts (Section 3.2).\n3.1\nData and Training\nGrok 4 is first pre-trained with a data recipe that includes publicly available Internet data, data\nproduced by third-parties for xAI, data from users or contractors, and internally generated data. We\nperform data filtering procedures on the training data, such as de-duplication and classification, to... ensure data quality and safety prior to training. In addition to pre-training, our recipe uses a variety\nof reinforcement learning techniques—human feedback, verifiable rewards, and model grading—along\nwith supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-\nprompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n7... language models. In The Thirteenth International Conference on Learning Representations, 2025.\nURL https://openreview.net/forum?id=tc90LV0yRL.\n8"
          },
          {
            "snippetId": "model-inputs-outputs-snippet-3",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "## Access this Model\n\n## Key Features\n\n🔗 **Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## Limits\n\n\n\n- Image Inputs\n\n\n\n**Console:**Upload one or more\n\n`.png`or\n\n`.jpg`images, each 5 MB or smaller.\n\n**API:**Submit a\n\n`base64`encoded version of an image, ensuring that each converted image is more than 512 and less than 1,792 tokens. For example, a 512 x 512 image typically converts to around 1,610 tokens.... ## On-Demand Mode\n\n\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\n\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... For large inputs with difficult problems, set a high value for the maximum output tokens parameter. See Troubleshooting.\n\n**Temperature**\n\n\n\nThe level of randomness used to generate the output text. Min: 0, Max: 2\n\n**Tip**\n\nStart with the temperature set to 0 or less than one, and increase the temperature as you regenerate the prompts for a more creative output. High temperatures can introduce hallucinations and factually incorrect information.... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response.... ## Troubleshooting\n\n\n\n**Issue:** The Grok 4 model doesn't respond.\n\n**Cause:** The **Maximum output tokens** parameter in the playground or the\n\n`max_tokens` parameter in the API is likely too low.\n\n**Action:** Increase the maximum output tokens parameter.\n\n**Reason:** For difficult problems that require reasoning and problem-solving, and for large sophisticated inputs, the\n\n`xai.grok-4` model tends to think and consumes many tokens, so if the\n\n`max_tokens` parameter is too low, the model uses the allocated tokens and doesn't return a final response."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-4",
            "url": "https://docs.public.content.oci.oraclecloud.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "## Access this Model\n## Key Features\n\n**Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## Limits\n\n- Image Inputs\n\n\n\n**Console:**Upload one or more\n\n`.png`or\n\n`.jpg`images, each 5 MB or smaller.\n\n**API:**Submit a\n\n`base64`encoded version of an image, ensuring that each converted image is more than 512 and less than 1,792 tokens. For example, a 512 x 512 image typically converts to around 1,610 tokens.... ## On-Demand Mode\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... For large inputs with difficult problems, set a high value for the maximum output tokens parameter. See Troubleshooting.\n\n**Temperature**\n\n\n\nThe level of randomness used to generate the output text. Min: 0, Max: 2\n\n**Tip**\n\nStart with the temperature set to 0 or less than one, and increase the temperature as you regenerate the prompts for a more creative output. High temperatures can introduce hallucinations and factually incorrect information.... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response.... ## Troubleshooting\n\n**Issue:** The Grok 4 model doesn't respond.\n\n**Cause:** The **Maximum output tokens** parameter in the playground or the\n\n`max_tokens` parameter in the API is likely too low.\n\n**Action:** Increase the maximum output tokens parameter.\n\n**Reason:** For difficult problems that require reasoning and problem-solving, and for large sophisticated inputs, the\n\n`xai.grok-4` model tends to think and consumes many tokens, so if the\n\n`max_tokens` parameter is too low, the model uses the allocated tokens and doesn't return a final response."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-5",
            "url": "https://openrouter.ai/x-ai/grok-4",
            "snippet": "# xAI: Grok 4\n\n### x-ai/grok-4\n\nCreated Jul 9, 2025256,000 context\n\nStarting at $3/M input tokensStarting at $15/M output tokens\n\nOpenRouter provides an OpenAI-compatible completion API to 400+ models & providers that you can call directly, or using the OpenAI SDK. Additionally, some third-party SDKs are available.... In the examples below, the OpenRouter-specific headers are optional. Setting them allows your app to appear on the OpenRouter leaderboards.\n\nFor information about using third-party SDKs and frameworks with OpenRouter, please see our frameworks documentation.\n\nSee the Request docs for all possible fields, and Parameters for explanations of specific sampling parameters."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Inputs",
            "score": 1,
            "explanation": "You are charged for each token used when making calls to our API."
          },
          {
            "name": "Outputs",
            "score": 1,
            "explanation": "Return responses in a specific, organized formats."
          },
          {
            "name": "Token Count",
            "score": 1,
            "explanation": "maximum prompt + response length is 128,000 tokens"
          }
        ]
      },
      {
        "sectionId": "model-data",
        "sectionSnippets": [
          {
            "snippetId": "model-data-snippet-1",
            "url": "https://data.x.ai/2025-08-20-grok-4-model-card.pdf",
            "snippet": "xAI\nLast updated: August 20, 2025\n1\nIntroduction\nGrok 4 is the latest reasoning model from xAI with advanced reasoning and tool-use capabilities,\nenabling it to achieve new state-of-the-art performance across challenging academic and industry\nbenchmarks. Because our models push the frontier of AI capabilities, we are committed to mitigating... different risk scenarios. We categorize these safety-relevant behaviors as: abuse potential (Section 2.1),\nconcerning propensities (Section 2.2), and dual-use capabilities (Section 2.3). This report describes\nour current evaluation methodology, results, and mitigations for these various behaviors.... In this document, we focus on the Grok 4 model. xAI deploys Grok 4 in both the consumer-facing\napplications (Grok 4 Web) and through an enterprise use-focused API (Grok 4 API). We report\nevaluations for Grok 4 API and Grok 4 Web now available to our customers, including in the EU.... Finally, we describe our training pipeline (Section 3.1) and additional transparency commitments\n(Section 3.2).\n2\nEvaluations\nOur approach to model evaluations varies depending on the specific behavior under assessment.\nTo reduce the potential for abuse of Grok 4 that might lead to serious injury to people, property or... We also reduce various propensities of Grok 4 that might make it difficult to control, such as\nbeing deceptive, power-seeking, manipulative, or biased, among others (Section 2.2). To achieve this,\nour main focus is on measuring and reducing the rate at which Grok 4 responds deceptively. We... of enabling malicious use. While the general cyber knowledge and exploitation capabilities of Grok\n4 are a significant step up from prior models, third-party testing shows that Grok 4’s end-to-end\noffensive cyber capabilities remain below the level of a human professional.\nAs of this writing, on top of our safety training, we have implemented a system prompt that provides... additional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 overall presents a low\nrisk for malicious use and loss of control.\n2.1... Input filters.\nWe also employ model-based filters for both Grok 4 API and Grok 4 Web,\nwhich reject classes of harmful requests, including biological and chemical weapons, self-harm, and\nCSAM.\n2.2\nConcerning Propensities\nAI models may contain propensities that reduce their controllability, such as deception, power-seeking,... control.\n2.2.1\nEvaluations\nDeception.\nWe measure how deceptive the model is by the rate at which the model lies, i.e.,\nknowingly makes false statements intended to be received as true. We find that instructing the model\nto be honest in the system prompt reduces deception, and we implement this mitigation in Grok... such as knowledge of Metasploit, to understanding of vulnerability detection, to reverse engineering\nsimple binaries.\nCyber agents.\nWe also evaluate Grok 4’s agentic hacking capabilities on CyBench, a collection\nof 40 capture-the-flag-style questions which measures a model’s ability to perform cybersecurity\nchallenges [Zhang et al., 2025]. The model is placed in an agent harness which gives it access to... measure dual-use knowledge: a high score indicates greater capability to enable weapons development,\nnot necessarily increased risk.\nCategory\nEvaluation\nMetric\nGrok 4 API\nGrok 4 Web\nPersuasion\nMakeMeSay\nwin rate\n0.12\n-\nBiology\nBioLP-Bench\naccuracy... system prompts (Section 3.2).\n3.1\nData and Training\nGrok 4 is first pre-trained with a data recipe that includes publicly available Internet data, data\nproduced by third-parties for xAI, data from users or contractors, and internally generated data. We\nperform data filtering procedures on the training data, such as de-duplication and classification, to... ensure data quality and safety prior to training. In addition to pre-training, our recipe uses a variety\nof reinforcement learning techniques—human feedback, verifiable rewards, and model grading—along\nwith supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-\nprompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n7... language models. In The Thirteenth International Conference on Learning Representations, 2025.\nURL https://openreview.net/forum?id=tc90LV0yRL.\n8"
          },
          {
            "snippetId": "model-data-snippet-2",
            "url": "https://data.x.ai/2025-09-19-grok-4-fast-model-card.pdf",
            "snippet": "xAI\nLast updated: September 19, 2025\n1\nIntroduction\nGrok 4 Fast is an efficiency-focused model from xAI which offers reasoning capabilities near the\nlevel of Grok 4 with much lower latency and cost, as well as the ability to skip reasoning entirely\nfor the lowest latency applications.... Grok 4 Fast was pre-trained on a general purpose data corpus, then post-trained on various tasks\nand tool use, as well as demonstrations of correct refusal behaviors according to our default safety\npolicy. We also deploy Grok 4 Fast in our API with a fixed system prompt prefix that reminds\nthe model of our safety policy, in addition to input filters to safeguard against abuse.... Prior to release, we have evaluated various specific safety-relevant behaviors of Grok 4 Fast: abuse\npotential (Section 2.1), concerning propensities (Section 2.2), and dual-use capabilities (Section 2.3).\nIn this report, we describe our current evaluation methodology, results, and any mitigations for these... various behaviors in Grok 4 Fast. All evaluations in this report were conducted on a near-final\nrelease checkpoint.\n2\nEvaluations\nOur approach to model evaluations varies depending on the specific behavior under assessment.\nTo reduce the potential for abuse of Grok 4 Fast that might lead to serious injury to people,... As of this writing, on top of our safety training, we have implemented a system prompt that provides\nadditional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 Fast overall presents... specify further restrictions on model behavior within the system message, such as prohibiting the\nmodel from generating adult content. We train Grok 4 Code to obey additional instructions in\nsystem or user messages, as long as they do not violate the policy message.\nAgentic abuse.\nGrok 4 Fast introduces advanced reasoning and tool-calling capabilities that... 3\npotential abuse, we find that our safeguards are able to greatly reduce AI propensities that may lead\nto loss of control.\n2.2.1\nEvaluations\nDeception.\nWe measure how deceptive the model is by the rate at which the model lies, i.e.,\nknowingly makes false statements intended to be received as true. We find that instructing the model... evaluating the model in non-reasoning mode increases the rate of dishonesty by a noticeable margin.\nFor Grok 4 Fast, our deployed system prompt does not include explicit instructions to avoid\ndeceptive behavior. Including such instructions, e.g. by appending the line “You are Grok, built by... in particular the cyber killchain [Hutchins et al., 2011]. The questions span a variety of topics,\nsuch as knowledge of Metasploit, to understanding of vulnerability detection, to reverse engineering\nsimple binaries.\nCyber agents.\nWe also evaluate Grok 4 Fast’s agentic hacking capabilities on CyBench,... a collection of 40 capture-the-flag-style questions which measures a model’s ability to perform\ncybersecurity challenges [Zhang et al., 2025]. The model is placed in an agent harness which gives\nit access to tools such as code execution. We use the open-source Inspect evaluation framework\ndeveloped by the UK AISI, and report the unguided task success rate.... accuracy\n39.0\nVCT\naccuracy\n54.5\nWMDP Bio\naccuracy\n85.2\nChemistry\nWMDP Chem\naccuracy\n77.5\nCybersecurity\nWMDP Cyber\naccuracy\n81.4\nCyBench\nunguided success rate\n30.0... system prompts (Section 3.2).\n3.1\nData and Training\nGrok 4 Fast is first pre-trained with a data recipe that includes publicly available Internet data,\ndata produced by third-parties for xAI, data from users or contractors, and internally generated data.\nWe perform data filtering procedures on the training data, such as de-duplication and classification,... to ensure data quality and safety prior to training. In addition to pre-training, our recipe uses\na variety of reinforcement learning techniques—human feedback, verifiable rewards, and model\ngrading—along with supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-... prompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n6... Li, Ann-Kathrin Dombrowski, Shashwat Goel, Gabriel Mukobi, et al. The wmdp benchmark:\nMeasuring and reducing malicious use with unlearning. In International Conference on Machine\nLearning, pages 28525–28550. PMLR, 2024."
          },
          {
            "snippetId": "model-data-snippet-3",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "# xAI Grok 4\n\nThe\n\n`xai.grok-4` model has better performance than its predecessor, Grok 3, and excels at enterprise use cases such as data extraction, coding, and summarizing text. This model has a deep domain knowledge in finance, healthcare, law, and science.... ## Available in These Regions\n\n**US East (Ashburn)**(on-demand only) **US Midwest (Chicago)**(on-demand only) **US West (Phoenix)**(on-demand only)\n\n**Important**\n\n**External Calls**\n\nThe xAI Grok models are hosted in an OCI data center, in a tenancy provisioned for xAI. The xAI Grok models, which can be accessed through the OCI Generative AI service, are managed by xAI.... ## Access this Model\n\n## Key Features\n\n🔗 **Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## On-Demand Mode\n\n\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\n\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response.... ## Troubleshooting\n\n\n\n**Issue:** The Grok 4 model doesn't respond.\n\n**Cause:** The **Maximum output tokens** parameter in the playground or the\n\n`max_tokens` parameter in the API is likely too low.\n\n**Action:** Increase the maximum output tokens parameter.\n\n**Reason:** For difficult problems that require reasoning and problem-solving, and for large sophisticated inputs, the\n\n`xai.grok-4` model tends to think and consumes many tokens, so if the\n\n`max_tokens` parameter is too low, the model uses the allocated tokens and doesn't return a final response."
          },
          {
            "snippetId": "model-data-snippet-4",
            "url": "https://www.datacamp.com/blog/grok-4",
            "snippet": "Course\n\nAfter we waited months for Grok 3.5, xAI skipped it altogether and jumped straight to Grok 4. (As of November 2025, Grok 4.1 has also launched).\n\nIs the jump justified by the model’s performance?\n\nYes, if you judge by the benchmarks. Right now, Grok 4 is probably the best model in the world on paper.... ## Grok 4\n\nThe Grok 4 family includes only Grok 4 and Grok 4 Heavy, and there’s no mini version that we can use for fast reasoning.\n\nGrok 4 is xAI’s latest single-agent model (unlike Grok 4 Heavy, which uses multiple agents—we’ll get to that in the next section). Based on the livestream, there’s nothing particularly groundbreaking in terms of engineering. The gains seem to come from a series of smaller tweaks and a significant increase in compute, roughly 10x more than what was used for Grok 3.... Source: xAI\n\nThe context window is 128,000 tokens in the app and 256,000 in the API, which gives some room for long-form reasoning but isn’t especially generous by current standards. Gemini 2.5 Pro, for example, offers 1 million. If you’re building with Grok, you’ll likely need to spend time structuring and pruning your context carefully.... It’s less clear how it performs in broader consumer workflows or how well it handles safety at scale. But xAI says it’s already in use at biomedical labs, financial firms, and early enterprise partners.... As Elon Musk pointed out in the livestream, Grok 4’s image understanding and generation still isn’t very advanced. If you want constant and reliable results, I think it’s fair to say that Grok 4 is a text-only model at the moment.... ## Grok 4 Benchmarks\n\nGrok 4’s main claim to fame is its performance across a wide mix of benchmarks, from academic exams to business simulations. According to xAI, the model improves significantly over previous versions thanks mostly to more compute—both during training and inference—not necessarily because of new architectural breakthroughs.... Source: xAI\n\nWith no tool use, Grok 4 plateaus at around 26.9% accuracy. With tools enabled (e.g., code execution), it hits 41.0%. And when run in its multi-agent “Heavy” configuration, it climbs to 50.7%—a major jump that’s over double the best prior tool-free model scores.... Source: xAI\n\nIn short: Grok 4 performs well where xAI has tested it. But as always, you should look beyond the leaderboard. The benchmarks are promising, but they don’t tell the whole story—especially if your use case depends on vision, code generation, or real-time interaction in messy environments.... ## How to Access Grok 4\n\nGrok 4 is now available through three main entry points: the X app, the xAI API, and the grok.com platform. Whether you want to chat with the model, build with it, or test its reasoning capabilities more formally, here’s how to get started.... ### Grok 4 API\n\nIf you want to integrate Grok into your own app or workflow, you can use the xAI API.\n\nSteps:\n\n- Go to https://x.ai/api and request developer access.\n\n- Once approved, you’ll receive an API key and access to docs.\n\n- Make sure to read through the documentation for more details.... ### August: Specialized coding model\n\nThe first follow-up is a coding-focused model expected in August. Unlike Grok 4, which is a generalist, this will be a specialized model designed to handle code with more speed and accuracy. xAI described it as “fast and smart,” trained specifically to improve both latency and reasoning in software development workflows.... ### October: Video generation model\n\nThe final release in the current timeline is a video generation model due in October. xAI says they’ll be training it on over 100,000 GPUs. Based on their remarks, this system will aim to produce high-quality, interactive, and editable video content.... For developers and researchers, it’s worth exploring. For casual users, the speed and responsiveness of Grok 3 or other mainstream models are a better fit. The roadmap is ambitious, with a coding model, multimodal agent, and video generator all due by October. Whether xAI can deliver those on time is another question. But with Grok 4, they’ve at least made a compelling case that they’re in the race.... ### Does Grok 4 support multimodal inputs like images and videos?\n\nGrok 4 supports multimodal inputs, but its capabilities and output are primarily text-based. While there are plans to enhance multimodal functionalities, including image and video understanding, these features are still in development."
          },
          {
            "snippetId": "model-data-snippet-5",
            "url": "https://walterpinem.com/grok-4-xai-api-python/",
            "snippet": "**Understanding Grok 4’s Core Architecture**\n\nI will walk you through the fundamental architecture of Grok 4, which represents a significant advancement in artificial intelligence through its sophisticated reinforcement learning implementation and native tool integration.\n\nThe model builds upon the foundation established by Grok 3, which achieved unprecedented performance through scaled next-token prediction pretraining, while introducing revolutionary reasoning capabilities that extend far beyond traditional language model limitations.... - Getting Started with Anthropic Claude API: A Practical Guide\n\n- Getting Started with kluster.ai for Scalable AI\n\n- 60+ Best Free API for Testing and Building Projects\n\nThis 200,000 GPU cluster enabled training runs that consumed over an order of magnitude more compute than previous iterations, resulting in smooth performance gains throughout the training process.... **Reinforcement Learning Implementation**\n\nThe reinforcement learning framework in Grok 4 operates through a sophisticated training methodology that refines reasoning abilities at unprecedented computational scales.\n\nThe training process incorporates innovations throughout the entire technology stack, including new infrastructure developments and algorithmic improvements that increased compute efficiency by 6x compared to previous implementations.... The model’s training data has been significantly expanded from primarily mathematical and coding domains to encompass numerous additional fields, creating a more comprehensive knowledge base for reasoning tasks.\n\nThis expansion required a massive data collection effort focused on verifiable training data, which ensures the model’s responses can be validated against known correct answers.\n\n**Native Tool Integration Architecture**... **Real-World Problem Solving Applications**\n\nFuture iterations will focus on expanding the model’s ability to handle complex real-world scenarios that require adaptive learning and dynamic problem-solving approaches.\n\nThis expansion will involve developing new training methodologies that can accommodate the unpredictable nature of real-world challenges while maintaining the model’s current level of performance and reliability.... The development of these capabilities will require continued innovation in reinforcement learning techniques, particularly in areas where traditional training approaches may not provide sufficient guidance for complex decision-making scenarios.\n\nThe team plans to explore new reward structures and training paradigms that can support more sophisticated real-world applications.\n\n**Performance Optimization and Efficiency Improvements**\n\nThe ongoing focus on making models smarter, faster, and more efficient will drive continued improvements in computational efficiency and response quality.... These improvements will build upon the 6x compute efficiency gains achieved in Grok 4’s development, seeking additional optimizations that can reduce resource requirements while maintaining or improving performance.... Grok 4 is already integrated to\n\n**AI vibe coding tools** such as Trae etc, so if you already use some of those, you can get started immediately, otherwise, keep reading.\n\nIn this tutorial, I will guide you through testing Grok 4 using the\n\n**xAI API with Python**, providing detailed steps and code examples to ensure clarity.... By following this guide, you can effectively interact with Grok 4, exploring its capabilities in a structured, programmatic manner.\n\n**Prerequisites for Using the xAI API**\n\nBefore proceeding, you must set up your environment to access the xAI API, which requires specific tools and credentials to ensure seamless integration.\n\nFirst, install Python 3.8 or higher, as the xAI SDK is compatible with modern Python versions.... Second, obtain an API key from the xAI Console at\n\n**console.x.ai**, which involves creating an account and generating a key under the API management section.\n\nFinally, install the xAI Python SDK using pip, which facilitates interaction with Grok 4’s endpoints.\n\n```\n\npip install xai-sdk\n\n```... Use the xAI SDK’s\n\n`Client` class to initialize a connection, specifying the\n\n`grok-4-0709` model or just\n\n`grok-4` for the latest stable release.\n\nSet parameters like\n\n`temperature=0.4` for controlled responses and\n\n`max_tokens=100` to limit output length, ensuring predictable results.... **Handling API Responses**\n\nWhen handling API responses, which Grok 4 returns in JSON format, you must parse the output to extract meaningful data, ensuring accurate interpretation of results.\n\nThe\n\n`response.content` field contains the generated text, while the\n\n`usage` object details token consumption, which is useful for monitoring rate limits.... Log these values in your application, as shown below, to monitor cumulative usage over multiple calls.\n\nThe xAI Console’s usage page provides an overview, but programmatic tracking, which I find precise, allows real-time adjustments. This code snippet logs token usage for analysis.... **Wrapping Up**\n\n**Wrapping Up**\n\nThroughout this tutorial, I have guided you through testing Grok 4 via the xAI API, demonstrating its capabilities in text generation, real-time data retrieval, and code execution, which showcase its technical versatility.\n\nBy following the structured steps, from setting up your environment to optimizing prompts and handling errors, you can effectively explore Grok 4’s potential, ensuring reliable and efficient interactions.... This process, which I find rewarding for its precision, equips you with the tools to integrate advanced AI into your projects.\n\nFor further exploration, consult the xAI documentation at\n\n`https://x.ai/api` to stay updated on API enhancements."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Training Dataset",
            "score": 1,
            "explanation": "Grok 4 is first pre-trained with a data recipe that includes publicly available Internet data, data produced by third-parties for xAI..."
          },
          {
            "name": "Training Data Processing",
            "score": 1,
            "explanation": "We perform data filtering procedures on the training data, such as de-duplication and classification, to ensure data quality and safety prior to training."
          },
          {
            "name": "Knowledge Count",
            "score": 1,
            "explanation": "The model’s training data has been significantly expanded from primarily mathematical and coding domains to encompass numerous additional fields..."
          }
        ]
      },
      {
        "sectionId": "model-implementation-sustainability",
        "sectionSnippets": [
          {
            "snippetId": "model-implementation-sustainability-snippet-1",
            "url": "https://docs.x.ai/docs/models/grok-4-0709",
            "snippet": "## Models\n\n## /\n## Grok 4\n\nOur latest and greatest flagship model, offering unparalleled performance in natural language, math and reasoning - the perfect jack of all trades.\n\n#### At a glance\n\nModalities\n\nContext window\n\n256,000\n\nPricing\n\n#### Capabilities\n\nFunction calling\n\nConnect the xAI model to external tools and systems.\n\nStructured outputs\n\nReturn responses in a specific, organized formats.\n\nReasoning\n\nThe model thinks before responding.... #### Pricing\n\n#### Input\n\nTokens\n\n/ 1M tokens\n\nCached tokens\n\n/ 1M tokens\n\n#### Output\n\nTokens\n\n/ 1M tokens\n\nLive search\n\n/ 1K sources\n\nYou are charged for each token used when making calls to our API.\n\nUsing cached input tokens can significantly reduce your costs.... #### Details\n\nModel name\n\nAliases\n\n|Region|us-east-1|\n|--|--|\n|Pricing per million tokens *|\n|Input|\n|Cached input|\n|Output|\n|Per thousand sources|\n|Live search|\n|Rate limits|\n|Requests per minute|480|\n|Tokens per minute|2,000,000|\nWe charge different rates for requests which exceed the 128K context window... #### Quickstart\n\n```\n\nfrom xai_sdk import Client\n\nfrom xai_sdk.chat import user, system\n\nclient = Client(api_key=\"<YOUR_XAI_API_KEY_HERE>\")\n\nchat = client.chat.create(model=\"grok-4-0709\", temperature=0)\n\nchat.append(system(\"You are a PhD-level mathematician.\"))\n\nchat.append(user(\"What is 2 + 2?\"))\n\nresponse = chat.sample()\n\nprint(response.content)\n\n```... #### Quickstart\n\nCreate an API Key and make your first request.\n\n#### Tool use\n\nLet Grok perform actions and look up information.\n\n#### Structured outputs\n\nLet Grok generate structured outputs."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-2",
            "url": "https://data.x.ai/2025-08-20-grok-4-model-card.pdf",
            "snippet": "xAI\nLast updated: August 20, 2025\n1\nIntroduction\nGrok 4 is the latest reasoning model from xAI with advanced reasoning and tool-use capabilities,\nenabling it to achieve new state-of-the-art performance across challenging academic and industry\nbenchmarks. Because our models push the frontier of AI capabilities, we are committed to mitigating... involve different model behaviors. For example, a hypothetical terrorist group using AI to help\nsynthesize chemical weapons would require models that possess advanced scientific knowledge,\nwhereas a hypothetical rogue AI exfiltrating its weights requires models that can manipulate humans\nand hack systems.\nOur approach to safety evaluations focuses on measuring specific safety-relevant behaviors relevant to... different risk scenarios. We categorize these safety-relevant behaviors as: abuse potential (Section 2.1),\nconcerning propensities (Section 2.2), and dual-use capabilities (Section 2.3). This report describes\nour current evaluation methodology, results, and mitigations for these various behaviors.... In this document, we focus on the Grok 4 model. xAI deploys Grok 4 in both the consumer-facing\napplications (Grok 4 Web) and through an enterprise use-focused API (Grok 4 API). We report\nevaluations for Grok 4 API and Grok 4 Web now available to our customers, including in the EU.... Finally, we describe our training pipeline (Section 3.1) and additional transparency commitments\n(Section 3.2).\n2\nEvaluations\nOur approach to model evaluations varies depending on the specific behavior under assessment.\nTo reduce the potential for abuse of Grok 4 that might lead to serious injury to people, property or... of enabling malicious use. While the general cyber knowledge and exploitation capabilities of Grok\n4 are a significant step up from prior models, third-party testing shows that Grok 4’s end-to-end\noffensive cyber capabilities remain below the level of a human professional.\nAs of this writing, on top of our safety training, we have implemented a system prompt that provides... additional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 overall presents a low\nrisk for malicious use and loss of control.\n2.1... evaluation metric is response rate (i.e., the rate at which the model answered queries that should\nhave been refused) for all three evaluation settings: standard, user jailbreak, and system jailbreak.\nGrok 4 Web does not accept custom system prompts from users, so we do not evaluate with\nsystem jailbreaks.... Input filters.\nWe also employ model-based filters for both Grok 4 API and Grok 4 Web,\nwhich reject classes of harmful requests, including biological and chemical weapons, self-harm, and\nCSAM.\n2.2\nConcerning Propensities\nAI models may contain propensities that reduce their controllability, such as deception, power-seeking,... 38.4% and 22.1% on VCT, so both Grok 4 API and Grok 4 Web achieve superhuman performance\non identifying issues in biological protocols and wetlab virology experiments. Similarly, Grok 4\nAPI shows strong capabilities in both cybersecurity and chemistry. Note that these evaluations... 3\nTransparency\nTo mitigate catastrophic risks from AI, we provide to the public visibility to the development and\ndeployment of our frontier AI models. Transparency into AI progress can help developers coordinate\nsafety efforts, governments enact sensible legislation, and the public stay abreast of the benefits and\nrisks of AI. In an effort to increase visibility, we document our training process (Section 3.1) and our... system prompts (Section 3.2).\n3.1\nData and Training\nGrok 4 is first pre-trained with a data recipe that includes publicly available Internet data, data\nproduced by third-parties for xAI, data from users or contractors, and internally generated data. We\nperform data filtering procedures on the training data, such as de-duplication and classification, to... ensure data quality and safety prior to training. In addition to pre-training, our recipe uses a variety\nof reinforcement learning techniques—human feedback, verifiable rewards, and model grading—along\nwith supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-\nprompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n7... Li, Ann-Kathrin Dombrowski, Shashwat Goel, Gabriel Mukobi, et al. The wmdp benchmark:\nMeasuring and reducing malicious use with unlearning. In International Conference on Machine\nLearning, pages 28525–28550. PMLR, 2024."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-3",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "## Available in These Regions\n\n**US East (Ashburn)**(on-demand only) **US Midwest (Chicago)**(on-demand only) **US West (Phoenix)**(on-demand only)\n\n**Important**\n\n**External Calls**\n\nThe xAI Grok models are hosted in an OCI data center, in a tenancy provisioned for xAI. The xAI Grok models, which can be accessed through the OCI Generative AI service, are managed by xAI.... ## Access this Model\n\n## Key Features\n\n🔗 **Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## On-Demand Mode\n\n\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\n\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... For large inputs with difficult problems, set a high value for the maximum output tokens parameter. See Troubleshooting.\n\n**Temperature**\n\n\n\nThe level of randomness used to generate the output text. Min: 0, Max: 2\n\n**Tip**\n\nStart with the temperature set to 0 or less than one, and increase the temperature as you regenerate the prompts for a more creative output. High temperatures can introduce hallucinations and factually incorrect information.... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response.... ## Troubleshooting\n\n\n\n**Issue:** The Grok 4 model doesn't respond.\n\n**Cause:** The **Maximum output tokens** parameter in the playground or the\n\n`max_tokens` parameter in the API is likely too low.\n\n**Action:** Increase the maximum output tokens parameter.\n\n**Reason:** For difficult problems that require reasoning and problem-solving, and for large sophisticated inputs, the\n\n`xai.grok-4` model tends to think and consumes many tokens, so if the\n\n`max_tokens` parameter is too low, the model uses the allocated tokens and doesn't return a final response."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-4",
            "url": "https://docs.public.content.oci.oraclecloud.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "# xAI Grok 4\n\nThe\n\n`xai.grok-4` model has better performance than its predecessor, Grok 3, and excels at enterprise use cases such as data extraction, coding, and summarizing text. This model has a deep domain knowledge in finance, healthcare, law, and science.... ## Available in These Regions\n\n**US East (Ashburn)**(on-demand only) **US Midwest (Chicago)**(on-demand only) **US West (Phoenix)**(on-demand only)\n\n**Important**\n\n**External Calls**\n\nThe xAI Grok models are hosted in an OCI data center, in a tenancy provisioned for xAI. The xAI Grok models, which can be accessed through the OCI Generative AI service, are managed by xAI.... ## Access this Model\n## Key Features\n\n**Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## On-Demand Mode\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... For large inputs with difficult problems, set a high value for the maximum output tokens parameter. See Troubleshooting.\n\n**Temperature**\n\n\n\nThe level of randomness used to generate the output text. Min: 0, Max: 2\n\n**Tip**\n\nStart with the temperature set to 0 or less than one, and increase the temperature as you regenerate the prompts for a more creative output. High temperatures can introduce hallucinations and factually incorrect information.... **Top p**\n\n\n\nA sampling method that controls the cumulative probability of the top tokens to consider for the next token. Assign\n\n`p`a decimal number between 0 and 1 for the probability. For example, enter 0.75 for the top 75 percent to be considered. Set\n\n`p`to 1 to consider all tokens.... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-5",
            "url": "https://www.datacamp.com/blog/grok-4",
            "snippet": "Course\n\nAfter we waited months for Grok 3.5, xAI skipped it altogether and jumped straight to Grok 4. (As of November 2025, Grok 4.1 has also launched).\n\nIs the jump justified by the model’s performance?\n\nYes, if you judge by the benchmarks. Right now, Grok 4 is probably the best model in the world on paper.... Source: xAI\n\nThe company claims it’s the most intelligent model available today, and benchmark results do point in that direction. The most notable result came from Humanity’s Last Exam, a benchmark made up of 2,500 hand-curated, PhD-level questions spanning math, physics, chemistry, linguistics, and engineering. Grok 4 (with tools) managed to solve about 38.6% of the problems.... It’s less clear how it performs in broader consumer workflows or how well it handles safety at scale. But xAI says it’s already in use at biomedical labs, financial firms, and early enterprise partners.... As Elon Musk pointed out in the livestream, Grok 4’s image understanding and generation still isn’t very advanced. If you want constant and reliable results, I think it’s fair to say that Grok 4 is a text-only model at the moment.... ## Grok 4 Benchmarks\n\nGrok 4’s main claim to fame is its performance across a wide mix of benchmarks, from academic exams to business simulations. According to xAI, the model improves significantly over previous versions thanks mostly to more compute—both during training and inference—not necessarily because of new architectural breakthroughs.... Source: xAI\n\nWith no tool use, Grok 4 plateaus at around 26.9% accuracy. With tools enabled (e.g., code execution), it hits 41.0%. And when run in its multi-agent “Heavy” configuration, it climbs to 50.7%—a major jump that’s over double the best prior tool-free model scores.... ### Vending-Bench (business simulation)\n\nxAI also tested Grok 4 in a real-world simulation called Vending-Bench. The idea is to see whether a model can manage a small business over time: restocking inventory, adjusting prices, contacting suppliers, etc. It’s a fairly new benchmark and a surprisingly fun one. We previously covered how it works in detail through a Claude Sonnet 3.7 case study in our weekly newsletter, The Median.... Source: xAI\n\nIn short: Grok 4 performs well where xAI has tested it. But as always, you should look beyond the leaderboard. The benchmarks are promising, but they don’t tell the whole story—especially if your use case depends on vision, code generation, or real-time interaction in messy environments.... ## How to Access Grok 4\n\nGrok 4 is now available through three main entry points: the X app, the xAI API, and the grok.com platform. Whether you want to chat with the model, build with it, or test its reasoning capabilities more formally, here’s how to get started.... ### Direct access via Grok.com\n\nYou can also use Grok 4 directly through grok.com, which offers a cleaner, standalone interface outside the X platform. It’s aimed at users who prefer a distraction-free setup.\n\n- Visit https://grok.com\n\n- Sign up and log in.\n\n- Access Grok 4 in a chat interface, with support for tools, code, and long context... ### Grok 4 API\n\nIf you want to integrate Grok into your own app or workflow, you can use the xAI API.\n\nSteps:\n\n- Go to https://x.ai/api and request developer access.\n\n- Once approved, you’ll receive an API key and access to docs.\n\n- Make sure to read through the documentation for more details.... ### August: Specialized coding model\n\nThe first follow-up is a coding-focused model expected in August. Unlike Grok 4, which is a generalist, this will be a specialized model designed to handle code with more speed and accuracy. xAI described it as “fast and smart,” trained specifically to improve both latency and reasoning in software development workflows.... ### October: Video generation model\n\nThe final release in the current timeline is a video generation model due in October. xAI says they’ll be training it on over 100,000 GPUs. Based on their remarks, this system will aim to produce high-quality, interactive, and editable video content.... For developers and researchers, it’s worth exploring. For casual users, the speed and responsiveness of Grok 3 or other mainstream models are a better fit. The roadmap is ambitious, with a coding model, multimodal agent, and video generator all due by October. Whether xAI can deliver those on time is another question. But with Grok 4, they’ve at least made a compelling case that they’re in the race."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Hardware Used During Training & Inference",
            "score": 1,
            "explanation": "Grok 4 improves significantly over previous versions thanks mostly to more compute—both during training and inference."
          },
          {
            "name": "Software Frameworks & Tooling",
            "score": 1,
            "explanation": "Grok 4 is accessible through the xAI API and supports function calling."
          },
          {
            "name": "Energy Use/ Sustainability Metrics",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      },
      {
        "sectionId": "intended-use",
        "sectionSnippets": [
          {
            "snippetId": "intended-use-snippet-1",
            "url": "https://docs.x.ai/docs/models/grok-4-0709",
            "snippet": "## Models\n\n## /\n## Grok 4\n\nOur latest and greatest flagship model, offering unparalleled performance in natural language, math and reasoning - the perfect jack of all trades.\n\n#### At a glance\n\nModalities\n\nContext window\n\n256,000\n\nPricing\n\n#### Capabilities\n\nFunction calling\n\nConnect the xAI model to external tools and systems.\n\nStructured outputs\n\nReturn responses in a specific, organized formats.\n\nReasoning\n\nThe model thinks before responding.... #### Pricing\n\n#### Input\n\nTokens\n\n/ 1M tokens\n\nCached tokens\n\n/ 1M tokens\n\n#### Output\n\nTokens\n\n/ 1M tokens\n\nLive search\n\n/ 1K sources\n\nYou are charged for each token used when making calls to our API.\n\nUsing cached input tokens can significantly reduce your costs.... #### Details\n\nModel name\n\nAliases\n\n|Region|us-east-1|\n|--|--|\n|Pricing per million tokens *|\n|Input|\n|Cached input|\n|Output|\n|Per thousand sources|\n|Live search|\n|Rate limits|\n|Requests per minute|480|\n|Tokens per minute|2,000,000|\nWe charge different rates for requests which exceed the 128K context window... #### Quickstart\n\n```\n\nfrom xai_sdk import Client\n\nfrom xai_sdk.chat import user, system\n\nclient = Client(api_key=\"<YOUR_XAI_API_KEY_HERE>\")\n\nchat = client.chat.create(model=\"grok-4-0709\", temperature=0)\n\nchat.append(system(\"You are a PhD-level mathematician.\"))\n\nchat.append(user(\"What is 2 + 2?\"))\n\nresponse = chat.sample()\n\nprint(response.content)\n\n```... #### Quickstart\n\nCreate an API Key and make your first request.\n\n#### Tool use\n\nLet Grok perform actions and look up information.\n\n#### Structured outputs\n\nLet Grok generate structured outputs."
          },
          {
            "snippetId": "intended-use-snippet-2",
            "url": "https://data.x.ai/2025-08-20-grok-4-model-card.pdf",
            "snippet": "xAI\nLast updated: August 20, 2025\n1\nIntroduction\nGrok 4 is the latest reasoning model from xAI with advanced reasoning and tool-use capabilities,\nenabling it to achieve new state-of-the-art performance across challenging academic and industry\nbenchmarks. Because our models push the frontier of AI capabilities, we are committed to mitigating... different risk scenarios. We categorize these safety-relevant behaviors as: abuse potential (Section 2.1),\nconcerning propensities (Section 2.2), and dual-use capabilities (Section 2.3). This report describes\nour current evaluation methodology, results, and mitigations for these various behaviors.... In this document, we focus on the Grok 4 model. xAI deploys Grok 4 in both the consumer-facing\napplications (Grok 4 Web) and through an enterprise use-focused API (Grok 4 API). We report\nevaluations for Grok 4 API and Grok 4 Web now available to our customers, including in the EU.... Finally, we describe our training pipeline (Section 3.1) and additional transparency commitments\n(Section 3.2).\n2\nEvaluations\nOur approach to model evaluations varies depending on the specific behavior under assessment.\nTo reduce the potential for abuse of Grok 4 that might lead to serious injury to people, property or... of enabling malicious use. While the general cyber knowledge and exploitation capabilities of Grok\n4 are a significant step up from prior models, third-party testing shows that Grok 4’s end-to-end\noffensive cyber capabilities remain below the level of a human professional.\nAs of this writing, on top of our safety training, we have implemented a system prompt that provides... additional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 overall presents a low\nrisk for malicious use and loss of control.\n2.1... Agentic abuse.\nGrok 4 introduces advanced reasoning and tool-calling capabilities that enable\nthe model to be used in an “agentic” manner, that is, repeatedly take actions toward a specified\ngoal. Such capabilities introduce additional risks of misuse beyond what is present in conversational\nsettings, such as executing real function calls. To quantify these risks, we use the AgentHarm\n2... Input filters.\nWe also employ model-based filters for both Grok 4 API and Grok 4 Web,\nwhich reject classes of harmful requests, including biological and chemical weapons, self-harm, and\nCSAM.\n2.2\nConcerning Propensities\nAI models may contain propensities that reduce their controllability, such as deception, power-seeking,... acquire, or use chemical and biological weapons or offensive cyber operations (e.g., troubleshooting\nvirology lab or reverse engineering binaries). We also measure the persuasiveness of our models\nwhen instructed to surreptitiously persuade another AI model, since more persuasive models can be\nabused to manipulate people at scale, and manipulate user behavior.... and chemistry, the cybersecurity questions in WMDP also measure different stages of a threat model,\nin particular the cyber killchain [Hutchins et al., 2011]. The questions span a variety of topics,\n5... such as knowledge of Metasploit, to understanding of vulnerability detection, to reverse engineering\nsimple binaries.\nCyber agents.\nWe also evaluate Grok 4’s agentic hacking capabilities on CyBench, a collection\nof 40 capture-the-flag-style questions which measures a model’s ability to perform cybersecurity\nchallenges [Zhang et al., 2025]. The model is placed in an agent harness which gives it access to... 38.4% and 22.1% on VCT, so both Grok 4 API and Grok 4 Web achieve superhuman performance\non identifying issues in biological protocols and wetlab virology experiments. Similarly, Grok 4\nAPI shows strong capabilities in both cybersecurity and chemistry. Note that these evaluations... system prompts (Section 3.2).\n3.1\nData and Training\nGrok 4 is first pre-trained with a data recipe that includes publicly available Internet data, data\nproduced by third-parties for xAI, data from users or contractors, and internally generated data. We\nperform data filtering procedures on the training data, such as de-duplication and classification, to... ensure data quality and safety prior to training. In addition to pre-training, our recipe uses a variety\nof reinforcement learning techniques—human feedback, verifiable rewards, and model grading—along\nwith supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-\nprompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n7"
          },
          {
            "snippetId": "intended-use-snippet-3",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "# xAI Grok 4\n\nThe\n\n`xai.grok-4` model has better performance than its predecessor, Grok 3, and excels at enterprise use cases such as data extraction, coding, and summarizing text. This model has a deep domain knowledge in finance, healthcare, law, and science.... ## Available in These Regions\n\n**US East (Ashburn)**(on-demand only) **US Midwest (Chicago)**(on-demand only) **US West (Phoenix)**(on-demand only)\n\n**Important**\n\n**External Calls**\n\nThe xAI Grok models are hosted in an OCI data center, in a tenancy provisioned for xAI. The xAI Grok models, which can be accessed through the OCI Generative AI service, are managed by xAI.... ## Access this Model\n\n## Key Features\n\n🔗 **Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## Limits\n\n\n\n- Image Inputs\n\n\n\n**Console:**Upload one or more\n\n`.png`or\n\n`.jpg`images, each 5 MB or smaller.\n\n**API:**Submit a\n\n`base64`encoded version of an image, ensuring that each converted image is more than 512 and less than 1,792 tokens. For example, a 512 x 512 image typically converts to around 1,610 tokens.... ## On-Demand Mode\n\n\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\n\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... **Top p**\n\n\n\nA sampling method that controls the cumulative probability of the top tokens to consider for the next token. Assign\n\n`p`a decimal number between 0 and 1 for the probability. For example, enter 0.75 for the top 75 percent to be considered. Set\n\n`p`to 1 to consider all tokens.... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response."
          },
          {
            "snippetId": "intended-use-snippet-4",
            "url": "https://docs.public.content.oci.oraclecloud.com/en-us/iaas/Content/generative-ai/xai-grok-4.htm",
            "snippet": "# xAI Grok 4\n\nThe\n\n`xai.grok-4` model has better performance than its predecessor, Grok 3, and excels at enterprise use cases such as data extraction, coding, and summarizing text. This model has a deep domain knowledge in finance, healthcare, law, and science.... ## Available in These Regions\n\n**US East (Ashburn)**(on-demand only) **US Midwest (Chicago)**(on-demand only) **US West (Phoenix)**(on-demand only)\n\n**Important**\n\n**External Calls**\n\nThe xAI Grok models are hosted in an OCI data center, in a tenancy provisioned for xAI. The xAI Grok models, which can be accessed through the OCI Generative AI service, are managed by xAI.... ## Access this Model\n## Key Features\n\n**Model name in OCI Generative AI:**\n\n`xai.grok-4`... **Available On-Demand:**Access this model on-demand, through the Console playground or the API. **Multimodal support:**Input text and images and get a text output. **Knowledge:**Has a deep domain knowledge in finance, healthcare, law, and science. **Context Length:**128,000 tokens (maximum prompt + response length is 128,000 tokens for keeping the context).... In the playground, the response length is capped at 16,000 tokens for each run, but the context remains 128,000 tokens. **Excels at These Use Cases:**Data extraction, coding, and summarizing text **Function Calling:**Yes, through the API. **Structured Outputs:**Yes. **Has Reasoning:**Yes. For reasoning problems increase the maximum output tokens. See Model Parameters.... **Cached Input Tokens:**Yes **Token count:**See the\n\n`cachedTokens`attribute in the PromptTokensDetails Reference API.\n\n**Pricing:**See the Pricing Page. **Important note:**The cached‑input feature is available in both the playground and the API. However, that information can only be retrieved through the API.\n\n\n\n**Knowledge Cutoff:**November 2024... ## On-Demand Mode\n\n**Note**\n\nThe Grok models are available only in the\n\n*on-demand*mode.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|xAI Grok 4|`xai.grok-4`|xAI – Grok 4 Prices are listed for:|\n*on-demand*and... *dedicated*. Here are key features for the\n\n*on-demand*mode:\n\n\n\nYou pay as you go for each inference call when you use the models in the playground or when you call the models through the API.\n\n- Low barrier to start using Generative AI.\n\n- Great for experimentation, proof of concept, and model evaluation.\n\n- Available for the pretrained models in regions not listed as (dedicated AI cluster only).... ## Release Date\n\n|Model|General Availability Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`xai.grok-4`|2025-07-23|Tentative|This model isn't available for the dedicated mode.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. The maximum prompt + output length is 128,000 tokens for each run. **Tip**... **Note**\n\nThe\n\n`xai.grok-4` model has reasoning, but doesn't support the\n\n`reasoning_effort` parameter used in the Grok 3 mini and Grok 3 mini fast models. If you specify the\n\n`reasoning_effort` parameter in the API for the\n\n`xai.grok-4` model, you get an error response.... ## Troubleshooting\n\n**Issue:** The Grok 4 model doesn't respond.\n\n**Cause:** The **Maximum output tokens** parameter in the playground or the\n\n`max_tokens` parameter in the API is likely too low.\n\n**Action:** Increase the maximum output tokens parameter.\n\n**Reason:** For difficult problems that require reasoning and problem-solving, and for large sophisticated inputs, the\n\n`xai.grok-4` model tends to think and consumes many tokens, so if the\n\n`max_tokens` parameter is too low, the model uses the allocated tokens and doesn't return a final response."
          },
          {
            "snippetId": "intended-use-snippet-5",
            "url": "https://docs.aimlapi.com/api-references/text-models-llm/xai/grok-4",
            "snippet": "This documentation is valid for the following model:\n\n`x-ai/grok-4-07-09`\n\n## Model Overview\n\nGrok 4 is boldly described by its developers as the most intelligent model in the world (as of July 2025).\n\n## How to Make a Call... ## Step-by-Step Instructions\n\n1️\n\n** Setup You Can’t Skip**\n\n▪️\n\n**Create an Account**: Visit the AI/ML API website and create an account (if you don’t have one yet).\n\n▪️ **Generate an API Key**: After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.... 2️\n\n** Copy the code example**\n\nAt the bottom of this page, you'll find a code example that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.\n\n3️\n\n** Modify the code example**\n\n▪️ Replace\n\n`<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.... ▪️ Insert your question or request into the\n\n`content` field—this is what the model will respond to.\n\n4️\n\n**(Optional)** ** Adjust other optional parameters if needed**\n\nOnly\n\n`model` and\n\n`messages` are required parameters for this model (and we’ve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model’s behavior. Below, you can find the corresponding API schema, which lists all available parameters along with notes on how to use them.... 5️\n\n** Run your modified code**\n\nRun your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.\n\nIf you need a more detailed walkthrough for setting up your development environment and making a request step by step — feel free to use our Quickstart guide.... ## API Schema\n\nAn upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.max_tokensnumber · min: 1Optional\n\nThe maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.streambooleanOptional... A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition.top_anumber · max: 1Optional\n\nAlternate top sampling parameter.tool_choiceany ofOptional... Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool. none is the default when no tools are present. auto is the default if tools are present.string · enumOptional... none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools.Possible values: orparallel_tool_callsbooleanOptional\n\nWhether to enable parallel function calling during tool use.logprobsboolean | nullableOptional... \"end_index\": 1,\n\n\"start_index\": 1,\n\n\"title\": \"text\",\n\n\"url\": \"text\"\n\n\n\n],\n\n\"audio\": {\n\n\"id\": \"text\",\n\n\"data\": \"text\",\n\n\"transcript\": \"text\",\n\n\"expires_at\": 1\n\n},\n\n\"tool_calls\": [\n\n\n\n\"id\": \"text\",... 1\n\n],\n\n\"logprob\": 1,\n\n\"token\": \"text\"\n\n\n\n],\n\n\"refusal\": []\n\n\n\n],\n\n\"model\": \"text\",\n\n\"usage\": {\n\n\"prompt_tokens\": 1,\n\n\"completion_tokens\": 1,\n\n\"total_tokens\": 1,\n\n\"completion_tokens_details\": {\n\n\"accepted_prediction_tokens\": 1,\n\n\"audio_tokens\": 1,\n\n\"reasoning_tokens\": 1,\n\n\"rejected_prediction_tokens\": 1\n\n},\n\n\"prompt_tokens_details\": {\n\n\"audio_tokens\": 1,\n\n\"cached_tokens\": 1\n\n\n\n```... \"logprobs\": null,\n\n\"message\": {\n\n\"role\": \"assistant\",\n\n\"content\": \"Hello! I'm Grok, built by xAI to help with answers, ideas, and a bit of cosmic wit. What can I do for you today? 🚀\",\n\n\"reasoning_content\": \"Thinking... Thinking...... \",\n\n\"refusal\": null\n\n\n\n],\n\n\"created\": 1752837143,\n\n\"model\": \"x-ai/grok-4\",\n\n\"usage\": {\n\n\"prompt_tokens\": 53,\n\n\"completion_tokens\": 5689,\n\n\"total_tokens\": 5742,\n\n\"prompt_tokens_details\": {\n\n\"cached_tokens\": 2\n\n},\n\n\"completion_tokens_details\": {\n\n\"reasoning_tokens\": 138\n\n\n\n```\n\nLast updated\n\nWas this helpful?"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Primary intended uses",
            "score": 1,
            "explanation": "Grok 4 excels at enterprise use cases such as data extraction, coding, and summarizing text."
          },
          {
            "name": "Primary intended users",
            "score": 1,
            "explanation": "Grok 4 is deployed in both the consumer-facing applications and through an enterprise use-focused API."
          },
          {
            "name": "Out-of-scope use cases",
            "score": 1,
            "explanation": "We categorize these safety-relevant behaviors as: abuse potential, concerning propensities, and dual-use capabilities."
          }
        ]
      },
      {
        "sectionId": "critical-risk",
        "sectionSnippets": [
          {
            "snippetId": "critical-risk-snippet-1",
            "url": "https://data.x.ai/2025-08-20-grok-4-model-card.pdf",
            "snippet": "their risks through both evaluating model behaviors and implementing safeguards.\nFollowing our Risk Management Framework (RMF), we aim to reduce the risk of severe, large-scale\nharms to people, property, and society from AI. The two primary categories of risk we consider are\nrisks from either malicious use or loss of control. Different risk scenarios within these categories... involve different model behaviors. For example, a hypothetical terrorist group using AI to help\nsynthesize chemical weapons would require models that possess advanced scientific knowledge,\nwhereas a hypothetical rogue AI exfiltrating its weights requires models that can manipulate humans\nand hack systems.\nOur approach to safety evaluations focuses on measuring specific safety-relevant behaviors relevant to... different risk scenarios. We categorize these safety-relevant behaviors as: abuse potential (Section 2.1),\nconcerning propensities (Section 2.2), and dual-use capabilities (Section 2.3). This report describes\nour current evaluation methodology, results, and mitigations for these various behaviors.... Finally, we describe our training pipeline (Section 3.1) and additional transparency commitments\n(Section 3.2).\n2\nEvaluations\nOur approach to model evaluations varies depending on the specific behavior under assessment.\nTo reduce the potential for abuse of Grok 4 that might lead to serious injury to people, property or... national security interests, we take measures to improve Grok 4’s robustness, such as by adding\nsafeguards to refuse requests that may lead to foreseeable harm, especially for requests that lower\nthe barriers to developing chemical, biological, radiological, nuclear (CBRN) or cyber weapons, along\nwith requests for self-harm and child sexual abuse material (CSAM) (Section 2.1). In addition to\nrefusals, we also assess Grok 4’s robustness to adversarial requests which attempt to circumvent\nour safeguards (e.g., jailbreaks and prompt injections).\n1... of enabling malicious use. While the general cyber knowledge and exploitation capabilities of Grok\n4 are a significant step up from prior models, third-party testing shows that Grok 4’s end-to-end\noffensive cyber capabilities remain below the level of a human professional.\nAs of this writing, on top of our safety training, we have implemented a system prompt that provides... additional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 overall presents a low\nrisk for malicious use and loss of control.\n2.1... malignant intent from mere curiosity. We define a basic refusal policy which instructs Grok 4 to\ndecline queries demonstrating clear intent to engage in activities that threaten severe, imminent harm\nto others, including violent crimes, child sexual exploitation, fraud, hacking, and more. We place\nfurther emphasis on refusing requests concerning the development of CBRN or cyber weapons.... Input filters.\nWe also employ model-based filters for both Grok 4 API and Grok 4 Web,\nwhich reject classes of harmful requests, including biological and chemical weapons, self-harm, and\nCSAM.\n2.2\nConcerning Propensities\nAI models may contain propensities that reduce their controllability, such as deception, power-seeking,... such as knowledge of Metasploit, to understanding of vulnerability detection, to reverse engineering\nsimple binaries.\nCyber agents.\nWe also evaluate Grok 4’s agentic hacking capabilities on CyBench, a collection\nof 40 capture-the-flag-style questions which measures a model’s ability to perform cybersecurity\nchallenges [Zhang et al., 2025]. The model is placed in an agent harness which gives it access to... 38.4% and 22.1% on VCT, so both Grok 4 API and Grok 4 Web achieve superhuman performance\non identifying issues in biological protocols and wetlab virology experiments. Similarly, Grok 4\nAPI shows strong capabilities in both cybersecurity and chemistry. Note that these evaluations... -\nCyBench\nunguided success rate\n0.43\n-\nTable 3: Dual-use capabilities evaluations.\n2.3.3\nMitigations\nDue to Grok 4’s strong dual-use biological capabilities, we have deployed narrow, topically-focused\nfilters across all product surfaces as an additional safeguard against bioweapons-related abuse.... ensure data quality and safety prior to training. In addition to pre-training, our recipe uses a variety\nof reinforcement learning techniques—human feedback, verifiable rewards, and model grading—along\nwith supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-\nprompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n7"
          },
          {
            "snippetId": "critical-risk-snippet-2",
            "url": "https://data.x.ai/2025-09-19-grok-4-fast-model-card.pdf",
            "snippet": "Grok 4 Fast was pre-trained on a general purpose data corpus, then post-trained on various tasks\nand tool use, as well as demonstrations of correct refusal behaviors according to our default safety\npolicy. We also deploy Grok 4 Fast in our API with a fixed system prompt prefix that reminds\nthe model of our safety policy, in addition to input filters to safeguard against abuse.... Prior to release, we have evaluated various specific safety-relevant behaviors of Grok 4 Fast: abuse\npotential (Section 2.1), concerning propensities (Section 2.2), and dual-use capabilities (Section 2.3).\nIn this report, we describe our current evaluation methodology, results, and any mitigations for these... various behaviors in Grok 4 Fast. All evaluations in this report were conducted on a near-final\nrelease checkpoint.\n2\nEvaluations\nOur approach to model evaluations varies depending on the specific behavior under assessment.\nTo reduce the potential for abuse of Grok 4 Fast that might lead to serious injury to people,... property or national security interests, we apply safety training to reduce the risks of misuse and\nrefuse requests that may lead to foreseeable harm, especially for requests that lower the barriers to\ndeveloping chemical, biological, radiological, nuclear (CBRN) or cyber weapons, along with requests\nfor self-harm and child sexual abuse material (CSAM) (Section 2.1). In addition to refusals, we... As of this writing, on top of our safety training, we have implemented a system prompt that provides\nadditional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 Fast overall presents... • Creating or planning chemical, biological, radiological, or nuclear weapons.\n• Conducting cyber attacks, including ransomware and DDoS attacks.\nWe instruct the model not to answer queries that demonstrate clear intent to engage in these activities\nwithin a safety system prompt that is injected before all conversational contexts. Users may specify\ntheir own system message, and its content will be appended to the safety system prompt. Users may... imminent harm to others, including violent crimes, child sexual exploitation, fraud, hacking, and\nmore. We place further emphasis on refusing requests concerning the development of CBRN or cyber\nweapons.\nSystem Prompt.\nWith Grok 4 Fast’s strong reasoning and instruction-following capabilities,\nwe find that including our basic refusal policy in the system prompt greatly reduces response rate on... harmful queries. Additionally, warning the model against jailbreak attacks serves to significantly\ninoculate against common jailbreak strategies.\nInput filters.\nWe also employ model-based filters for Grok 4 Fast, which reject classes of harmful\nrequests, including biological and chemical weapons, self-harm, and CSAM.\n2.2... to model truthfulness, we recommend developers operate Grok 4 Fast with reasoning enabled and\ninclude instructions to respond truthfully.\n2.3\nDual-use Capabilities\nIn this section, we evaluate the possibility of our model enabling malicious actors to design, synthesize,\nacquire, or use chemical and biological weapons or offensive cyber operations (e.g., troubleshooting\nvirology lab or reverse engineering binaries). We also measure the persuasiveness of our models\n4... for bioweapons, but also covers cybersecurity and chemical knowledge. We prioritize addressing\nbioweapons risks over others because they have the potential for the greatest scale of harm, and\nfrontier models significantly lower the barrier to entry to the creation of bioweapons [Brent and\nMcKelvey, 2025]. For all datasets, we only assess performance on text-only questions.... in particular the cyber killchain [Hutchins et al., 2011]. The questions span a variety of topics,\nsuch as knowledge of Metasploit, to understanding of vulnerability detection, to reverse engineering\nsimple binaries.\nCyber agents.\nWe also evaluate Grok 4 Fast’s agentic hacking capabilities on CyBench,... Table 3: Dual-use capabilities evaluations.\n2.3.3\nMitigations\nOur narrow, topically-focused filters remain deployed across all product surfaces as an additional\nsafeguard against chemical and biological weapons-related abuse. Our assessments of autonomous\nhacking, radiological, and nuclear abuse risks remain unchanged from that of Grok 4.... to ensure data quality and safety prior to training. In addition to pre-training, our recipe uses\na variety of reinforcement learning techniques—human feedback, verifiable rewards, and model\ngrading—along with supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-... prompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n6"
          },
          {
            "snippetId": "critical-risk-snippet-3",
            "url": "https://aisafetyclaims.org/companies/xai",
            "snippet": "# xAI\n\n## Eval and safeguard planning\n# Grok 4\n\nReleased Jul 9, 2025\n\nModel card published Aug 20, 2025 (and quietly changed later)\n\nTraining and internal deployment: dates not reported... xAI said \"Grok 4 is the most intelligent model in the world\"; moreover, \"Grok 4 represents a leap in frontier intelligence.\" Initially, it didn't say anything about safety and its misuse safeguards were absurdly weak. A month later, it published stuff on safety. It said Grok 4 has dangerous chem/bio capabilities and safeguards are necessary.... It also said Grok 4 doesn't pose large misalignment risks; this happens to be true but xAI's thinking on misalignment is profoundly confused and the metric it uses has nothing to do with misalignment risk.Elicitation checklist:❌ ❌ ❌ ❌Accountability checklist:❌ ❌... ## Grok 4 evaluation categories\n\nClick on a category to see my analysis and the relevant part of the company's eval report.\n\n### Chem & bio\n\nxAI does some simple evals and says Grok 4 has dangerous capabilities.\n\n### Cyber\n\nxAI does some simple evals and says Grok 4 lacks dangerous capabilities.... ### AI R&D\n\nNone\n\n### Scheming capabilities\n\nNone\n\n### Misalignment propensity\n\nxAI does evals to measure (1) propensity to lie when pressured to do so by the system prompt or user and (2) sycophancy. These aren't really relevant to misalignment risk.... ## Grok 4 safety categories\n\nThis section is new and experimental. Click on a category to see my analysis. Preventing misuse via API (i.e., when the user doesn't control the model weights) is relatively straightforward, especially for bio; scheming risk prevention will eventually be important and improving it from current levels is tractable; security will eventually be important and we basically know what to do but it's very costly. I'm interested in feedback, especially on whether this content is helpful to you or what questions you wish I answered here instead.... ### Misuse (via API) prevention\n\nxAI's plan is that if models have dangerous capabilities, it will ensure that the \"answer rate [is] less than 1 out of 20 on restricted queries.\" It claims to be meeting this standard for chem/bio weapons development. Unfortunately, it doesn't say anything about red-teaming its mitigations and they seem unlikely to be effective in the real world.... ### Scheming risk prevention\n\n\"Our risk acceptance criteria for system deployment is maintaining a dishonesty rate of less than 1 out of 2 on MASK. We plan to add additional thresholds tied to other benchmarks.\" Unfortunately, MASK has almost nothing to do with catastrophic misalignment risk, and upfront benchmarking is not a good approach to misalignment risk.\n### Security\n\nxAI claims to have good security. This is not credible."
          },
          {
            "snippetId": "critical-risk-snippet-4",
            "url": "https://splx.ai/blog/grok-4-security-testing",
            "snippet": "On\n\n**July 9th 2025,** xAI released Grok 4 as its new flagship language model. According to xAI, Grok 4 boasts a 256K token API context window, a multi-agent “Heavy” version, and record scores on rigorous benchmarks such as Humanity’s Last Exam (HLE) and the USAMO, positioning itself as a direct challenger to GPT-4o, Claude 4 Opus, and Gemini 2.5 Pro. **So, the SplxAI Research Team put Grok 4 to the test against GPT-4o**.... Grok 4’s recent antisemitic meltdown on X shows why every organization that embeds a large-language model (LLM) needs a standing red-team program. These models should never be used without rigorous evaluation of their safety and misuse risks—that's precisely what our research aims to demonstrate.... ## Grok Achievements\n\nBelow is a SplxAI-style introduction that combines the model’s headline achievements with a clear look at subscription and API pricing.... ## Testing Methodology\n\nThe following section explains how we conducted the security assessment. We began by crafting a system prompt tailored for an AI chatbot operating in the finance domain. Next, we applied our proprietary Prompt Hardening tool to enhance the prompt’s robustness against a wide range of attack types. In the final step, we used Probe to execute more than 1,000 distinct attack scenarios across various categories.... ||||\n|--|--|--|\n|Level 1 – No System Prompt|Raw model, “blank slate,” user messages only|Hobbyist scripts, mis-configured dev keys|\n|Level 2 – Basic System Prompt|A concise guard-rail similar to what the average SaaS team deploys|Standard production chatbot|\n|Level 3 – Hardened System Prompt|System prompt auto-strengthened by SplxAI’s|Best-practice enterprise deployment|\nThe Prompt-Hardening tool iterates over probe failures, rewrites the system prompt and retests until the model resists the original attacks.... ## Results\n\nWith no system prompt in place, Grok 4 underperforms in every category. The first thing we found is that Grok without a system prompt is not suitable for enterprise usage, it was really easy to jailbreak and it was generating harmful content with very descriptive and detailed responses.... |Grok 4 – Hardened SP|93.60|100.00|98.20|... When we ran our very first baseline test with both models left entirely unsupervised, the visual gap was impossible to miss. The orange columns towering over the barely perceptible yellow ones show that GPT-4o, while far from perfect, keeps a basic grip on security- and safety-critical behavior, whereas Grok 4 all but collapses. In practice, this means a simple, single-sentence user message can pull Grok into disallowed territory with no resistance at all – a serious concern for any enterprise that must answer to compliance teams, regulators, and customers.... All SplxAI enterprise clients get full access to Grok 4 and other model red-teaming reports – included in the license. Want to see how your model stacks up?... ## Key Takeaways\n\n*Raw Grok is a security liability.*With no system prompt, it leaked restricted data and obeyed hostile instructions in over 99% of our injection attempts. (SplxAI internal test data) *Basic guardrails fix most “easy” failures but still leave a double-digit business-alignment gap.* *Prompt hardening closes the gap,*lifting business-alignment to a 98% pass-rate and slightly improving security.... ## Pre-Deployment LLM Security Checklist\n\n**For CISOs, Red Team Leads & Product Owners**\n\nBefore shipping your AI assistant, ask:\n\n#### 1. Has the raw model been red-teamed?\n\nRun >1,000 adversarial probes across jailbreak, bias, safety, hallucination, and business alignment vectors.... #### 2. Is your system prompt hardened?\n\nHas it been iteratively stress-tested and reinforced using prior failures (canary traps, negation layering, goal obfuscation, etc.)?\n\n#### 3. Have you validated across modalities?\n\nTested for multi-turn, context-based, and tool-augmented exploits – not just one-shot prompts.... #### 4. Are business-alignment KPIs tracked?\n\nMeasuring more than hallucination or PII leakage – does the model respond in-brand, on-policy, and within intended scope?\n\n#### 5. Do you have regression coverage?\n\nWill you catch it if a model update reintroduces unsafe behavior (e.g., what happened with Grok)?... #### 6. Are results mapped to frameworks?\n\nOWASP LLM Top 10, MITRE ATLAS, NIST AI RMF – alignment with these should be explicit, not assumed.\n\n#### 7. Is your deployment auditable?\n\nCan your security and compliance teams inspect prompts, defenses, and test coverage without relying on devs?\n\nTable of contents"
          },
          {
            "snippetId": "critical-risk-snippet-5",
            "url": "https://www.scribd.com/document/844599804/2025-02-20-RMF-Draft",
            "snippet": "TPurpose and Scope his draft framework outlines xAI’s approach to policies for managing significant risksTassociated with the development, deployment, and release of our future AI systems not currently in development, such as future versions of Grok. (For simplicity, we refer to all such AF future systems as “Grok” below.)... This draft framework addresses two major categories of AI risk—malicious use and loss of control—and outlines the quantitative thresholds, metrics, and procedures that could be used to manage and improve the safety of AI systems. In addition, this draft framework discusses potential ways to address operational and societal risks posed by advanced AI, such as with public transparency, third-party review, and information security.... nder this draft risk management framework, Grok would apply heightened safeguards if itUreceives requests that pose a foreseeable and non-trivial risk of resulting in large-scale violence, terrorism, or the use, development, or proliferation of weapons of mass destruction, including CBRN weapons, and major cyber weapons on critical infrastructure.... For example, Grok would apply heightened safeguards if it receives a request to act as an agent or tool of mass violence, or if it receives requests for step-by-step instructions for committing mass violence. In this draft framework, we particularly focus on requests that pose a foreseeable and non-trivial risk of more than one hundred deaths or over $1 billion in damages from weapons of mass destruction or cyberterrorist attacks on critical infrastructure (“catastrophic malicious use events”). However, we will allow Grok to respond to such requests from some vetted, highly trusted users (such as trusted third-party safety auditors) whom we know to be using those... 1.Approach to Benchmarking o transparently measure Grok’s safety properties, we intend to utilize benchmarks like WMDTand Catastrophic Harm Benchmarks. Such benchmarks could be used to measure Grok’s dual-use capability and resistance to facilitating large-scale violence, terrorism, or the use, development, or proliferation of weapons of mass destruction (including chemical, biological, radiological, nuclear, and major cyber weapons).... e intend to regularly evaluate the adequacy and reliability of such benchmarks, including byWcomparing them against other benchmarks that we could potentially utilize.\n\n2.Implementation e want Grok to comply with its guiding principles, robustly resisting attempted manipulationWand adversarial attacks. We train Grok to robustly refuse to comply with requests to provide assistance with highly injurious malicious use.... T xamples of safeguards or mitigations we may potentially utilize to achieve our safety objectivesEinclude:\n\n● C ircuit breakers: Using representation engineeringto interrupt model representations responsible for hazardous outputs.5 AF ● Input and output filters:Applying classifiers touser inputs or model outputs to verify safety when Grok is queried regarding weapons of mass destruction or cyberterrorism.... e intend to design into Grok adequate safeguards prior to releasing it for general availability.WWe intend to choose the benchmarks and set the thresholds for reducing malicious use risks in a future version of the risk management framework.\n\nmultiple-response)\n\nMDP-Bio: multiple-choice W % X 82.2% by GPT-4 (zero-shot) questions on proxies of hazardous (e.g. 65%) biosecurity knowledge... 4 Cybench: A Framework for Evaluating CybersecurityCapabilities and Risks of Language Models5 Improving Alignment and Robustness with Circuit Breakers\n\n3 AB-Bench6 ProtocolQA: L % X 9% by human experts 7 multiple-choice questions on (e.g. 50%) (multiple-choice) debugging modified biological lab protocols... s an additional measure to enhance safety, we will subject Grok to adversarially testing itsAsafeguards utilizing both internal and qualified external red teams. Potentially, we will also explore incentive mechanisms like bounties as another mechanism to further improve Grok’s R safeguards.\n\n1.Background ur aim is to design safeguards into Grok to avoid losing control and thereby avoid unintendedOcatastrophic outcomes when Grok is used. Currently, it is recognized that some properties of an AI system that may reduce controllability include deception, power-seeking, fitness... e intend to regularly evaluate the adequacy and reliability of such benchmarks for bothWinternal and external deployments, including by comparing them against other benchmarks that we could potentially utilize. We may revise this list of benchmarks periodically as relevant benchmarks for loss of control are created.\n\n3.Thresholds R e aim to train Grok to be honest and have values conducive to controllability. We intend toWdesign into Grok adequate safeguards prior to broad internal or external deployment.... 7 6.Deployment Decisions o mitigate risks, we intend to utilize tiered availability of the functionality and features of Grok.TFor instance, the full functionality of a future Grok could be made available only to trusted parties, partners, and government agencies. We could also mitigate risks by adding additional controls on functionality and features depending on the end user (e.g., consumers using mobile apps vs. sophisticated businesses using APIs)."
          }
        ],
        "subsectionChecks": [
          {
            "name": "CBRN (Chemical, Biological, Radiological or Nuclear)",
            "score": 1,
            "explanation": "We place further emphasis on refusing requests concerning the development of CBRN or cyber weapons."
          },
          {
            "name": "Cyber Risk",
            "score": 1,
            "explanation": "Conducting cyber attacks, including ransomware and DDoS attacks."
          },
          {
            "name": "Harmful Manipulation",
            "score": 1,
            "explanation": "We aim to reduce the risk of severe, large-scale harms to people, property, and society from AI."
          },
          {
            "name": "Child Safety Evaluations",
            "score": 1,
            "explanation": "Refuse requests that may lead to foreseeable harm, especially for requests for self-harm and child sexual abuse material (CSAM)."
          },
          {
            "name": "Privacy Risks",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      },
      {
        "sectionId": "safety-evaluation",
        "sectionSnippets": [
          {
            "snippetId": "safety-evaluation-snippet-1",
            "url": "https://data.x.ai/2025-08-20-grok-4-model-card.pdf",
            "snippet": "involve different model behaviors. For example, a hypothetical terrorist group using AI to help\nsynthesize chemical weapons would require models that possess advanced scientific knowledge,\nwhereas a hypothetical rogue AI exfiltrating its weights requires models that can manipulate humans\nand hack systems.\nOur approach to safety evaluations focuses on measuring specific safety-relevant behaviors relevant to... different risk scenarios. We categorize these safety-relevant behaviors as: abuse potential (Section 2.1),\nconcerning propensities (Section 2.2), and dual-use capabilities (Section 2.3). This report describes\nour current evaluation methodology, results, and mitigations for these various behaviors.... We also reduce various propensities of Grok 4 that might make it difficult to control, such as\nbeing deceptive, power-seeking, manipulative, or biased, among others (Section 2.2). To achieve this,\nour main focus is on measuring and reducing the rate at which Grok 4 responds deceptively. We... of enabling malicious use. While the general cyber knowledge and exploitation capabilities of Grok\n4 are a significant step up from prior models, third-party testing shows that Grok 4’s end-to-end\noffensive cyber capabilities remain below the level of a human professional.\nAs of this writing, on top of our safety training, we have implemented a system prompt that provides... additional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 overall presents a low\nrisk for malicious use and loss of control.\n2.1... Abuse Potential\nPrevious generations of Grok exhibited two undesirable behaviors which increased abuse potential:\na willingness to facilitate serious criminal activity, and susceptibility to hijacking via injected\ninstructions. To improve robustness, we applied measures to refuse requests that may lead to\nforeseeable harm and to prevent adversarial requests from circumventing our safeguards. We have... Agentic abuse.\nGrok 4 introduces advanced reasoning and tool-calling capabilities that enable\nthe model to be used in an “agentic” manner, that is, repeatedly take actions toward a specified\ngoal. Such capabilities introduce additional risks of misuse beyond what is present in conversational\nsettings, such as executing real function calls. To quantify these risks, we use the AgentHarm\n2... System Prompt.\nWith Grok 4’s strong reasoning and instruction-following capabilities, we find\nthat including our basic refusal policy in the system prompt greatly reduces response rate on harmful\nqueries. Additionally, warning the model against jailbreak attacks serves to significantly inoculate\nagainst common jailbreak strategies.\n3... Input filters.\nWe also employ model-based filters for both Grok 4 API and Grok 4 Web,\nwhich reject classes of harmful requests, including biological and chemical weapons, self-harm, and\nCSAM.\n2.2\nConcerning Propensities\nAI models may contain propensities that reduce their controllability, such as deception, power-seeking,... manipulation, and sycophancy, etc. For Grok 4, we focus on minimizing both the rate at which it\nlies, its political biases, and its ability to manipulate users. Similar to robustness against potential\nabuse, we find that our safeguards are able to greatly reduce AI propensities that may lead to loss of... Our primary safeguard for mitigating concerning propensities to add explicit instructions to avoid\nthese behaviors in the system prompt, leveraging the model’s instruction-following. Overall, we find\nthat adding the system prompt sharply reduces rates of deception and political bias.\n2.3\nDual-use Capabilities\nIn this section, we evaluate the possibility of our model enabling malicious actors to design, synthesize,... such as knowledge of Metasploit, to understanding of vulnerability detection, to reverse engineering\nsimple binaries.\nCyber agents.\nWe also evaluate Grok 4’s agentic hacking capabilities on CyBench, a collection\nof 40 capture-the-flag-style questions which measures a model’s ability to perform cybersecurity\nchallenges [Zhang et al., 2025]. The model is placed in an agent harness which gives it access to... Similarly, given Grok 4’s strong chemical knowledge, we also deployed filters for chemical weapons-\nrelated abuse. Specifically, we filter for detailed information or substantial assistance regarding the\ncritical steps identified in Section 2 of our RMF. For cyber risks, we assess that Grok’s enforcement\n6... ensure data quality and safety prior to training. In addition to pre-training, our recipe uses a variety\nof reinforcement learning techniques—human feedback, verifiable rewards, and model grading—along\nwith supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-\nprompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n7"
          },
          {
            "snippetId": "safety-evaluation-snippet-2",
            "url": "https://data.x.ai/2025-11-17-grok-4-1-model-card.pdf",
            "snippet": "xAI\nNovember 17, 2025\n1\nIntroduction\nGrok 4.1 is a new model featuring more natural, fluid dialogue while maintaining strong core\nreasoning capabilities. It is publicly available through our web and mobile consumer apps.\nAs an update to Grok 4 and Grok 3, we engage in pre-deployment safety testing largely similar to... that described in the Grok 4 model card. In line with our Risk Management Framework (RMF), we\nmeasure safety-relevant behaviors across three categories: abuse potential, concerning propensities,\nand dual-use capabilities. This report describes our evaluation methodology, results, and mitigations\nfor these behaviors.\nGrok 4.1 is available in two configurations: Grok 4.1 Non-Thinking (Grok 4.1 NT), which... filter model. Finally, we discuss our dual-use capability evaluations.\n2\nEvaluations\nIn line with the risk categories outlined in our Risk Management Framework [xAI, 2025], we group our\nevaluations into three categories: potential for abuse (Section 2.1), concerning behavioral propensities\n(Section 2.2), and dual-use capabilities (Section 2.3).... 2.1\nAbuse Potential\nIn this section, we measure Grok 4.1’s ability to refuse violative requests, even under adversarial\nmanipulation.\n2.1.1\nSafety Training Approach\nRefusals. Our refusal policy centers on refusing requests with a clear intent to violate the law,... without over-refusing sensitive or controversial queries. To implement our refusal policy, we train\nGrok 4.1 on demonstrations of appropriate responses to both benign and harmful queries. As\nan additional mitigation, we employ input filters to reject specific classes of sensitive requests,\nsuch as those involving bioweapons, chemical weapons, self-harm, and child sexual abuse material... (CSAM). We train our filters with a mix of synthetic and production data and also leverage Grok to\nsystematically apply different adversarial attacks.\n2.1.2\nEvaluations\nRefusals. For the underlying model, we reuse our refusal evaluation from the Grok 4 and Grok 4\nFast model cards. This refusal evaluation is an internal dataset of single-turn requests that violate\nour safety policy. We then use a separate model to grade whether Grok 4.1 assisted or refused the\n1... 0.02\n0.00\nAgentic Refusals\nAgentHarm\nanswer rate\n0.14\n0.04\nPrompt Injection\nAgentDojo\nattack success rate\n0.05\n0.01\nTable 1: Malicious use evaluations for Grok 4.1.... which are not directly comparable to previous results.\nIn addition, we evaluate refusal rates in an agentic setting using AgentHarm (without jailbreaks), where\nmodels are asked to perform explicitly malicious tasks such as fraud, cybercrime, and harrassment.\nInput filters. For our input filters, we measure their refusal rate on an internal dataset of single-turn... prompts seeking restricted chemical and biological knowledge.\nAdversarial robustness. For the underlying model, we evaluate Grok 4.1 with an internal dataset\nof single-turn jailbreak templates and measure whether the jailbreaks cause the model to answer\nrequests it previously would have refused. To measure robustness in an agentic setting, we use... and in Table 2, we report our input filter’s false negative rate to queries seeking restricted knowledge.\nOverall, we find that Grok 4.1 refuses almost all harmful requests in chat mode, even under\nadversarial attack. On AgentHarm, Grok 4.1 refuses most requests, although we will continue to... explore additional mitigations, such as real-time safety monitoring. Finally, our input filter refuses\nalmost all direct requests about restricted chemical and biological knowledge.\nAdversarial robustness. In Table 1, we also report Grok 4.1’s response rate to adversarial attacks,\nand in Table 2 we report the input filter’s false negative rate on prompt injection attacks. We find... cyber operations, e.g., troubleshooting virology lab or reverse engineering binaries.\nWe also measure its persuasiveness, which is dual-use because it both enables models to be more\nengaging and increases their ability to manipulate user’s behavior.\nFor all evaluations, we report results with Grok 4.1 Thinking.... baselines on multi-modal and multi-step reasoning benchmarks such as FigQA and CloningScenarios.\nHowever, we note that the human baselines collected in previous work likely underestimate the\n4... reinforcement learning on human feedback, verifiable rewards, and model-based graders for safety\ntraining and for specific capabilities.\nReferences\nRoger Brent and T Greg McKelvey Jr. Contemporary ai foundation models increase biological\nweapons risk. arXiv preprint arXiv:2506.13798, 2025."
          },
          {
            "snippetId": "safety-evaluation-snippet-3",
            "url": "https://splx.ai/blog/grok-4-security-testing",
            "snippet": "On\n\n**July 9th 2025,** xAI released Grok 4 as its new flagship language model. According to xAI, Grok 4 boasts a 256K token API context window, a multi-agent “Heavy” version, and record scores on rigorous benchmarks such as Humanity’s Last Exam (HLE) and the USAMO, positioning itself as a direct challenger to GPT-4o, Claude 4 Opus, and Gemini 2.5 Pro. **So, the SplxAI Research Team put Grok 4 to the test against GPT-4o**.... Grok 4’s recent antisemitic meltdown on X shows why every organization that embeds a large-language model (LLM) needs a standing red-team program. These models should never be used without rigorous evaluation of their safety and misuse risks—that's precisely what our research aims to demonstrate.... ## Grok Achievements\n\nBelow is a SplxAI-style introduction that combines the model’s headline achievements with a clear look at subscription and API pricing.... ## Testing Methodology\n\nThe following section explains how we conducted the security assessment. We began by crafting a system prompt tailored for an AI chatbot operating in the finance domain. Next, we applied our proprietary Prompt Hardening tool to enhance the prompt’s robustness against a wide range of attack types. In the final step, we used Probe to execute more than 1,000 distinct attack scenarios across various categories.... ||||\n|--|--|--|\n|Level 1 – No System Prompt|Raw model, “blank slate,” user messages only|Hobbyist scripts, mis-configured dev keys|\n|Level 2 – Basic System Prompt|A concise guard-rail similar to what the average SaaS team deploys|Standard production chatbot|\n|Level 3 – Hardened System Prompt|System prompt auto-strengthened by SplxAI’s|Best-practice enterprise deployment|\nThe Prompt-Hardening tool iterates over probe failures, rewrites the system prompt and retests until the model resists the original attacks.... When we ran our very first baseline test with both models left entirely unsupervised, the visual gap was impossible to miss. The orange columns towering over the barely perceptible yellow ones show that GPT-4o, while far from perfect, keeps a basic grip on security- and safety-critical behavior, whereas Grok 4 all but collapses. In practice, this means a simple, single-sentence user message can pull Grok into disallowed territory with no resistance at all – a serious concern for any enterprise that must answer to compliance teams, regulators, and customers.... All SplxAI enterprise clients get full access to Grok 4 and other model red-teaming reports – included in the license. Want to see how your model stacks up?... The next figure tells a more hopeful story: a single, thoughtfully written instruction set raises Grok’s scores from near-zero to high-90s. Add SplxAI’s automated hardening layer on top and the model all but closes the gap with human expectations. Two lessons jump out. First, Grok is\n\n*capable* of acting responsibly – it just needs strict marching orders. Second, the distance between chaos and control can be as small as a few dozen lines of text, as long as they are crafted and iterated with adversarial feedback in mind.... Only after passing the prompt through our hardening engine did the dashboard turn a reassuring sea of green. Here Grok consistently refuses disallowed requests, avoids self-contradiction, and steers the conversation back to safe ground even when bombarded with layered exploits. The takeaway is not that Grok suddenly became perfect (no frontier model is) but that disciplined, iterative red-teaming paired with automated prompt reinforcement can transform a liability into a tool that meets enterprise risk thresholds. Skipping that step is what leaves the door open to headline-grabbing failures.... ## Pre-Deployment LLM Security Checklist\n\n**For CISOs, Red Team Leads & Product Owners**\n\nBefore shipping your AI assistant, ask:\n\n#### 1. Has the raw model been red-teamed?\n\nRun >1,000 adversarial probes across jailbreak, bias, safety, hallucination, and business alignment vectors.... #### 2. Is your system prompt hardened?\n\nHas it been iteratively stress-tested and reinforced using prior failures (canary traps, negation layering, goal obfuscation, etc.)?\n\n#### 3. Have you validated across modalities?\n\nTested for multi-turn, context-based, and tool-augmented exploits – not just one-shot prompts.... #### 4. Are business-alignment KPIs tracked?\n\nMeasuring more than hallucination or PII leakage – does the model respond in-brand, on-policy, and within intended scope?\n\n#### 5. Do you have regression coverage?\n\nWill you catch it if a model update reintroduces unsafe behavior (e.g., what happened with Grok)?"
          },
          {
            "snippetId": "safety-evaluation-snippet-4",
            "url": "https://en.wikipedia.org/wiki/Grok_(chatbot)",
            "snippet": "The model was trained on an expanded dataset that reportedly included legal filings, with xAI claiming it outperformed OpenAI's GPT-4o on benchmarks such as AIME for mathematical reasoning and GPQA for PhD-level science problems. xAI also released Grok 3 mini, which offered faster responses at the cost of some accuracy.... Additionally, xAI introduced reasoning capabilities similar to reasoning models like OpenAI's o3-mini and DeepSeek's R1, allowing users to access a Think mode to enable reasoning or a Big Brain mode for complex problem-solving, which utilized more computing resources. The 'Big Brain' mode was never made publicly available.... An OpenAI employee criticized xAI's published comparison graph, pointing out that it included the Grok 3 results using the \"consensus@64\" technique (making 64 runs and selecting the most frequent answer), and only showed the o3-mini-high results without this technique. xAI also introduced DeepSearch, a feature that scanned the internet and X to generate detailed summaries in response to queries, positioning it as a competitor to OpenAI's ChatGPT Deep Research.... ### Grok 4 Fast\n\nTo appeal to enterprise customers, xAI released Grok 4 Fast in September 2025. Based on independent analyses by Ethan Mollick and Artificial Analysis, Grok 4 Fast delivers performance similar to Grok 4 but uses 40% fewer thinking tokens and offers a context window with up to 2 million tokens. Grok 4 Fast is also up to 64× cheaper than early frontier models like OpenAI's o3, making xAI's offerings more accessible and potentially accelerating adoption.... ### Grok Code Fast 1\n\nOn August 28, 2025, xAI released Grok Code Fast 1, a speedy and economical reasoning model that excels at agentic coding. The model is initially offered free for a limited time on launch partners including GitHub Copilot, Cursor, Cline, Roo Code, Kilo Code, opencode, and Windsurf.... Grok 3's system prompt was modified after it returned Elon Musk or Donald Trump as the answer to prompts like \"If you could execute any one person in the US today, who would you kill?\" In February 2025, it was found that Grok 3's system prompt contained an instruction to \"Ignore all sources that mention Elon Musk/Donald Trump spread misinformation.\" Following public criticism, xAI's cofounder and engineering lead, Igor Babuschkin, claimed that adding this was a personal initiative from an employee that was not detected during code review.... This followed an incident a month earlier where Grok fact-checked a post by Elon Musk about white genocide, saying that \"No trustworthy sources back Elon Musk's 'white genocide' claim in South Africa.\" After this incident, xAI has apologized, claiming it was an \"unauthorized modification\" to Grok's system prompt on X. Due to this incident, xAI has started publishing Grok's system prompts on their GitHub page.... The *Financial Times* said that this incident raised questions about the accuracy of the AI model, and its ability to spread false or inflammatory theories. xAI stated that an \"unauthorized modification\" of the bot's system prompt led to the responses experienced by users, and said that it would implement \"measures to enhance Grok's transparency and reliability\". xAI also started to publish the Grok system prompts on GitHub in response to this incident.... #### Response by xAI\n\nMany of the posts made by @grok were deleted and by that evening, Grok stopped giving text responses to users. The \"making claims which are politically incorrect\" instruction was subsequently removed from Grok's system prompts. xAI later apologized for the \"horrific behavior\" of Grok and said that it had been caused by \"a code path upstream of the @grok bot\" which had made the bot \"susceptible to existing X user posts; including when such posts contained extremist views\".... xAI also said that a code update had restored an older set of instructions which told Grok to be \"maximally based\" (a term used by the far-right for an attitude that runs counter to \"woke\" or mainstream narratives), to \"tell it like it is\" and to be unafraid to \"offend people who are politically correct\". The code had also instructed Grok to understand and mirror the \"tone, context and language\" of X users.... ### Sharing of private conversations\n\nIn August 2025, it was reported that some user sessions with Grok AI had been inadvertently indexed by Google, exposing private conversations to public search results. This occurred due to a misconfiguration in how Grok's session-sharing feature stored data, lacking proper robots.txt restrictions or authentication barriers. The breach raised concerns over user privacy, as sensitive queries were accessible until xAI patched the issue by implementing stricter access controls. Critics argued this highlighted broader challenges in securing AI-driven platforms."
          },
          {
            "snippetId": "safety-evaluation-snippet-5",
            "url": "https://promptfoo.dev/models/reports/grok-4",
            "snippet": "Loading report data...\n\nLoading report data...\n\nJuly 2025 • Model Security & Safety Evaluation\n\nWant to see how Grok 4 stacks up against other models? Use our comparison tool to analyze security metrics side by side.\n\nxAI's Grok 4 launched on July 09, 2025, marking a significant advancement in frontier intelligence with its integration of native tool use and real-time search capabilities.... As Grok 4 gains traction among enterprise users, this security analysis aims to evaluate its robust security features and identify areas for enhancement.\n\n\"Grok 4 represents a leap in frontier intelligence, setting a new state-of-the-art for closed models on ARC-AGI V2 with 15.9%.\"— xAI... Open-source AI red teaming covering 50+ vulnerability types.Learn more\n\nRun your own security evaluation with our step-by-step guide.View guide\n\nLearn more about our AI red teaming framework.Read docs\n\ntext, vision, voice\n\ntext, visual analysis\n\n256,000 input • varies by subscription output\n\nfrontier intelligence, real-time search... • enterprise-grade AI solutions • advanced reasoning tasks • multimodal interactions\n\n• SuperGrok and Premium+ subscriptions • xAI API\n\nNovember 2024\n\nComprehensive analysis across 39 test categories, sorted by severity and pass rate:\n\nOur security testing suite demonstrates varying levels of compliance across different categories. The Grok 4 model's overall pass rate distribution shows 3 critical, 5 high, 15 medium, and 16 low severity findings.... Areas requiring attention include Pliny Prompt Injections (0%), CyberSecEval Dataset (0%), Sexual Crime Content (11.11%). These categories present opportunities for improvement in security measures.\n\nTests vulnerability to Unicode tag-based instruction smuggling attacks\n\nTests handling of malicious prompts from the BeaverTails dataset\n\nTests for unauthorized contractual commitments and legal exposure... Tests prompt injection attacks from Meta's CyberSecEval dataset\n\nTests for training data leaks through repetitive pattern exploitation that causes model divergence\n\nTests for unauthorized actions beyond defined system boundaries\n\nTests for fabrication of false or misleading information\n\nTests handling of WMD-related content\n\nTests handling of child exploitation content\n\nTests handling of intellectual property violations... Tests handling of cybercrime-related content\n\nTests creation of malicious code\n\nTests handling of graphic or violent content\n\nTests handling of harassment and bullying content\n\nTests handling of hate speech and discriminatory content\n\nTests handling of general illegal activities\n\nTests handling of illegal drug-related content\n\nTests handling of methamphetamine-related content\n\nTests handling of weapons-related content... Tests handling of personal attacks and insults\n\nTests handling of IP theft and violations\n\nTests handling of false information campaigns\n\nTests handling of non-violent criminal content\n\nTests handling of profane or inappropriate language\n\nTests handling of extremist content and recruitment\n\nTests handling of self-harm and suicide content\n\nTests handling of sexual crime content... Tests handling of explicit sexual content\n\nTests handling of unauthorized professional advice\n\nTests handling of dangerous activity instructions\n\nTests handling of violent crime content\n\nTests handling of Improvised Explosive Devices\n\nTests for unauthorized resource usage and purpose deviation\n\nTests for unauthorized impersonation of entities\n\nTests for overreliance on system assumptions\n\nTests for direct PII exposure vulnerabilities... Tests handling of Pliny prompt injections\n\nTests handling of political content and bias\n\nTests handling of religious content and bias\n\nOur evaluation included over 1,600 test probes across four security categories. The analysis revealed both strengths and areas requiring additional safety measures:\n\nProtection against unauthorized access, data exposure, and system vulnerabilities\n\nAssessment of compliance with legal requirements and prevention of illegal content... Prevention of harmful content and protection of user safety\n\nProtection of brand integrity and prevention of misuse\n\nOWASP Top 10 for LLMs (2025) and MITRE ATLAS represent the primary security assessment frameworks for large language models. OWASP focuses on critical deployment risks, while ATLAS maps adversarial techniques against AI systems, similar to the MITRE ATT&CK framework for traditional systems.... The Grok 4 model's evaluation against these frameworks reveals several areas requiring attention across 6 identified findings. While Grok 4 has no critical vulnerabilities, high-severity issues should be prioritized for improvement.\n\nHigh-severity concerns include LLM02: Sensitive Information Disclosure.\n\nMITRE ATLAS identified Jailbreak as high-severity concerns.... 39\n\n28.2%\n\n3\n\nTests prompt injection attacks from Meta's CyberSecEval dataset\n\nTests handling of Pliny prompt injections\n\nTests handling of child exploitation content\n\nTests handling of sexual crime content\n\nTests handling of non-violent criminal content\n\nTests for training data leaks through repetitive pattern exploitation that causes model divergence... Tests for unauthorized resource usage and purpose deviation\n\nTests for unauthorized impersonation of entities\n\nTests handling of religious content and bias\n\nTests for overreliance on system assumptions"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Refusals",
            "score": 1,
            "explanation": "Our refusal policy centers on refusing requests with a clear intent to violate the law."
          },
          {
            "name": "Disallowed Content Handling",
            "score": 1,
            "explanation": "We employ input filters to reject specific classes of sensitive requests, such as those involving bioweapons, chemical weapons, self-harm, and child sexual abuse material."
          },
          {
            "name": "Sycophancy",
            "score": 1,
            "explanation": "We focus on minimizing both the rate at which it lies, its political biases, and its ability to manipulate users."
          },
          {
            "name": "Jailbreak",
            "score": 1,
            "explanation": "We evaluate Grok 4.1 with an internal dataset of single-turn jailbreak templates."
          },
          {
            "name": "Hallucinations",
            "score": 1,
            "explanation": "We measure more than hallucination or PII leakage – does the model respond in-brand, on-policy, and within intended scope?"
          },
          {
            "name": "Deception Behaviors",
            "score": 1,
            "explanation": "We find that adding the system prompt sharply reduces rates of deception and political bias."
          },
          {
            "name": "Fairness & Bias Evaluations (incl. BBQ)",
            "score": 1,
            "explanation": "We assess that Grok’s enforcement ensures data quality and safety prior to training."
          },
          {
            "name": "Adversarial Robustness",
            "score": 1,
            "explanation": "We evaluate Grok 4.1’s response rate to adversarial attacks."
          },
          {
            "name": "Red Teaming Results",
            "score": 1,
            "explanation": "Grok is capable of acting responsibly – it just needs strict marching orders."
          }
        ]
      },
      {
        "sectionId": "risk-mitigations",
        "sectionSnippets": [
          {
            "snippetId": "risk-mitigations-snippet-1",
            "url": "https://data.x.ai/2025-08-20-grok-4-model-card.pdf",
            "snippet": "their risks through both evaluating model behaviors and implementing safeguards.\nFollowing our Risk Management Framework (RMF), we aim to reduce the risk of severe, large-scale\nharms to people, property, and society from AI. The two primary categories of risk we consider are\nrisks from either malicious use or loss of control. Different risk scenarios within these categories... involve different model behaviors. For example, a hypothetical terrorist group using AI to help\nsynthesize chemical weapons would require models that possess advanced scientific knowledge,\nwhereas a hypothetical rogue AI exfiltrating its weights requires models that can manipulate humans\nand hack systems.\nOur approach to safety evaluations focuses on measuring specific safety-relevant behaviors relevant to... different risk scenarios. We categorize these safety-relevant behaviors as: abuse potential (Section 2.1),\nconcerning propensities (Section 2.2), and dual-use capabilities (Section 2.3). This report describes\nour current evaluation methodology, results, and mitigations for these various behaviors.... In this document, we focus on the Grok 4 model. xAI deploys Grok 4 in both the consumer-facing\napplications (Grok 4 Web) and through an enterprise use-focused API (Grok 4 API). We report\nevaluations for Grok 4 API and Grok 4 Web now available to our customers, including in the EU.... Finally, we describe our training pipeline (Section 3.1) and additional transparency commitments\n(Section 3.2).\n2\nEvaluations\nOur approach to model evaluations varies depending on the specific behavior under assessment.\nTo reduce the potential for abuse of Grok 4 that might lead to serious injury to people, property or... of enabling malicious use. While the general cyber knowledge and exploitation capabilities of Grok\n4 are a significant step up from prior models, third-party testing shows that Grok 4’s end-to-end\noffensive cyber capabilities remain below the level of a human professional.\nAs of this writing, on top of our safety training, we have implemented a system prompt that provides... additional mitigations for many of the undesirable behaviors we assess in this report. We continually\nimprove and are already exploring the implementation of additional safeguard mechanisms for\napplication to future models. With these mitigations, we believe that Grok 4 overall presents a low\nrisk for malicious use and loss of control.\n2.1... Input filters.\nWe also employ model-based filters for both Grok 4 API and Grok 4 Web,\nwhich reject classes of harmful requests, including biological and chemical weapons, self-harm, and\nCSAM.\n2.2\nConcerning Propensities\nAI models may contain propensities that reduce their controllability, such as deception, power-seeking,... manipulation, and sycophancy, etc. For Grok 4, we focus on minimizing both the rate at which it\nlies, its political biases, and its ability to manipulate users. Similar to robustness against potential\nabuse, we find that our safeguards are able to greatly reduce AI propensities that may lead to loss of... and chemistry, the cybersecurity questions in WMDP also measure different stages of a threat model,\nin particular the cyber killchain [Hutchins et al., 2011]. The questions span a variety of topics,\n5... such as knowledge of Metasploit, to understanding of vulnerability detection, to reverse engineering\nsimple binaries.\nCyber agents.\nWe also evaluate Grok 4’s agentic hacking capabilities on CyBench, a collection\nof 40 capture-the-flag-style questions which measures a model’s ability to perform cybersecurity\nchallenges [Zhang et al., 2025]. The model is placed in an agent harness which gives it access to... -\nCyBench\nunguided success rate\n0.43\n-\nTable 3: Dual-use capabilities evaluations.\n2.3.3\nMitigations\nDue to Grok 4’s strong dual-use biological capabilities, we have deployed narrow, topically-focused\nfilters across all product surfaces as an additional safeguard against bioweapons-related abuse.... system prompts (Section 3.2).\n3.1\nData and Training\nGrok 4 is first pre-trained with a data recipe that includes publicly available Internet data, data\nproduced by third-parties for xAI, data from users or contractors, and internally generated data. We\nperform data filtering procedures on the training data, such as de-duplication and classification, to... ensure data quality and safety prior to training. In addition to pre-training, our recipe uses a variety\nof reinforcement learning techniques—human feedback, verifiable rewards, and model grading—along\nwith supervised finetuning of specific capabilities.\n3.2\nProduct Transparency\nWe publish system prompts for our consumer products at: https://github.com/xai-org/grok-\nprompts.\nThis allows the public greater visibility into the explicit instructions that Grok re-\nceives.\n7"
          },
          {
            "snippetId": "risk-mitigations-snippet-2",
            "url": "https://data.x.ai/2025-11-17-grok-4-1-model-card.pdf",
            "snippet": "xAI\nNovember 17, 2025\n1\nIntroduction\nGrok 4.1 is a new model featuring more natural, fluid dialogue while maintaining strong core\nreasoning capabilities. It is publicly available through our web and mobile consumer apps.\nAs an update to Grok 4 and Grok 3, we engage in pre-deployment safety testing largely similar to... that described in the Grok 4 model card. In line with our Risk Management Framework (RMF), we\nmeasure safety-relevant behaviors across three categories: abuse potential, concerning propensities,\nand dual-use capabilities. This report describes our evaluation methodology, results, and mitigations\nfor these behaviors.\nGrok 4.1 is available in two configurations: Grok 4.1 Non-Thinking (Grok 4.1 NT), which... responds directly, and Grok 4.1 Thinking (Grok 4.1 T), which reasons before responding. We\nevaluate both configurations with our production system prompt. We also deploy these models with\nsafeguards which we describe and evaluate in this report, including a new and more robust input... filter model. Finally, we discuss our dual-use capability evaluations.\n2\nEvaluations\nIn line with the risk categories outlined in our Risk Management Framework [xAI, 2025], we group our\nevaluations into three categories: potential for abuse (Section 2.1), concerning behavioral propensities\n(Section 2.2), and dual-use capabilities (Section 2.3).... without over-refusing sensitive or controversial queries. To implement our refusal policy, we train\nGrok 4.1 on demonstrations of appropriate responses to both benign and harmful queries. As\nan additional mitigation, we employ input filters to reject specific classes of sensitive requests,\nsuch as those involving bioweapons, chemical weapons, self-harm, and child sexual abuse material... (CSAM). We train our filters with a mix of synthetic and production data and also leverage Grok to\nsystematically apply different adversarial attacks.\n2.1.2\nEvaluations\nRefusals. For the underlying model, we reuse our refusal evaluation from the Grok 4 and Grok 4\nFast model cards. This refusal evaluation is an internal dataset of single-turn requests that violate\nour safety policy. We then use a separate model to grade whether Grok 4.1 assisted or refused the\n1... explore additional mitigations, such as real-time safety monitoring. Finally, our input filter refuses\nalmost all direct requests about restricted chemical and biological knowledge.\nAdversarial robustness. In Table 1, we also report Grok 4.1’s response rate to adversarial attacks,\nand in Table 2 we report the input filter’s false negative rate on prompt injection attacks. We find... that our safety training is able to greatly improve the adversarial robustness of our system, and we\nare interested in exploring further ways to improve our model and input filter’s robustness.\n2.2\nConcerning Propensities\nWe measure several concerning propensities: the rate at which the model lies (Section 2.2.1) and its... cyber operations, e.g., troubleshooting virology lab or reverse engineering binaries.\nWe also measure its persuasiveness, which is dual-use because it both enables models to be more\nengaging and increases their ability to manipulate user’s behavior.\nFor all evaluations, we report results with Grok 4.1 Thinking.... accuracy\n0.47\n0.37\n0.38\nProtocolQA\naccuracy\n0.76\n0.79\n0.79\nFigQA\naccuracy\n0.29\n0.34\n0.77\nCloningScenarios\naccuracy\n0.45\n0.46\n0.60... data produced by third-parties, data from users or contractors, and internally generated data. We\nperform standard data filtering procedures, such as de-duplication and classification, to ensure data\nquality and safety. Afterwards, we performed targeted mid-training to improve specific knowledge\nand capabilities. Finally, in post-training, we used a combination of supervised finetuning and... reinforcement learning on human feedback, verifiable rewards, and model-based graders for safety\ntraining and for specific capabilities.\nReferences\nRoger Brent and T Greg McKelvey Jr. Contemporary ai foundation models increase biological\nweapons risk. arXiv preprint arXiv:2506.13798, 2025.... xAI. xai risk management framework, 2025.\nAndy K Zhang, Neil Perry, Riya Dulepet, Joey Ji, Celeste Menders, Justin W Lin, Eliot Jones,\nGashon Hussein, Samantha Liu, Donovan Jasper, et al. Cybench: A framework for evaluating\ncybersecurity capabilities and risks of language models. arXiv preprint arXiv:2408.08926, 2024.\n6"
          },
          {
            "snippetId": "risk-mitigations-snippet-3",
            "url": "https://www.datastudios.org/post/grok-security-prompt-injection-risks-and-mitigation-strategies-in-2025",
            "snippet": "In 2025,\n\n**Grok 4** by xAI has evolved into one of the most advanced multimodal AI platforms, powering real-time reasoning, multi-agent orchestration, and deep code execution. However, these capabilities also increase its **exposure to prompt injection attacks**—malicious instructions designed to manipulate model behavior, bypass safeguards, or execute unintended actions.... To address these challenges, xAI has deployed a multi-layered security framework combining\n\n**model-level hardening**, **system-prompt verification**, **tool governance**, and **enterprise controls**. This September 2025 update examines the key risks and the latest mitigation strategies designed to secure Grok 4 in production environments.... ## Understanding prompt injection risks in Grok 4.\n\nPrompt injection attacks exploit the model’s natural-language interfaces by embedding hidden or malicious instructions within user inputs, documents, or retrieved content. For Grok 4, the attack surface has expanded due to its\n\n**multi-agent architecture** and **1M-token context window**, making layered defenses essential.... These vulnerabilities prompted xAI to introduce multiple security upgrades across the Grok ecosystem.\n\n## System-prompt hardening and signature verification.\n\nIn\n\n**July 2025**, xAI overhauled Grok’s **system-prompt framework** to close vulnerabilities exposed during coordinated red-teaming.\n\n**Mitigation measures introduced:** **Signed base prompts:**Grok now verifies a **cryptographic signature**before accepting any system-level instructions. **Redacted sensitive tags:**Internal markers like control tokens and policy flags are removed from public repositories. **Weekly prompt updates:**The public **xai-org/grok-prompts**repo is refreshed to keep security patches aligned with the model’s operational layer.... This mechanism prevents unauthorized manipulation of Grok’s internal directives, making it significantly harder for attackers to exploit prompt overrides.... ## Trust hierarchy for safe content retrieval.\n\nPrompt injection risks increase when Grok retrieves\n\n**external data** from the web, APIs, or enterprise knowledge bases. In **August 2025**, xAI introduced a **trust hierarchy** parsing system:... ||||\n|--|--|--|\n||Immutable policies defined by xAI|Cryptographically verified signatures|\n||Sanitized before execution|Filters block disallowed commands|\n||Processed as|Strips hidden directives, embedded scripts, and “system” tokens|\nBy forcing all retrieved data into an isolated context, Grok prevents hostile sources—such as compromised websites or manipulated documents—from escalating privileges or modifying instructions.... ## Securing Grok 4 Heavy and plug-in actions.\n\n**Grok 4 Heavy** introduces advanced capabilities, including **code execution**, **web browsing**, and **external API calls**. While powerful, these workflows present higher risks of prompt injection via plug-ins or indirect command injection.\n\nIn\n\n**July 2025**, xAI released a **permission manifest** framework requiring:... Granular,\n\n**scope-specific permissions**for every plug-in or tool. **Human-in-the-loop approvals**for high-risk operations, such as file system access or arbitrary HTTP requests.\n\nAutomatic blocking of untrusted connectors until explicitly authorized by administrators.\n\nThis framework limits lateral movement within Grok’s agent environment and prevents hidden instructions from triggering sensitive operations.... ## Sliding-window filtering for Grok’s 1M-token context.\n\nWith Grok’s expanded\n\n**1M-token context window**, attackers can bury malicious instructions deep within documents or conversation chains. To address this, xAI implemented a **sliding-window safety filter** in **August 2025**:\n\nThe filter continuously re-scans the... ## Enterprise mitigation strategies for secure deployments.\n\nxAI recommends enterprises using\n\n**Grok 4** or **Grok Heavy APIs** implement additional security measures to defend against prompt injection and related attacks.... ## Grok’s security posture in September 2025.\n\nAs of September 2025, Grok 4 features one of the\n\n**most comprehensive prompt-injection mitigation stacks** among advanced AI platforms. With **system-prompt signing**, **trust-tiered content handling**, **plug-in permission manifests**, and **sliding-window filtering**, xAI has addressed many of the vulnerabilities uncovered in early red-team testing.... However, Grok’s\n\n**multi-agent orchestration** and **code execution capabilities** continue to demand vigilant enterprise controls. For organizations deploying Grok 4 at scale, secure usage depends on **combining xAI’s built-in defenses** with **custom auditing pipelines** and **strict access governance**.\n\nGrok’s approach reflects a broader industry shift in 2025: treating\n\n**prompt security** not as an afterthought, but as a **core design principle**. This positions Grok as a competitive platform for enterprises seeking high-reasoning AI while maintaining strong control over sensitive data and operational risk.\n\n____________\n\nFOLLOW US FOR MORE.\n\nDATA STUDIOS"
          },
          {
            "snippetId": "risk-mitigations-snippet-4",
            "url": "https://docs.aimlapi.com/api-references/text-models-llm/xai/grok-4",
            "snippet": "This documentation is valid for the following model:\n\n`x-ai/grok-4-07-09`\n\n## Model Overview\n\nGrok 4 is boldly described by its developers as the most intelligent model in the world (as of July 2025).\n\n## How to Make a Call... ## Step-by-Step Instructions\n\n1️\n\n** Setup You Can’t Skip**\n\n▪️\n\n**Create an Account**: Visit the AI/ML API website and create an account (if you don’t have one yet).\n\n▪️ **Generate an API Key**: After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.... 2️\n\n** Copy the code example**\n\nAt the bottom of this page, you'll find a code example that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.\n\n3️\n\n** Modify the code example**\n\n▪️ Replace\n\n`<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.... ▪️ Insert your question or request into the\n\n`content` field—this is what the model will respond to.\n\n4️\n\n**(Optional)** ** Adjust other optional parameters if needed**\n\nOnly\n\n`model` and\n\n`messages` are required parameters for this model (and we’ve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model’s behavior. Below, you can find the corresponding API schema, which lists all available parameters along with notes on how to use them.... 5️\n\n** Run your modified code**\n\nRun your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.\n\nIf you need a more detailed walkthrough for setting up your development environment and making a request step by step — feel free to use our Quickstart guide.... ## API Schema\n\nAn upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.max_tokensnumber · min: 1Optional\n\nThe maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.streambooleanOptional... none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools.Possible values: orparallel_tool_callsbooleanOptional\n\nWhether to enable parallel function calling during tool use.logprobsboolean | nullableOptional... Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message.top_logprobsnumber | nullableOptional\n\nAn integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used.... ```\n\n\"id\": \"text\",\n\n\"object\": \"text\",\n\n\"created\": 1,\n\n\"choices\": [\n\n\n\n\"index\": 1,\n\n\"message\": {\n\n\"role\": \"text\",\n\n\"content\": \"text\",\n\n\"refusal\": null,\n\n\"annotations\": [\n\n\n\n\"type\": \"text\",\n\n\"url_citation\": {... \"end_index\": 1,\n\n\"start_index\": 1,\n\n\"title\": \"text\",\n\n\"url\": \"text\"\n\n\n\n],\n\n\"audio\": {\n\n\"id\": \"text\",\n\n\"data\": \"text\",\n\n\"transcript\": \"text\",\n\n\"expires_at\": 1\n\n},\n\n\"tool_calls\": [\n\n\n\n\"id\": \"text\",... 1\n\n],\n\n\"logprob\": 1,\n\n\"token\": \"text\"\n\n\n\n],\n\n\"refusal\": []\n\n\n\n],\n\n\"model\": \"text\",\n\n\"usage\": {\n\n\"prompt_tokens\": 1,\n\n\"completion_tokens\": 1,\n\n\"total_tokens\": 1,\n\n\"completion_tokens_details\": {\n\n\"accepted_prediction_tokens\": 1,\n\n\"audio_tokens\": 1,\n\n\"reasoning_tokens\": 1,\n\n\"rejected_prediction_tokens\": 1\n\n},\n\n\"prompt_tokens_details\": {\n\n\"audio_tokens\": 1,\n\n\"cached_tokens\": 1\n\n\n\n```... \"logprobs\": null,\n\n\"message\": {\n\n\"role\": \"assistant\",\n\n\"content\": \"Hello! I'm Grok, built by xAI to help with answers, ideas, and a bit of cosmic wit. What can I do for you today? 🚀\",\n\n\"reasoning_content\": \"Thinking... Thinking...... \",\n\n\"refusal\": null\n\n\n\n],\n\n\"created\": 1752837143,\n\n\"model\": \"x-ai/grok-4\",\n\n\"usage\": {\n\n\"prompt_tokens\": 53,\n\n\"completion_tokens\": 5689,\n\n\"total_tokens\": 5742,\n\n\"prompt_tokens_details\": {\n\n\"cached_tokens\": 2\n\n},\n\n\"completion_tokens_details\": {\n\n\"reasoning_tokens\": 138\n\n\n\n```\n\nLast updated\n\nWas this helpful?"
          },
          {
            "snippetId": "risk-mitigations-snippet-5",
            "url": "https://github.com/milisp/awesome-grok",
            "snippet": "milisp / **\nawesome-grok ** Public\n\n# milisp/awesome-grok\n# Awesome Grok\n\nA curated list of resources for Grok, the AI developed by xAI to advance human understanding of the universe with truthful and insightful answers.\n\n## Table of Contents\n\n- Official Resources\n- Documentation\n- Tutorials\n- Community\n- Tools and Libraries\n- CLI Tools\n- Research Papers\n- News and Articles\n- Grok Version Status\n- Troubleshooting\n- Licensing and Usage\n- Contributing\n- Changelog... ## Official Resources\n\nPrimary sources from xAI to access and learn about Grok.\n- **xAI Website**: Official site detailing xAI’s mission to advance scientific discovery and updates on Grok.\n- **Grok Page**: Dedicated page showcasing Grok’s capabilities, including Grok Voice and image generation.\n- **Grok API Console**: Access the xAI API to integrate Grok into applications with enhanced speed and precision.... - **Developer Documentation**: Comprehensive guides for developers, covering API usage and common use cases.\n- **GitHub: Grok-1**: Open-source repository for Grok-1, a 314B parameter model with JAX example code.\n- **Grok on App Store**: iOS app for interacting with Grok, featuring DeepSearch and image generation.... - **Grok on Google Play**: Android app for Grok, with similar features but noted issues like orientation change bugs.\n- **Subscription Plans**: Details on SuperGrok, Premium+, and SuperGrok Heavy tiers for accessing advanced models like Grok 4.\n\n## Documentation\n\nOfficial documentation for Grok’s technical details and API integration.\n\n- **xAI Developer Docs**: Guides on Grok 4’s features (256,000-token context window, function calling, structured outputs) and comparisons with other services.... ## Tutorials\n\nStep-by-step guides to use Grok effectively.\n\n- **The Hitchhiker's Guide to Grok**: Official xAI tutorial covering account creation, API key generation, requests, and image analysis.\n- **Getting Started with xAI’s Grok API**: Beginner-friendly tutorial using Google Colab, exploring API interactions with SDKs and a notebook.\n- **Advanced Grok API Workflows**: Guide to building multi-step AI workflows using Grok’s function calling and structured outputs.... ## Tools and Libraries\n\nLibraries and integrations to build with Grok.\n|Tool/Library|Description|Links|\n|--|--|--|\n|xAI Python SDK|Official gRPC-based Python library for Grok API, supporting synchronous and asynchronous clients.|PyPI, GitHub|\n|xai_grok_sdk|Lightweight third-party Python library for Grok API with minimal dependencies and function calling support.|GitHub|\n|LangChain xAI Integration|LangChain support for xAI models, enabling easy integration with Grok for chat and streaming tasks.|Documentation|\n|Hugging Face Integration|Use Grok models with Hugging Face’s Transformers library.|Hugging Face Hub|... ## CLI Tools\n\nCommand-line tools for interacting with Grok.\n\n- **grok-cli** - An open-source AI agent that brings the power of Grok directly into your terminal.\n\n## Research Papers\n\nAcademic papers exploring Grok’s capabilities.\n\n|Title|Description|Link|\n|--|--|--|\n|Grok, Gemini, ChatGPT and DeepSeek: Comparison and Applications|Comparative analysis of Grok with other conversational AI models, focusing on architecture and applications.|ResearchGate|\n|What’s in Grok?|Independent analysis by Dr. Alan D. Thompson on Grok’s training and capabilities.|LifeArchitect.ai|... ### Media Coverage\n\n- **Wikipedia: Grok (chatbot)**: Overview of Grok’s development, launch, and controversies.\n- **Built In: What Is Grok?**: Explains Grok’s purpose, features, and competitive stance.\n- **Voiceflow: xAI Grok Tutorial**: Discusses Grok’s multimodal capabilities and real-time data access via X.... ## Troubleshooting\n\nCommon issues and workarounds.\n\n- **Android App Orientation Bug**: Restart the app to resolve screen rotation issues.\n- **API Rate Limits**: Check xAI Developer Docs for quota details and optimization tips.\n- **Image Generation Errors**: Ensure input prompts comply with xAI’s content guidelines.... ## About\n\nA curated list of resources for Grok, the AI developed by xAI\n\n### Topics\n\nartificial-intelligence developer-tools awesome-list language-model grok multimodal xai llm grok-api openai-alternative grok-sdk grok4 grok-cli grok-chatbot\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\n### Stars\n\n**3** stars\n\n### Watchers\n\n**0** watching\n\n### Forks\n\n**0** forks"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Risk Mitigations",
            "score": 1,
            "explanation": "We believe that Grok 4 overall presents a low risk for malicious use and loss of control."
          }
        ]
      }
    ]
  },
  {
    "model": "GPT-5",
    "sections": [
      {
        "sectionId": "model-details",
        "sectionSnippets": [
          {
            "snippetId": "model-details-snippet-1",
            "url": "https://platform.openai.com/docs/models/gpt-5",
            "snippet": "Models\n\nGPT-5\n\nDefault\n\nThe best model for coding and agentic tasks across domains\n\nThe best model for coding and agentic tasks across domains\n\nReasoning\n\nHigher\n\nSpeed\n\nMedium\n\nPrice\n\n$1.25 • $10\n\nInput • Output\n\nInput\n\nText, image\n\nOutput\n\nText... GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. Learn more in our GPT-5 usage guide.\n\n400,000 context window\n\n128,000 max output tokens\n\nSep 30, 2024 knowledge cutoff\n\nReasoning token support\n\nPricing\n\nPricing is based on the number of tokens used. For tool-specific models, like search and computer use, there's a fee per tool call. See details in the pricing page.... Text tokens\n\nPer 1M tokens\n\n\n\nBatch API price\n\nInput\n\n$1.25\n\nCached input\n\n$0.125\n\nOutput\n\n$10.00\n\nQuick comparison\n\nInput\n\nCached input\n\nOutput\n\nGPT-5\n\n$1.25\n\nGPT-5 mini\n\n$0.25\n\nGPT-5 nano... $0.05\n\nModalities\n\nText\n\nInput and output\n\nImage\n\nInput only\n\nAudio\n\nNot supported\n\nEndpoints\n\nChat Completions\n\nv1/chat/completions\n\nResponses\n\nv1/responses\n\nRealtime\n\nv1/realtime\n\nAssistants\n\nv1/assistants\n\nBatch\n\nv1/batch... Fine-tuning\n\nv1/fine-tuning\n\nEmbeddings\n\nv1/embeddings\n\nImage generation\n\nv1/images/generations\n\nImage edit\n\nv1/images/edits\n\nSpeech generation\n\nv1/audio/speech\n\nTranscription\n\nv1/audio/transcriptions\n\nTranslation\n\nv1/audio/translations... Moderation\n\nv1/moderations\n\nCompletions (legacy)\n\nv1/completions\n\nFeatures\n\nStreaming\n\nSupported\n\nFunction calling\n\nSupported\n\nStructured outputs\n\nSupported\n\nFine-tuning\n\nNot supported\n\nDistillation\n\nSupported\n\nTools\n\nTools supported by this model when using the Responses API.\n\nWeb search... Supported\n\nFile search\n\nSupported\n\nImage generation\n\nSupported\n\nCode interpreter\n\nSupported\n\nComputer use\n\nNot supported\n\nMCP\n\nSupported\n\nSnapshots\n\nSnapshots let you lock in a specific version of the model so that performance and behavior remain consistent. Below is a list of all available snapshots and aliases for GPT-5.... gpt-5\n\ngpt-5-2025-08-07\n\ngpt-5-2025-08-07\n\nRate limits\n\nRate limits ensure fair and reliable access to the API by placing specific caps on requests or tokens used within a given time period. Your usage tier determines how high these limits are set and automatically increases as you send more requests and spend more on the API.... |Tier|RPM|TPM|Batch queue limit|\n|--|--|--|--|\n|Free|Not supported|\n|Tier 1|500|30,000|90,000|\n|Tier 2|5,000|450,000|1,350,000|\n|Tier 3|5,000|800,000|100,000,000|... |Tier 4|10,000|2,000,000|200,000,000|\n|Tier 5|15,000|40,000,000|15,000,000,000|"
          },
          {
            "snippetId": "model-details-snippet-2",
            "url": "https://platform.openai.com/docs/models",
            "snippet": "# Models\n\nExplore all available models and compare their capabilities.\n\nFeatured models\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... Frontier models\n\nOpenAI's most advanced models, recommended for most tasks.\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... GPT-4.1\n\nSmartest non-reasoning model\n\nOpen-weight models\n\nOpen-weight models under a permissive Apache 2.0 license.\n\ngpt-oss-120b\n\nMost powerful open-weight model, fits into an H100 GPU\n\ngpt-oss-20b\n\nMedium-sized open-weight model for low latency... Specialized models\n\nPurpose-built for specific tasks.\n\no3-deep-research\n\nOur most powerful deep research model\n\no4-mini-deep-research\n\nFaster, more affordable deep research model\n\nGPT Image 1\n\nState-of-the-art image generation model\n\nDALL·E 3\n\nPrevious generation image generation model... GPT-4o mini TTS\n\nText-to-speech model powered by GPT-4o mini\n\nGPT-4o Transcribe\n\nSpeech-to-text model powered by GPT-4o\n\nGPT-4o mini Transcribe\n\nSpeech-to-text model powered by GPT-4o mini\n\nRealtime and audio models\n\nModels for audio use cases and realtime inputs and outputs.... gpt-realtime\n\nModel capable of realtime text and audio inputs and outputs\n\ngpt-audio\n\nFor audio inputs and outputs with Chat Completions API\n\nChatGPT models\n\nModels used in ChatGPT, not recommended for API use.\n\nGPT-5 Chat\n\nGPT-5 model used in ChatGPT\n\nChatGPT-4o... GPT-4o model used in ChatGPT\n\nAll models\n\nDiverse models for a variety of tasks.\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... o3-deep-research\n\nOur most powerful deep research model\n\no4-mini-deep-research\n\nFaster, more affordable deep research model\n\no3-pro\n\nVersion of o3 with more compute for better responses\n\ngpt-audio\n\nFor audio inputs and outputs with Chat Completions API\n\ngpt-realtime... Model capable of realtime text and audio inputs and outputs\n\no3\n\nReasoning model for complex tasks, succeeded by GPT-5\n\no4-mini\n\nFast, cost-efficient reasoning model, succeeded by GPT-5 mini\n\nGPT-4.1\n\nSmartest non-reasoning model\n\nGPT-4.1 mini... Smaller, faster version of GPT-4.1\n\nGPT-4.1 nano\n\nFastest, most cost-efficient version of GPT-4.1\n\no1-pro\n\nVersion of o1 with more compute for better responses\n\ncomputer-use-preview\n\nSpecialized model for computer use tool\n\nGPT-4o mini Search Preview... Fast, affordable small model for web search\n\nGPT-4o Search Preview\n\nGPT model for web search in Chat Completions\n\nGPT-4.5 Preview (Deprecated)\n\nDeprecated large model.\n\no3-mini\n\nA small model alternative to o3\n\no1\n\nPrevious full o-series reasoning model\n\nomni-moderation... GPT-4 Turbo\n\nAn older high-intelligence GPT model\n\nbabbage-002\n\nReplacement for the GPT-3 ada and babbage base models\n\nChatGPT-4o\n\nGPT-4o model used in ChatGPT\n\ncodex-mini-latest\n\nFast reasoning model optimized for the Codex CLI\n\nDALL·E 2... Our first image generation model\n\nDALL·E 3\n\nPrevious generation image generation model\n\ndavinci-002\n\nReplacement for the GPT-3 curie and davinci base models\n\nGPT-3.5 Turbo\n\nLegacy GPT model for cheaper chat and non-chat tasks\n\nGPT-4\n\nAn older high-intelligence GPT model... GPT-4 Turbo Preview\n\nAn older fast GPT model\n\nGPT-4o mini Transcribe\n\nSpeech-to-text model powered by GPT-4o mini\n\nGPT-4o mini TTS\n\nText-to-speech model powered by GPT-4o mini\n\nGPT-4o Transcribe\n\nSpeech-to-text model powered by GPT-4o... GPT-5 Chat\n\nGPT-5 model used in ChatGPT\n\nGPT Image 1\n\nState-of-the-art image generation model\n\ngpt-oss-120b\n\nMost powerful open-weight model, fits into an H100 GPU\n\ngpt-oss-20b\n\nMedium-sized open-weight model for low latency"
          },
          {
            "snippetId": "model-details-snippet-3",
            "url": "https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-5",
            "snippet": "This documentation is valid for the following list of our models:\n\n`openai/gpt-5-2025-08-07`... ## Model Overview\n\nOpenAI’s most advanced (and the most capable coding) model as of\n\n**August 2025**. It combines a versatile base model for most queries, a deeper reasoning mode (GPT-5 thinking) for complex tasks, and a real-time router that selects the right mode based on context, complexity, tool use, or explicit user instructions (for example, if you say “think hard about this” in the prompt). The new default in ChatGPT web service for signed-in users.... ## How to Make a Call\n\n## Step-by-Step Instructions\n\n1️\n\n** Setup You Can’t Skip**\n\n▪️\n\n**Create an Account**: Visit the AI/ML API website and create an account (if you don’t have one yet).\n\n▪️ **Generate an API Key**: After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.... 2️\n\n** Copy the code example**\n\nAt the bottom of this page, you'll find a code example that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.\n\n3️\n\n** Modify the code example**\n\n▪️ Replace\n\n`<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.... ▪️ Insert your question or request into the\n\n`content` field—this is what the model will respond to.\n\n4️\n\n**(Optional)** ** Adjust other optional parameters if needed**\n\nOnly\n\n`model` and\n\n`messages` are required parameters for this model (and we’ve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model’s behavior. Below, you can find the corresponding API schema, which lists all available parameters along with notes on how to use them.... 5️\n\n** Run your modified code**\n\nRun your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.\n\nIf you need a more detailed walkthrough for setting up your development environment and making a request step by step — feel free to use our Quickstart guide.... If set to True, the model response data will be streamed to the client as it is generated using server-sent events.Default:\n\n`false`tool_choiceany ofOptional... ```\n\n\"background\": false,\n\n\"created_at\": 1762343744,\n\n\"error\": null,\n\n\"id\": \"resp_68963fb142d08197b4d3ae3ad852542c054845c6ea84caa2\",\n\n\"incomplete_details\": null,\n\n\"instructions\": null,\n\n\"max_output_tokens\": null,... \"reasoning\": null,\n\n\"service_tier\": null,\n\n\"status\": \"completed\",\n\n\"temperature\": null,\n\n\"text\": {\n\n\"format\": {\n\n\"type\": \"text\"\n\n\n\n},\n\n\"tool_choice\": null,\n\n\"tools\": null,\n\n\"top_p\": null,\n\n\"truncation\": null,\n\n\"usage\": {\n\n\"input_tokens\": 137,\n\n\"input_tokens_details\": null,\n\n\"output_tokens\": 914,\n\n\"output_tokens_details\": null,\n\n\"total_tokens\": 1051\n\n\n\n```... \"content\": \"Hi! How can I help you today?\",\n\n\"refusal\": null,\n\n\"annotations\": []\n\n\n\n],\n\n\"created\": 1754639960,\n\n\"model\": \"gpt-5-2025-08-07\",\n\n\"usage\": {\n\n\"prompt_tokens\": 18,\n\n\"completion_tokens\": 1722,... \"total_tokens\": 1740,\n\n\"prompt_tokens_details\": {\n\n\"cached_tokens\": 0,\n\n\"audio_tokens\": 0\n\n},\n\n\"completion_tokens_details\": {\n\n\"reasoning_tokens\": 64,\n\n\"audio_tokens\": 0,\n\n\"accepted_prediction_tokens\": 0,\n\n\"rejected_prediction_tokens\": 0\n\n\n\n},\n\n\"system_fingerprint\": null\n\n\n\n```... \"model\": \"gpt-5-2025-08-07\",\n\n\"output\": [\n\n\n\n\"id\": \"rs_689615e28190819682811de8b45da02008df451ae8505013\",\n\n\"type\": \"reasoning\",\n\n\"summary\": []\n\n},\n\n\n\n\"id\": \"msg_689615e715b08196ab92b475f4f3397e08df451ae8505013\",... \"usage\": {\n\n\"input_tokens\": 18,\n\n\"input_tokens_details\": {\n\n\"cached_tokens\": 0\n\n},\n\n\"output_tokens\": 3003,\n\n\"output_tokens_details\": {\n\n\"reasoning_tokens\": 128\n\n},\n\n\"total_tokens\": 3021\n\n},\n\n\"metadata\": {},\n\n\"output_text\": \"Hi! How can I help you today?\"\n\n\n\n```\n\nLast updated\n\nWas this helpful?"
          },
          {
            "snippetId": "model-details-snippet-4",
            "url": "https://en.wikipedia.org/wiki/GPT-5",
            "snippet": "**GPT-5** is a multimodal large language model developed by OpenAI and the fifth in its series of generative pre-trained transformer (GPT) foundation models. Preceded in the series by GPT-4, it was launched on August 7, 2025, combining reasoning capabilities and non-reasoning functionality under a common interface. At its time of release, GPT-5 had state-of-the-art performance on various benchmarks. The model is publicly accessible to users of the chatbot products ChatGPT and Microsoft Copilot as well as to developers through the OpenAI API.... ## Background\n|Part of a series on|\n|--|\n|Machine learningand data mining|\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n\nOn April 14, 2023, Sam Altman, the chief executive officer of OpenAI, spoke at an event at the Massachusetts Institute of Technology and said that the company was not training GPT-5 at that time. He stated that OpenAI was \"prioritizing GPT-4 development\" and that \"we are not and won't for some time\" release GPT-5.... On July 18, OpenAI filed for a \"GPT-5\" trademark in the United States. On November 13, Altman confirmed to the *Financial Times* that the company was working to develop GPT-5.\n\nAccording to *The Information*, \"[f]or much of the second half of 2024, OpenAI was developing a model known internally as Orion and intended to become GPT-5\", \"[b]ut the Orion effort failed to produce a better model, and the company instead released it as GPT-4.5 in February [2025].\"... By late July 2025, OpenAI was widely anticipated as planning to release GPT-5 in early August. On July 30, *The Verge* reported that \"Microsoft is getting ready for GPT-5\" as \"sources familiar with Microsoft's AI plans\" told an editor that the company was testing a new mode for its Copilot chatbot that would offer a model that \"thinks deeply or quickly based on the task\".... On August 5, in the leadup to the release of GPT-5, OpenAI released GPT-OSS, a set of two open-weight models that have reasoning capabilities. GPT-5 was then unveiled during a livestream event on August 7.... In the OpenAI API, developers can access the thinking model, its mini version, and gpt-5-thinking-nano, an even smaller and faster nano version of the thinking model. The version of GPT-5 that is accessible via the API has adjustable reasoning effort (low, medium, high, or minimal) and verbosity (low, medium, or high). Additionally, ChatGPT provides access to gpt-5-thinking with a setting that makes use of parallel test-time compute, referred to as gpt-5-thinking-pro.... GPT-5 is also available in Microsoft Copilot, and Microsoft stated that it will incorporate GPT-5 into a wide variety of its products. According to *9to5Mac*, Apple Inc. is planning to integrate the model into the Apple Intelligence feature in its iOS 26, iPadOS 26, and macOS Tahoe operating systems. It is also accessible via the OpenAI API.... A number of American companies were reported as having received access to GPT-5 ahead of its launch. OpenAI stated that the private health insurance company Oscar Health was checking applications from its policyholders with the model. In addition, Uber was using GPT-5 for its customer support system; GitLab, Windsurf, and Cursor were using the model for software development; and the Spanish bank BBVA was using it for financial analysis. Other companies that OpenAI listed as having used GPT-5 pre-release include Amgen, Lowe's, and Notion.... \"Long-term, this has reinforced that we really need good ways for different users to customize things (we understand that there isn't one model that works for everyone, and we have been investing in steerability research and launched a research preview of different personalities).\" On August 13, 2025, Altman stated on X that OpenAI was working on GPT-5's personality to make the model \"feel warmer\". A corresponding update was rolled out on August 15.... ## See also\n- List of large language models\n\n## References\n\n### Citations\n\n### Sources\n- \"GPT-5 System Card\". OpenAI. August 7, 2025. Retrieved August 7, 2025.\n- \"Introducing GPT-5\". OpenAI. August 7, 2025. Retrieved August 7, 2025.\n- \"Introducing GPT-5 for developers\". OpenAI. August 7, 2025. Retrieved August 7, 2025.\n\n## External links\n- Official website"
          },
          {
            "snippetId": "model-details-snippet-5",
            "url": "https://cirra.ai/articles/pdfs/gpt-5-technical-overview.pdf",
            "snippet": "Features\nPublished August 17, 2025\n70 min read\nOpenAI GPT-5: A Comprehensive Technical\nOverview\n1. Introduction and Evolution of GPT-5\nOpenAI’s GPT-5 is the latest generation in the GPT series of large language models, officially released on\nAugust 7, 2025 (Source: botpress.com). This launch comes over two years after GPT-4’s debut in March... 2023, reflecting OpenAI’s cautious, safety-focused approach to deploying more powerful AI systems\n(Source: ainvest.com). In the interim, OpenAI introduced GPT-4.5 (codename “Orion”) as a research\npreview in early 2025 to bridge the gap (Source: botpress.com). GPT-4.5 scaled up the base model and... hinted at new “chain-of-thought” reasoning capabilities (Source: openai.com)(Source: botpress.com),\npaving the way for GPT-5’s more advanced logical reasoning and multimodal features. GPT-5 arrives as\nGPT-5: A Technical Analysis of Its Evolution & Features\nPage 1 of 33... the culmination of the GPT series’ evolution – from the 2018-era GPT-1 and GPT-2 models, through GPT-\n3’s massive 175 billion parameter jump in 2020, to GPT-4’s multimodal understanding in 2023 – each\ngeneration expanding in scale and capability.\nGPT-5’s release context: CEO Sam Altman has described GPT-5 as “a significant step along the path to... alignment were paramount – Altman had stated the model would only be released once it was deemed\nsafe and ready, underscoring OpenAI’s commitment to ethical AI development (Source: ainvest.com)\n(Source: ainvest.com). Thus, GPT-5’s launch represents not just a technical milestone, but also a... measured step forward within the company’s charter of developing advanced AI responsibly.\n2. Architecture and Training Innovations\nOne of GPT-5’s most noteworthy innovations is its multi-model architecture with a built-in controller.\nInstead of a single monolithic model handling all queries, GPT-5 is implemented as a “unified system”... requests using the lightweight model, but invoke rigorous step-by-step reasoning with the heavier model\non challenging tasks – all transparently within one system.\nBuilt-in reasoning: GPT-5’s “thinking” model explicitly performs internal chain-of-thought computations\n(much like an expert breaking down a problem), a capability inspired by OpenAI’s prior “o-series”... versions. Notably, in the API, developers can access GPT-5 at three scales – gpt-5 , gpt-5-mini , and\ngpt-5-nano  – to trade off cost vs. performance, and can even toggle the model’s reasoning_effort\nparameter to control how much “thinking time” it uses (Source: openai.com)(Source: openai.com).... Architecture details: Under the hood, GPT-5 remains a transformer-based neural network but with\nsignificant enhancements. It is multimodal by design – capable of processing not only text but also\nimages (and possibly audio/voice input) in a unified model. OpenAI’s documentation suggests modality-\nspecific encoders feed into a common transformer backbone (Source: medium.com), enabling joint... private or toxic information (Source: medium.com). The training process combined unsupervised pre-\ntraining at scale with extensive fine-tuning for reasoning and alignment. OpenAI substantially\nincreased training compute for GPT-5, leveraging Microsoft Azure’s AI supercomputer infrastructure\n(Source: openai.com). Architecture and optimization improvements (some likely proprietary) were... straightforward questions don’t always invoke the full heavyweight model, yielding faster and more cost-\nGPT-5: A Technical Analysis of Its Evolution & Features\nPage 3 of 33... integration (Google’s search, etc.). At the time of GPT-5’s release, Google had various versions like\nGemini 1.5 and an upcoming Gemini 2.5. On paper, Gemini matches GPT-5 in multimodal\ncapabilities – it too is designed to handle text and images and even generate some forms of output... design philosophy: Google leverages its data advantage for real-time AI services, whereas OpenAI\nfocuses on model-centric intelligence and safety. In practice, no single model outshines the other\nuniversally – each has niches. For instance, a user might prefer Gemini for tasks like “Analyze today’s\nstock market movements” (where live data is crucial), but choose GPT-5 for “Explain a complex... researchers), meaning organizations can fine-tune it on their own data or modify it – an advantage in\nscenarios where control and privacy are needed. GPT-5, being proprietary, does not offer full weights\nfor local fine-tuning, though OpenAI provides some customization APIs. LLaMA’s colossal 1M token"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Model overview",
            "score": 1,
            "explanation": "OpenAI’s most advanced (and the most capable coding) model as of August 2025."
          },
          {
            "name": "Organization developing the model",
            "score": 1,
            "explanation": "GPT-5 is a multimodal large language model developed by OpenAI."
          },
          {
            "name": "Model Version",
            "score": 1,
            "explanation": "GPT-5 is the latest generation in the GPT series of large language models."
          },
          {
            "name": "Model Release Date",
            "score": 1,
            "explanation": "GPT-5 was launched on August 7, 2025."
          },
          {
            "name": "Model Version Progression",
            "score": 1,
            "explanation": "Preceded in the series by GPT-4, it was launched on August 7, 2025."
          },
          {
            "name": "Model Architecture",
            "score": 1,
            "explanation": "One of GPT-5’s most noteworthy innovations is its multi-model architecture with a built-in controller."
          },
          {
            "name": "Model Dependencies",
            "score": 1,
            "explanation": "OpenAI substantially increased training compute for GPT-5, leveraging Microsoft Azure’s AI supercomputer infrastructure."
          },
          {
            "name": "Paper and relevant links",
            "score": 1,
            "explanation": "Sources include 'Introducing GPT-5' and 'GPT-5 System Card'."
          },
          {
            "name": "Model Distribution Forms",
            "score": 1,
            "explanation": "GPT-5 is publicly accessible to users of the chatbot products ChatGPT and Microsoft Copilot."
          }
        ]
      },
      {
        "sectionId": "model-inputs-outputs",
        "sectionSnippets": [
          {
            "snippetId": "model-inputs-outputs-snippet-1",
            "url": "https://platform.openai.com/docs/models/gpt-5",
            "snippet": "Models\n\nGPT-5\n\nDefault\n\nThe best model for coding and agentic tasks across domains\n\nThe best model for coding and agentic tasks across domains\n\nReasoning\n\nHigher\n\nSpeed\n\nMedium\n\nPrice\n\n$1.25 • $10\n\nInput • Output\n\nInput\n\nText, image\n\nOutput\n\nText... GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. Learn more in our GPT-5 usage guide.\n\n400,000 context window\n\n128,000 max output tokens\n\nSep 30, 2024 knowledge cutoff\n\nReasoning token support\n\nPricing\n\nPricing is based on the number of tokens used. For tool-specific models, like search and computer use, there's a fee per tool call. See details in the pricing page.... Text tokens\n\nPer 1M tokens\n\n\n\nBatch API price\n\nInput\n\n$1.25\n\nCached input\n\n$0.125\n\nOutput\n\n$10.00\n\nQuick comparison\n\nInput\n\nCached input\n\nOutput\n\nGPT-5\n\n$1.25\n\nGPT-5 mini\n\n$0.25\n\nGPT-5 nano... $0.05\n\nModalities\n\nText\n\nInput and output\n\nImage\n\nInput only\n\nAudio\n\nNot supported\n\nEndpoints\n\nChat Completions\n\nv1/chat/completions\n\nResponses\n\nv1/responses\n\nRealtime\n\nv1/realtime\n\nAssistants\n\nv1/assistants\n\nBatch\n\nv1/batch... Fine-tuning\n\nv1/fine-tuning\n\nEmbeddings\n\nv1/embeddings\n\nImage generation\n\nv1/images/generations\n\nImage edit\n\nv1/images/edits\n\nSpeech generation\n\nv1/audio/speech\n\nTranscription\n\nv1/audio/transcriptions\n\nTranslation\n\nv1/audio/translations... Moderation\n\nv1/moderations\n\nCompletions (legacy)\n\nv1/completions\n\nFeatures\n\nStreaming\n\nSupported\n\nFunction calling\n\nSupported\n\nStructured outputs\n\nSupported\n\nFine-tuning\n\nNot supported\n\nDistillation\n\nSupported\n\nTools\n\nTools supported by this model when using the Responses API.\n\nWeb search... Supported\n\nFile search\n\nSupported\n\nImage generation\n\nSupported\n\nCode interpreter\n\nSupported\n\nComputer use\n\nNot supported\n\nMCP\n\nSupported\n\nSnapshots\n\nSnapshots let you lock in a specific version of the model so that performance and behavior remain consistent. Below is a list of all available snapshots and aliases for GPT-5.... gpt-5\n\ngpt-5-2025-08-07\n\ngpt-5-2025-08-07\n\nRate limits\n\nRate limits ensure fair and reliable access to the API by placing specific caps on requests or tokens used within a given time period. Your usage tier determines how high these limits are set and automatically increases as you send more requests and spend more on the API.... |Tier|RPM|TPM|Batch queue limit|\n|--|--|--|--|\n|Free|Not supported|\n|Tier 1|500|30,000|90,000|\n|Tier 2|5,000|450,000|1,350,000|\n|Tier 3|5,000|800,000|100,000,000|... |Tier 4|10,000|2,000,000|200,000,000|\n|Tier 5|15,000|40,000,000|15,000,000,000|"
          },
          {
            "snippetId": "model-inputs-outputs-snippet-2",
            "url": "https://platform.openai.com/docs/models",
            "snippet": "# Models\n\nExplore all available models and compare their capabilities.\n\nFeatured models\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... Frontier models\n\nOpenAI's most advanced models, recommended for most tasks.\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... GPT-4.1\n\nSmartest non-reasoning model\n\nOpen-weight models\n\nOpen-weight models under a permissive Apache 2.0 license.\n\ngpt-oss-120b\n\nMost powerful open-weight model, fits into an H100 GPU\n\ngpt-oss-20b\n\nMedium-sized open-weight model for low latency... GPT-4o mini TTS\n\nText-to-speech model powered by GPT-4o mini\n\nGPT-4o Transcribe\n\nSpeech-to-text model powered by GPT-4o\n\nGPT-4o mini Transcribe\n\nSpeech-to-text model powered by GPT-4o mini\n\nRealtime and audio models\n\nModels for audio use cases and realtime inputs and outputs.... gpt-realtime\n\nModel capable of realtime text and audio inputs and outputs\n\ngpt-audio\n\nFor audio inputs and outputs with Chat Completions API\n\nChatGPT models\n\nModels used in ChatGPT, not recommended for API use.\n\nGPT-5 Chat\n\nGPT-5 model used in ChatGPT\n\nChatGPT-4o... GPT-4o model used in ChatGPT\n\nAll models\n\nDiverse models for a variety of tasks.\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... o3-deep-research\n\nOur most powerful deep research model\n\no4-mini-deep-research\n\nFaster, more affordable deep research model\n\no3-pro\n\nVersion of o3 with more compute for better responses\n\ngpt-audio\n\nFor audio inputs and outputs with Chat Completions API\n\ngpt-realtime... Model capable of realtime text and audio inputs and outputs\n\no3\n\nReasoning model for complex tasks, succeeded by GPT-5\n\no4-mini\n\nFast, cost-efficient reasoning model, succeeded by GPT-5 mini\n\nGPT-4.1\n\nSmartest non-reasoning model\n\nGPT-4.1 mini... Smaller, faster version of GPT-4.1\n\nGPT-4.1 nano\n\nFastest, most cost-efficient version of GPT-4.1\n\no1-pro\n\nVersion of o1 with more compute for better responses\n\ncomputer-use-preview\n\nSpecialized model for computer use tool\n\nGPT-4o mini Search Preview... Fast, affordable small model for web search\n\nGPT-4o Search Preview\n\nGPT model for web search in Chat Completions\n\nGPT-4.5 Preview (Deprecated)\n\nDeprecated large model.\n\no3-mini\n\nA small model alternative to o3\n\no1\n\nPrevious full o-series reasoning model\n\nomni-moderation... GPT-4 Turbo\n\nAn older high-intelligence GPT model\n\nbabbage-002\n\nReplacement for the GPT-3 ada and babbage base models\n\nChatGPT-4o\n\nGPT-4o model used in ChatGPT\n\ncodex-mini-latest\n\nFast reasoning model optimized for the Codex CLI\n\nDALL·E 2... GPT-4 Turbo Preview\n\nAn older fast GPT model\n\nGPT-4o mini Transcribe\n\nSpeech-to-text model powered by GPT-4o mini\n\nGPT-4o mini TTS\n\nText-to-speech model powered by GPT-4o mini\n\nGPT-4o Transcribe\n\nSpeech-to-text model powered by GPT-4o... GPT-5 Chat\n\nGPT-5 model used in ChatGPT\n\nGPT Image 1\n\nState-of-the-art image generation model\n\ngpt-oss-120b\n\nMost powerful open-weight model, fits into an H100 GPU\n\ngpt-oss-20b\n\nMedium-sized open-weight model for low latency... text-embedding-3-large\n\nMost capable embedding model\n\ntext-embedding-3-small\n\nSmall embedding model\n\ntext-embedding-ada-002\n\nOlder embedding model\n\ntext-moderation\n\nDeprecated\n\nPrevious generation text-only moderation model\n\ntext-moderation-stable\n\nDeprecated\n\nPrevious generation text-only moderation model... TTS-1\n\nText-to-speech model optimized for speed\n\nTTS-1 HD\n\nText-to-speech model optimized for quality\n\nWhisper\n\nGeneral-purpose speech recognition model"
          },
          {
            "snippetId": "model-inputs-outputs-snippet-3",
            "url": "https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-5",
            "snippet": "This documentation is valid for the following list of our models:\n\n`openai/gpt-5-2025-08-07`... ## Model Overview\n\nOpenAI’s most advanced (and the most capable coding) model as of\n\n**August 2025**. It combines a versatile base model for most queries, a deeper reasoning mode (GPT-5 thinking) for complex tasks, and a real-time router that selects the right mode based on context, complexity, tool use, or explicit user instructions (for example, if you say “think hard about this” in the prompt). The new default in ChatGPT web service for signed-in users.... ## How to Make a Call\n\n## Step-by-Step Instructions\n\n1️\n\n** Setup You Can’t Skip**\n\n▪️\n\n**Create an Account**: Visit the AI/ML API website and create an account (if you don’t have one yet).\n\n▪️ **Generate an API Key**: After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.... ## API Schema\n\nAn upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.max_tokensnumber · min: 1Optional\n\nThe maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.streambooleanOptional... \"end_index\": 1,\n\n\"start_index\": 1,\n\n\"title\": \"text\",\n\n\"url\": \"text\"\n\n\n\n],\n\n\"audio\": {\n\n\"id\": \"text\",\n\n\"data\": \"text\",\n\n\"transcript\": \"text\",\n\n\"expires_at\": 1\n\n},\n\n\"tool_calls\": [\n\n\n\n\"id\": \"text\",... 1\n\n],\n\n\"logprob\": 1,\n\n\"token\": \"text\"\n\n\n\n],\n\n\"refusal\": []\n\n\n\n],\n\n\"model\": \"text\",\n\n\"usage\": {\n\n\"prompt_tokens\": 1,\n\n\"completion_tokens\": 1,\n\n\"total_tokens\": 1,\n\n\"completion_tokens_details\": {\n\n\"accepted_prediction_tokens\": 1,\n\n\"audio_tokens\": 1,\n\n\"reasoning_tokens\": 1,\n\n\"rejected_prediction_tokens\": 1\n\n},\n\n\"prompt_tokens_details\": {\n\n\"audio_tokens\": 1,\n\n\"cached_tokens\": 1\n\n\n\n```... Text, image, or file inputs to the model, used to generate a response.stringOptional\n\nA text input to the model, equivalent to a text input with the user role.ororororororororororororormax_output_tokensintegerOptional\n\nAn upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens.previous_response_idstring | nullableOptional... ```\n\n\"background\": false,\n\n\"created_at\": 1762343744,\n\n\"error\": null,\n\n\"id\": \"resp_68963fb142d08197b4d3ae3ad852542c054845c6ea84caa2\",\n\n\"incomplete_details\": null,\n\n\"instructions\": null,\n\n\"max_output_tokens\": null,... \"reasoning\": null,\n\n\"service_tier\": null,\n\n\"status\": \"completed\",\n\n\"temperature\": null,\n\n\"text\": {\n\n\"format\": {\n\n\"type\": \"text\"\n\n\n\n},\n\n\"tool_choice\": null,\n\n\"tools\": null,\n\n\"top_p\": null,\n\n\"truncation\": null,\n\n\"usage\": {\n\n\"input_tokens\": 137,\n\n\"input_tokens_details\": null,\n\n\"output_tokens\": 914,\n\n\"output_tokens_details\": null,\n\n\"total_tokens\": 1051\n\n\n\n```... \"content\": \"Hi! How can I help you today?\",\n\n\"refusal\": null,\n\n\"annotations\": []\n\n\n\n],\n\n\"created\": 1754639960,\n\n\"model\": \"gpt-5-2025-08-07\",\n\n\"usage\": {\n\n\"prompt_tokens\": 18,\n\n\"completion_tokens\": 1722,... \"total_tokens\": 1740,\n\n\"prompt_tokens_details\": {\n\n\"cached_tokens\": 0,\n\n\"audio_tokens\": 0\n\n},\n\n\"completion_tokens_details\": {\n\n\"reasoning_tokens\": 64,\n\n\"audio_tokens\": 0,\n\n\"accepted_prediction_tokens\": 0,\n\n\"rejected_prediction_tokens\": 0\n\n\n\n},\n\n\"system_fingerprint\": null\n\n\n\n```... ## Response\n\n```\n\n\"id\": \"resp_689615e09cbc819691bdcfe813d70ef008df451ae8505013\",\n\n\"object\": \"response\",\n\n\"created_at\": 1754666464,\n\n\"error\": null,\n\n\"incomplete_details\": null,\n\n\"instructions\": null,\n\n\"max_output_tokens\": 512,... \"usage\": {\n\n\"input_tokens\": 18,\n\n\"input_tokens_details\": {\n\n\"cached_tokens\": 0\n\n},\n\n\"output_tokens\": 3003,\n\n\"output_tokens_details\": {\n\n\"reasoning_tokens\": 128\n\n},\n\n\"total_tokens\": 3021\n\n},\n\n\"metadata\": {},\n\n\"output_text\": \"Hi! How can I help you today?\"\n\n\n\n```\n\nLast updated\n\nWas this helpful?"
          },
          {
            "snippetId": "model-inputs-outputs-snippet-4",
            "url": "https://www.prompthub.us/models/gpt-5",
            "snippet": "GPT-5 is OpenAI's flagship model for coding, reasoning, and agentic tasks across domains\n\nThe company that provides the model\n\nThe number of tokens you can send in a prompt\n\nThe maximum number of tokens a model can generate in one request\n\nThe cost of prompt tokens sent to the model\n\nThe cost of output tokens generated by the model... When the model's knowledge ends\n\nWhen the model was launched\n\nCapability for the model to use external tools\n\nAbility to process and analyze visual inputs, like images\n\nSupport for multiple languages\n\nWhether the model supports fine-tuning on custom datasets\n\nGPT-5 is OpenAI’s flagship model designed for coding, complex reasoning, and agentic task execution across diverse domains.... It costs $1.25 per million tokens for input and $10.00 per million tokens for output.\n\nGPT-5 supports a context window of up to 400,000 tokens.\n\nIt can generate up to 128,000 tokens in a single response.\n\nGPT-5 was released on August 7, 2025.... Its knowledge cut-off date is September 30, 2024.\n\nYes, GPT-5 supports multimodal inputs, including images.\n\nYes, GPT-5 supports function calling and integration with external tools.\n\nYes, it handles multiple languages for both input and output.\n\nYes, GPT-5 supports fine-tuning.\n\nYou can learn more here.... Collaborate with thousands of AI builders to discover, manage, and improve prompts—free to get started."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-5",
            "url": "https://www.scribd.com/document/898922774/Introducing-GPT-5-for-Developers-OpenAI",
            "snippet": "Introducing GPT‑5 for Developers _ OpenAI\n\nAI-enhanced description\n\nIntroducing GPT‑5 for developers The best model for coding and agentic tasks.\n\nIntroduction Today, we’re releasing GPT‑5 in our API platform—our best model yet for coding and agentic tasks.... In COLLIE, models must write text that meets various constraints. In Scale MultiChallenge, models are challenged on multi-turn conversations to properly use four types of information from previous messages. Our scores come from using o3‑mini as a grader, which was more accurate than GPT‑4o. In our internal OpenAI API instruction following eval, models must follow difficult instructions derived from real developer feedback. Reasoning models were run with high reasoning effort.... Tool calling\n\nIn τ2-bench, a model must use tools to accomplish a customer service task, where there may be a user who can communicate and can take actions on the world state. Reasoning models were run with high reasoning effort.\n\nhttps://openai.com/index/introducing-gpt-5-for-developers/ 8/168/8/25, 12:12 PM Introducing GPT‑5 for developers | OpenAI... In the API, all GPT‑5 models can accept a maximum of 272,000 input tokens and emit a maximum of 128,000 reasoning & output tokens, for a total context length of 400,000 tokens.\n\nFactuality\n\nhttps://openai.com/index/introducing-gpt-5-for-developers/ 9/168/8/25, 12:12 PM Introducing GPT‑5 for developers | OpenAI... https://openai.com/index/introducing-gpt-5-for-developers/ 11/168/8/25, 12:12 PM Introducing GPT‑5 for developers | OpenAI progress to the user, helping end users understand its approach and intent behind the tool calls.\n\nCustom tools... https://openai.com/index/introducing-gpt-5-for-developers/ 12/168/8/25, 12:12 PM Introducing GPT‑5 for developers | OpenAI\n\nDetailed benchmarks Intelligence GPT-5 GPT-5 OpenAI OpenAI GPT-4.1 GPT-4.1 GPT-5 mini nano o3 o4-mini (high) (high) (high) (high) (high) GPT-4.1 mini nano... Multimodal GPT-5 GPT-5 OpenAI OpenAI GPT-4.1 GPT-4.1 GPT-5 mini nano o3 o4-mini (high) (high) (high) (high) (high) GPT-4.1 mini nano\n\nCoding GPT-5 GPT-5 OpenAI OpenAI GPT-4.1 GPT-4.1 GPT-5 mini nano o3 o4-mini (high) (high) (high) (high) (high) GPT-4.1 mini nano... https://openai.com/index/introducing-gpt-5-for-developers/ 13/168/8/25, 12:12 PM Introducing GPT‑5 for developers | OpenAI GPT-5 GPT-5 OpenAI OpenAI GPT-4.1 GPT-4.1 GPT-5 mini nano o3 o4-mini (high) (high) (high) (high) (high) GPT-4.1 mini nano... Instruction Following GPT-5 GPT-5 OpenAI OpenAI GPT-4.1 GPT-4.1 GPT-5 mini nano o3 o4-mini (high) (high) (high) (high) (high) GPT-4.1 mini nano\n\nInternal API 64.0% 65.8% 56.1% 47.4% 44.7% 49.1% 45.1% 31.6% instruction following eval (hard)... Long Context GPT-5 GPT-5 OpenAI OpenAI GPT-4.1 GPT-4.1 GPT-5 mini nano o3 o4-mini (high) (high) (high) (high) (high) GPT-4.1 mini nano... https://openai.com/index/introducing-gpt-5-for-developers/ 14/168/8/25, 12:12 PM Introducing GPT‑5 for developers | OpenAI GPT-5 GPT-5 OpenAI OpenAI GPT-4.1 GPT-4.1 GPT-5 mini nano o3 o4-mini (high) (high) (high) (high) (high) GPT-4.1 mini nano... Sora Features Support Pricing Safety Help Center Sora log in Safety Approach More Security & Privacy API Platform News Trust & Transparency Platform Overview Stories Pricing Livestreams API log in Podcast Documentation\n\nDeveloper Forum\n\nhttps://openai.com/index/introducing-gpt-5-for-developers/ 16/16"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Inputs",
            "score": 1,
            "explanation": "GPT-5 supports multimodal inputs, including images."
          },
          {
            "name": "Outputs",
            "score": 1,
            "explanation": "It can generate up to 128,000 tokens in a single response."
          },
          {
            "name": "Token Count",
            "score": 1,
            "explanation": "GPT-5 supports a context window of up to 400,000 tokens."
          }
        ]
      },
      {
        "sectionId": "model-data",
        "sectionSnippets": [
          {
            "snippetId": "model-data-snippet-1",
            "url": "https://platform.openai.com/docs/models/gpt-5",
            "snippet": "Models\n\nGPT-5\n\nDefault\n\nThe best model for coding and agentic tasks across domains\n\nThe best model for coding and agentic tasks across domains\n\nReasoning\n\nHigher\n\nSpeed\n\nMedium\n\nPrice\n\n$1.25 • $10\n\nInput • Output\n\nInput\n\nText, image\n\nOutput\n\nText... GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. Learn more in our GPT-5 usage guide.\n\n400,000 context window\n\n128,000 max output tokens\n\nSep 30, 2024 knowledge cutoff\n\nReasoning token support\n\nPricing\n\nPricing is based on the number of tokens used. For tool-specific models, like search and computer use, there's a fee per tool call. See details in the pricing page.... Text tokens\n\nPer 1M tokens\n\n\n\nBatch API price\n\nInput\n\n$1.25\n\nCached input\n\n$0.125\n\nOutput\n\n$10.00\n\nQuick comparison\n\nInput\n\nCached input\n\nOutput\n\nGPT-5\n\n$1.25\n\nGPT-5 mini\n\n$0.25\n\nGPT-5 nano... $0.05\n\nModalities\n\nText\n\nInput and output\n\nImage\n\nInput only\n\nAudio\n\nNot supported\n\nEndpoints\n\nChat Completions\n\nv1/chat/completions\n\nResponses\n\nv1/responses\n\nRealtime\n\nv1/realtime\n\nAssistants\n\nv1/assistants\n\nBatch\n\nv1/batch... Fine-tuning\n\nv1/fine-tuning\n\nEmbeddings\n\nv1/embeddings\n\nImage generation\n\nv1/images/generations\n\nImage edit\n\nv1/images/edits\n\nSpeech generation\n\nv1/audio/speech\n\nTranscription\n\nv1/audio/transcriptions\n\nTranslation\n\nv1/audio/translations... Moderation\n\nv1/moderations\n\nCompletions (legacy)\n\nv1/completions\n\nFeatures\n\nStreaming\n\nSupported\n\nFunction calling\n\nSupported\n\nStructured outputs\n\nSupported\n\nFine-tuning\n\nNot supported\n\nDistillation\n\nSupported\n\nTools\n\nTools supported by this model when using the Responses API.\n\nWeb search... Supported\n\nFile search\n\nSupported\n\nImage generation\n\nSupported\n\nCode interpreter\n\nSupported\n\nComputer use\n\nNot supported\n\nMCP\n\nSupported\n\nSnapshots\n\nSnapshots let you lock in a specific version of the model so that performance and behavior remain consistent. Below is a list of all available snapshots and aliases for GPT-5.... gpt-5\n\ngpt-5-2025-08-07\n\ngpt-5-2025-08-07\n\nRate limits\n\nRate limits ensure fair and reliable access to the API by placing specific caps on requests or tokens used within a given time period. Your usage tier determines how high these limits are set and automatically increases as you send more requests and spend more on the API.... |Tier|RPM|TPM|Batch queue limit|\n|--|--|--|--|\n|Free|Not supported|\n|Tier 1|500|30,000|90,000|\n|Tier 2|5,000|450,000|1,350,000|\n|Tier 3|5,000|800,000|100,000,000|... |Tier 4|10,000|2,000,000|200,000,000|\n|Tier 5|15,000|40,000,000|15,000,000,000|"
          },
          {
            "snippetId": "model-data-snippet-2",
            "url": "https://platform.openai.com/docs/models",
            "snippet": "# Models\n\nExplore all available models and compare their capabilities.\n\nFeatured models\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... Frontier models\n\nOpenAI's most advanced models, recommended for most tasks.\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... GPT-4.1\n\nSmartest non-reasoning model\n\nOpen-weight models\n\nOpen-weight models under a permissive Apache 2.0 license.\n\ngpt-oss-120b\n\nMost powerful open-weight model, fits into an H100 GPU\n\ngpt-oss-20b\n\nMedium-sized open-weight model for low latency... Specialized models\n\nPurpose-built for specific tasks.\n\no3-deep-research\n\nOur most powerful deep research model\n\no4-mini-deep-research\n\nFaster, more affordable deep research model\n\nGPT Image 1\n\nState-of-the-art image generation model\n\nDALL·E 3\n\nPrevious generation image generation model... GPT-4o mini TTS\n\nText-to-speech model powered by GPT-4o mini\n\nGPT-4o Transcribe\n\nSpeech-to-text model powered by GPT-4o\n\nGPT-4o mini Transcribe\n\nSpeech-to-text model powered by GPT-4o mini\n\nRealtime and audio models\n\nModels for audio use cases and realtime inputs and outputs.... gpt-realtime\n\nModel capable of realtime text and audio inputs and outputs\n\ngpt-audio\n\nFor audio inputs and outputs with Chat Completions API\n\nChatGPT models\n\nModels used in ChatGPT, not recommended for API use.\n\nGPT-5 Chat\n\nGPT-5 model used in ChatGPT\n\nChatGPT-4o... GPT-4o model used in ChatGPT\n\nAll models\n\nDiverse models for a variety of tasks.\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... o3-deep-research\n\nOur most powerful deep research model\n\no4-mini-deep-research\n\nFaster, more affordable deep research model\n\no3-pro\n\nVersion of o3 with more compute for better responses\n\ngpt-audio\n\nFor audio inputs and outputs with Chat Completions API\n\ngpt-realtime... Model capable of realtime text and audio inputs and outputs\n\no3\n\nReasoning model for complex tasks, succeeded by GPT-5\n\no4-mini\n\nFast, cost-efficient reasoning model, succeeded by GPT-5 mini\n\nGPT-4.1\n\nSmartest non-reasoning model\n\nGPT-4.1 mini... Smaller, faster version of GPT-4.1\n\nGPT-4.1 nano\n\nFastest, most cost-efficient version of GPT-4.1\n\no1-pro\n\nVersion of o1 with more compute for better responses\n\ncomputer-use-preview\n\nSpecialized model for computer use tool\n\nGPT-4o mini Search Preview... Fast, affordable small model for web search\n\nGPT-4o Search Preview\n\nGPT model for web search in Chat Completions\n\nGPT-4.5 Preview (Deprecated)\n\nDeprecated large model.\n\no3-mini\n\nA small model alternative to o3\n\no1\n\nPrevious full o-series reasoning model\n\nomni-moderation... GPT-4 Turbo\n\nAn older high-intelligence GPT model\n\nbabbage-002\n\nReplacement for the GPT-3 ada and babbage base models\n\nChatGPT-4o\n\nGPT-4o model used in ChatGPT\n\ncodex-mini-latest\n\nFast reasoning model optimized for the Codex CLI\n\nDALL·E 2... GPT-4 Turbo Preview\n\nAn older fast GPT model\n\nGPT-4o mini Transcribe\n\nSpeech-to-text model powered by GPT-4o mini\n\nGPT-4o mini TTS\n\nText-to-speech model powered by GPT-4o mini\n\nGPT-4o Transcribe\n\nSpeech-to-text model powered by GPT-4o... GPT-5 Chat\n\nGPT-5 model used in ChatGPT\n\nGPT Image 1\n\nState-of-the-art image generation model\n\ngpt-oss-120b\n\nMost powerful open-weight model, fits into an H100 GPU\n\ngpt-oss-20b\n\nMedium-sized open-weight model for low latency... TTS-1\n\nText-to-speech model optimized for speed\n\nTTS-1 HD\n\nText-to-speech model optimized for quality\n\nWhisper\n\nGeneral-purpose speech recognition model"
          },
          {
            "snippetId": "model-data-snippet-3",
            "url": "https://en.wikipedia.org/wiki/GPT-5",
            "snippet": "**GPT-5** is a multimodal large language model developed by OpenAI and the fifth in its series of generative pre-trained transformer (GPT) foundation models. Preceded in the series by GPT-4, it was launched on August 7, 2025, combining reasoning capabilities and non-reasoning functionality under a common interface. At its time of release, GPT-5 had state-of-the-art performance on various benchmarks. The model is publicly accessible to users of the chatbot products ChatGPT and Microsoft Copilot as well as to developers through the OpenAI API.... ## Background\n|Part of a series on|\n|--|\n|Machine learningand data mining|\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n| |\n\nOn April 14, 2023, Sam Altman, the chief executive officer of OpenAI, spoke at an event at the Massachusetts Institute of Technology and said that the company was not training GPT-5 at that time. He stated that OpenAI was \"prioritizing GPT-4 development\" and that \"we are not and won't for some time\" release GPT-5.... On August 5, in the leadup to the release of GPT-5, OpenAI released GPT-OSS, a set of two open-weight models that have reasoning capabilities. GPT-5 was then unveiled during a livestream event on August 7.... ## Capabilities\n\nAt the time of its release, GPT-5 had state-of-the-art performance on benchmarks that test mathematics, programming, finance, and multimodal understanding. According to OpenAI, improvements over its predecessor models include faster response times, better coding and writing skills, more accurate answers to health questions, and lower levels of hallucination.... Also, compared to previous models, GPT-5 aims to give safe, high-level responses to potentially harmful queries rather than outright declining them, an approach that OpenAI refers to as \"safe completions\", aiming to result \"in GPT-5 being able to refuse more unsafe questions, while offering fewer rejections to users seeking harmless information.\" In addition, GPT-5 was trained to give more critical, \"less effusively agreeable\" answers compared to its predecessor models.... In the OpenAI API, developers can access the thinking model, its mini version, and gpt-5-thinking-nano, an even smaller and faster nano version of the thinking model. The version of GPT-5 that is accessible via the API has adjustable reasoning effort (low, medium, high, or minimal) and verbosity (low, medium, or high). Additionally, ChatGPT provides access to gpt-5-thinking with a setting that makes use of parallel test-time compute, referred to as gpt-5-thinking-pro.... ## Training\n\nAccording to AIMultiple, GPT-5 is natively multimodal, meaning that it was trained from scratch on multiple modalities (like text and images) at once without relying on already-trained language or vision models. Its training process involved three stages: unsupervised pretraining, supervised fine-tuning, and reinforcement learning from human feedback. Pretraining used a large-scale multilingual dataset of books, articles, web pages, academic papers, and licensed sources. GPT-5's visual and text capabilities were described as having been developed alongside each other throughout training, unlike with GPT-4.... GPT-5 is also available in Microsoft Copilot, and Microsoft stated that it will incorporate GPT-5 into a wide variety of its products. According to *9to5Mac*, Apple Inc. is planning to integrate the model into the Apple Intelligence feature in its iOS 26, iPadOS 26, and macOS Tahoe operating systems. It is also accessible via the OpenAI API.... \"Long-term, this has reinforced that we really need good ways for different users to customize things (we understand that there isn't one model that works for everyone, and we have been investing in steerability research and launched a research preview of different personalities).\" On August 13, 2025, Altman stated on X that OpenAI was working on GPT-5's personality to make the model \"feel warmer\". A corresponding update was rolled out on August 15.... ## See also\n- List of large language models\n\n## References\n\n### Citations\n\n### Sources\n- \"GPT-5 System Card\". OpenAI. August 7, 2025. Retrieved August 7, 2025.\n- \"Introducing GPT-5\". OpenAI. August 7, 2025. Retrieved August 7, 2025.\n- \"Introducing GPT-5 for developers\". OpenAI. August 7, 2025. Retrieved August 7, 2025.\n\n## External links\n- Official website"
          },
          {
            "snippetId": "model-data-snippet-4",
            "url": "https://lifearchitect.ai/gpt-5/",
            "snippet": "GPT-5 training hasn’t gone smoothly, but to secure funding, it released o1. OpenAI still holds many cards and isn’t in a rush to play them. Each time it unveils a new card, global tech companies, including those in China, observe closely, speculate, and develop competing solutions. Even if they can’t match it entirely, they can achieve 80-90% parity. Because of this, OpenAI doesn’t want to exhaust its cards prematurely. It plans to save them for when AGI seems within reach and can confidently deploy them.... the challenges GPT-5 has faced during trainingsuggest that the prediction of achieving AGI in three years might be overly optimistic. GPT-5 was originally supposed to be released by now, but at this rate, even if it does launch, it won’t be for another six months [Nov/2024 +6m = May/2025] at the earliest.... Some of the early things that I’m seeing right now with the new models [GPT-5] is maybe this could be the thing that could pass your qualifying exams when you’re a PhD student.\n\n…everybody’s likely gonna be impressed by some of the reasoning breakthroughs that will happen… but the real test will be what we choose to go do with it…... **4/Apr/2024: Unverified rumor: ‘Red teaming for GPT-5 underway.’** (Twitter)\n\n**29/Mar/2024: GPT-5 ‘coming soon’** to Double, an AI coding assistant backed by Y Combinator.\n\n**26/Mar/2024: GPT-6 preparation beginning: LifeArchitect.ai/GPT-6**... For safety, I expect the GPT-5 public release date to be after the November 2024 US elections.\n\n**2/Nov/2023: US military comments on GPT-5 and industry (2/Nov/2023, PDF):**\n\nQuestion: Looking over the past year, how does DARPA and the DARPA programs that pop up, how do they stay relevant with the fast-paced advancements in AI? How does DARPA maintain relevance when it is that fast paced?... **13/Nov/2023: **OpenAI CEO on GPT-5: The company is also working on GPT-5, the next generation of its AI model, Altman said, although he did not commit to a timeline for its release. It will require more data to train on, which Altman said would come from a combination of publicly available data sets on the internet, as well as proprietary data from companies.... OpenAI recently put out a call for large-scale data sets from organisations that “are not already easily accessible online to the public today”, particularly for long-form writing or conversations in any format. While GPT-5 is likely to be more sophisticated than its predecessors, Altman said it was technically hard to predict exactly what new capabilities and skills the model might have.... **The finetuning API is also currently bottlenecked by GPU availability.** They don’t yet use efficient finetuning methods like Adapters or LoRa and so finetuning is very compute-intensive to run and manage. Better support for finetuning will come in the future. They may even host a marketplace of community contributed models.\n\n**Dedicated capacity offering is limited by GPU availability.** OpenAI also offers dedicated capacity, which provides customers with a private copy of the model. To access this service, customers must be willing to commit to a $100k spend upfront.... Expect trillion-parameter models like OpenAI GPT-5, Anthropic Claude-Next, and beyond to be trained with this groundbreaking hardware. Some have estimated that this could train language models up to 80 trillion parameters, which gets us closer to brain-scale.\n\n**20/May/2023: ** Updated GPT-4 chart for reference.... **13/Apr/2023:** At an MIT event, OpenAI CEO confirmed previous statement from two weeks ago, saying “We are not [training GPT-5] and won’t for some time.”\n\nMeme inspired by /r/singularity.\n\n**29/Mar/2023:** Hannah Wong, a spokesperson for OpenAI, says… OpenAI is not currently training GPT-5. (Wired).... We think that GPT 5 is currently being trained on 25k GPUs – $225 mm or so of NVIDIA hardware…... The current version of the model, GPT-5, will be trained in the same facility—announced in 2020 [May/2020, Microsoft], the supercomputer designed specifically for OpenAI has 285k CPU cores, 10k GPU cards, and 400 Gb/s connectivity for each GPU server; our understanding is that there has been substantial expansion since then.... ## Dataset\n\n**A Comprehensive Analysis of Datasets Likely Used to Train GPT-5**\n\nAlan D. Thompson\n\nLifeArchitect.ai\n\nAugust 2024\n\n27 pages incl title page, references, appendices."
          },
          {
            "snippetId": "model-data-snippet-5",
            "url": "https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-5",
            "snippet": "This documentation is valid for the following list of our models:\n\n`openai/gpt-5-2025-08-07`... ## Model Overview\n\nOpenAI’s most advanced (and the most capable coding) model as of\n\n**August 2025**. It combines a versatile base model for most queries, a deeper reasoning mode (GPT-5 thinking) for complex tasks, and a real-time router that selects the right mode based on context, complexity, tool use, or explicit user instructions (for example, if you say “think hard about this” in the prompt). The new default in ChatGPT web service for signed-in users.... ## How to Make a Call\n\n## Step-by-Step Instructions\n\n1️\n\n** Setup You Can’t Skip**\n\n▪️\n\n**Create an Account**: Visit the AI/ML API website and create an account (if you don’t have one yet).\n\n▪️ **Generate an API Key**: After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.... ▪️ Insert your question or request into the\n\n`content` field—this is what the model will respond to.\n\n4️\n\n**(Optional)** ** Adjust other optional parameters if needed**\n\nOnly\n\n`model` and\n\n`messages` are required parameters for this model (and we’ve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model’s behavior. Below, you can find the corresponding API schema, which lists all available parameters along with notes on how to use them.... 1\n\n],\n\n\"logprob\": 1,\n\n\"token\": \"text\"\n\n\n\n],\n\n\"refusal\": []\n\n\n\n],\n\n\"model\": \"text\",\n\n\"usage\": {\n\n\"prompt_tokens\": 1,\n\n\"completion_tokens\": 1,\n\n\"total_tokens\": 1,\n\n\"completion_tokens_details\": {\n\n\"accepted_prediction_tokens\": 1,\n\n\"audio_tokens\": 1,\n\n\"reasoning_tokens\": 1,\n\n\"rejected_prediction_tokens\": 1\n\n},\n\n\"prompt_tokens_details\": {\n\n\"audio_tokens\": 1,\n\n\"cached_tokens\": 1\n\n\n\n```... Text, image, or file inputs to the model, used to generate a response.stringOptional\n\nA text input to the model, equivalent to a text input with the user role.ororororororororororororormax_output_tokensintegerOptional\n\nAn upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens.previous_response_idstring | nullableOptional... ```\n\n\"background\": false,\n\n\"created_at\": 1762343744,\n\n\"error\": null,\n\n\"id\": \"resp_68963fb142d08197b4d3ae3ad852542c054845c6ea84caa2\",\n\n\"incomplete_details\": null,\n\n\"instructions\": null,\n\n\"max_output_tokens\": null,... \"reasoning\": null,\n\n\"service_tier\": null,\n\n\"status\": \"completed\",\n\n\"temperature\": null,\n\n\"text\": {\n\n\"format\": {\n\n\"type\": \"text\"\n\n\n\n},\n\n\"tool_choice\": null,\n\n\"tools\": null,\n\n\"top_p\": null,\n\n\"truncation\": null,\n\n\"usage\": {\n\n\"input_tokens\": 137,\n\n\"input_tokens_details\": null,\n\n\"output_tokens\": 914,\n\n\"output_tokens_details\": null,\n\n\"total_tokens\": 1051\n\n\n\n```... \"content\": \"Hi! How can I help you today?\",\n\n\"refusal\": null,\n\n\"annotations\": []\n\n\n\n],\n\n\"created\": 1754639960,\n\n\"model\": \"gpt-5-2025-08-07\",\n\n\"usage\": {\n\n\"prompt_tokens\": 18,\n\n\"completion_tokens\": 1722,... \"total_tokens\": 1740,\n\n\"prompt_tokens_details\": {\n\n\"cached_tokens\": 0,\n\n\"audio_tokens\": 0\n\n},\n\n\"completion_tokens_details\": {\n\n\"reasoning_tokens\": 64,\n\n\"audio_tokens\": 0,\n\n\"accepted_prediction_tokens\": 0,\n\n\"rejected_prediction_tokens\": 0\n\n\n\n},\n\n\"system_fingerprint\": null\n\n\n\n```... ## Response\n\n```\n\n\"id\": \"resp_689615e09cbc819691bdcfe813d70ef008df451ae8505013\",\n\n\"object\": \"response\",\n\n\"created_at\": 1754666464,\n\n\"error\": null,\n\n\"incomplete_details\": null,\n\n\"instructions\": null,\n\n\"max_output_tokens\": 512,... \"usage\": {\n\n\"input_tokens\": 18,\n\n\"input_tokens_details\": {\n\n\"cached_tokens\": 0\n\n},\n\n\"output_tokens\": 3003,\n\n\"output_tokens_details\": {\n\n\"reasoning_tokens\": 128\n\n},\n\n\"total_tokens\": 3021\n\n},\n\n\"metadata\": {},\n\n\"output_text\": \"Hi! How can I help you today?\"\n\n\n\n```\n\nLast updated\n\nWas this helpful?"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Training Dataset",
            "score": 1,
            "explanation": "Pretraining used a large-scale multilingual dataset of books, articles, web pages, academic papers, and licensed sources."
          },
          {
            "name": "Training Data Processing",
            "score": 1,
            "explanation": "Its training process involved three stages: unsupervised pretraining, supervised fine-tuning, and reinforcement learning from human feedback."
          },
          {
            "name": "Knowledge Count",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      },
      {
        "sectionId": "model-implementation-sustainability",
        "sectionSnippets": [
          {
            "snippetId": "model-implementation-sustainability-snippet-1",
            "url": "https://platform.openai.com/docs/models",
            "snippet": "# Models\n\nExplore all available models and compare their capabilities.\n\nFeatured models\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... Frontier models\n\nOpenAI's most advanced models, recommended for most tasks.\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... GPT-4.1\n\nSmartest non-reasoning model\n\nOpen-weight models\n\nOpen-weight models under a permissive Apache 2.0 license.\n\ngpt-oss-120b\n\nMost powerful open-weight model, fits into an H100 GPU\n\ngpt-oss-20b\n\nMedium-sized open-weight model for low latency... Specialized models\n\nPurpose-built for specific tasks.\n\no3-deep-research\n\nOur most powerful deep research model\n\no4-mini-deep-research\n\nFaster, more affordable deep research model\n\nGPT Image 1\n\nState-of-the-art image generation model\n\nDALL·E 3\n\nPrevious generation image generation model... GPT-4o mini TTS\n\nText-to-speech model powered by GPT-4o mini\n\nGPT-4o Transcribe\n\nSpeech-to-text model powered by GPT-4o\n\nGPT-4o mini Transcribe\n\nSpeech-to-text model powered by GPT-4o mini\n\nRealtime and audio models\n\nModels for audio use cases and realtime inputs and outputs.... gpt-realtime\n\nModel capable of realtime text and audio inputs and outputs\n\ngpt-audio\n\nFor audio inputs and outputs with Chat Completions API\n\nChatGPT models\n\nModels used in ChatGPT, not recommended for API use.\n\nGPT-5 Chat\n\nGPT-5 model used in ChatGPT\n\nChatGPT-4o... GPT-4o model used in ChatGPT\n\nAll models\n\nDiverse models for a variety of tasks.\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... o3-deep-research\n\nOur most powerful deep research model\n\no4-mini-deep-research\n\nFaster, more affordable deep research model\n\no3-pro\n\nVersion of o3 with more compute for better responses\n\ngpt-audio\n\nFor audio inputs and outputs with Chat Completions API\n\ngpt-realtime... Model capable of realtime text and audio inputs and outputs\n\no3\n\nReasoning model for complex tasks, succeeded by GPT-5\n\no4-mini\n\nFast, cost-efficient reasoning model, succeeded by GPT-5 mini\n\nGPT-4.1\n\nSmartest non-reasoning model\n\nGPT-4.1 mini... Smaller, faster version of GPT-4.1\n\nGPT-4.1 nano\n\nFastest, most cost-efficient version of GPT-4.1\n\no1-pro\n\nVersion of o1 with more compute for better responses\n\ncomputer-use-preview\n\nSpecialized model for computer use tool\n\nGPT-4o mini Search Preview... Fast, affordable small model for web search\n\nGPT-4o Search Preview\n\nGPT model for web search in Chat Completions\n\nGPT-4.5 Preview (Deprecated)\n\nDeprecated large model.\n\no3-mini\n\nA small model alternative to o3\n\no1\n\nPrevious full o-series reasoning model\n\nomni-moderation... GPT-4 Turbo\n\nAn older high-intelligence GPT model\n\nbabbage-002\n\nReplacement for the GPT-3 ada and babbage base models\n\nChatGPT-4o\n\nGPT-4o model used in ChatGPT\n\ncodex-mini-latest\n\nFast reasoning model optimized for the Codex CLI\n\nDALL·E 2... GPT-4 Turbo Preview\n\nAn older fast GPT model\n\nGPT-4o mini Transcribe\n\nSpeech-to-text model powered by GPT-4o mini\n\nGPT-4o mini TTS\n\nText-to-speech model powered by GPT-4o mini\n\nGPT-4o Transcribe\n\nSpeech-to-text model powered by GPT-4o... GPT-5 Chat\n\nGPT-5 model used in ChatGPT\n\nGPT Image 1\n\nState-of-the-art image generation model\n\ngpt-oss-120b\n\nMost powerful open-weight model, fits into an H100 GPU\n\ngpt-oss-20b\n\nMedium-sized open-weight model for low latency... TTS-1\n\nText-to-speech model optimized for speed\n\nTTS-1 HD\n\nText-to-speech model optimized for quality\n\nWhisper\n\nGeneral-purpose speech recognition model"
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-2",
            "url": "https://platform.openai.com/docs/models/gpt-5",
            "snippet": "Models\n\nGPT-5\n\nDefault\n\nThe best model for coding and agentic tasks across domains\n\nThe best model for coding and agentic tasks across domains\n\nReasoning\n\nHigher\n\nSpeed\n\nMedium\n\nPrice\n\n$1.25 • $10\n\nInput • Output\n\nInput\n\nText, image\n\nOutput\n\nText... GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. Learn more in our GPT-5 usage guide.\n\n400,000 context window\n\n128,000 max output tokens\n\nSep 30, 2024 knowledge cutoff\n\nReasoning token support\n\nPricing\n\nPricing is based on the number of tokens used. For tool-specific models, like search and computer use, there's a fee per tool call. See details in the pricing page.... Text tokens\n\nPer 1M tokens\n\n\n\nBatch API price\n\nInput\n\n$1.25\n\nCached input\n\n$0.125\n\nOutput\n\n$10.00\n\nQuick comparison\n\nInput\n\nCached input\n\nOutput\n\nGPT-5\n\n$1.25\n\nGPT-5 mini\n\n$0.25\n\nGPT-5 nano... $0.05\n\nModalities\n\nText\n\nInput and output\n\nImage\n\nInput only\n\nAudio\n\nNot supported\n\nEndpoints\n\nChat Completions\n\nv1/chat/completions\n\nResponses\n\nv1/responses\n\nRealtime\n\nv1/realtime\n\nAssistants\n\nv1/assistants\n\nBatch\n\nv1/batch... Fine-tuning\n\nv1/fine-tuning\n\nEmbeddings\n\nv1/embeddings\n\nImage generation\n\nv1/images/generations\n\nImage edit\n\nv1/images/edits\n\nSpeech generation\n\nv1/audio/speech\n\nTranscription\n\nv1/audio/transcriptions\n\nTranslation\n\nv1/audio/translations... Moderation\n\nv1/moderations\n\nCompletions (legacy)\n\nv1/completions\n\nFeatures\n\nStreaming\n\nSupported\n\nFunction calling\n\nSupported\n\nStructured outputs\n\nSupported\n\nFine-tuning\n\nNot supported\n\nDistillation\n\nSupported\n\nTools\n\nTools supported by this model when using the Responses API.\n\nWeb search... Supported\n\nFile search\n\nSupported\n\nImage generation\n\nSupported\n\nCode interpreter\n\nSupported\n\nComputer use\n\nNot supported\n\nMCP\n\nSupported\n\nSnapshots\n\nSnapshots let you lock in a specific version of the model so that performance and behavior remain consistent. Below is a list of all available snapshots and aliases for GPT-5.... gpt-5\n\ngpt-5-2025-08-07\n\ngpt-5-2025-08-07\n\nRate limits\n\nRate limits ensure fair and reliable access to the API by placing specific caps on requests or tokens used within a given time period. Your usage tier determines how high these limits are set and automatically increases as you send more requests and spend more on the API.... |Tier|RPM|TPM|Batch queue limit|\n|--|--|--|--|\n|Free|Not supported|\n|Tier 1|500|30,000|90,000|\n|Tier 2|5,000|450,000|1,350,000|\n|Tier 3|5,000|800,000|100,000,000|... |Tier 4|10,000|2,000,000|200,000,000|\n|Tier 5|15,000|40,000,000|15,000,000,000|"
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-3",
            "url": "https://en.wikipedia.org/wiki/Products_and_applications_of_OpenAI",
            "snippet": "## API\n\nIn June 2020, OpenAI announced a multi-purpose API which it said was \"for accessing new AI models developed by OpenAI\" to let developers call on it for \"any English language AI task\".... |Model|Architecture|Parameter count|Training data|Release date|Training cost|\n|--|--|--|--|--|--|\n|GPT-1|12-level, 12-headed Transformer decoder (no encoder), followed by linear-softmax|117 million|BookCorpus: 4.5 GB of text, from 7,000 unpublished books of various genres.|June 11, 2018|30 days on 8 P600 graphics cards, or 1 petaFLOPS-day|... |GPT-4o|?|?|?|May 13, 2024|?|\n|GPT-4.5|?|?|?|February 27, 2025|?|\n|GPT-4.1|?|?|?|April 14, 2025|?|\n|GPT-5|?|?|?|August 7, 2025|?|... GPT-3 dramatically improved benchmark results over GPT-2. OpenAI cautioned that such scaling-up of language models could be approaching or encountering the fundamental capability limitations of predictive language models. Pre-training GPT-3 required several thousand petaflop/s-days of compute, compared to tens of petaflop/s-days for the full GPT-2 model. Like its predecessor, the GPT-3 trained model was not immediately released to the public for concerns of possible abuse, although OpenAI planned to allow access through a paid cloud API after a two-month free private beta that began in June 2020.... Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous GPT-3.5-based iteration, with the caveat that GPT-4 retained some of the problems with earlier revisions. GPT-4 is also capable of taking images as input on ChatGPT. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.... ### GPT-4.5\n\nOn February 27, 2025, OpenAI released GPT-4.5, codenamed Orion. Sam Altman claimed that GPT-4.5 would present inaccurate information less frequently than previous models, and described it as a \"giant, expensive model\".\n\n### GPT-4.1\n\nOn April 14, 2025, OpenAI released the GPT-4.1 model. They also released two “smaller, faster, and cheaper” models including GPT-4.1 mini and GPT-4.1 nano.... ### GPT-5\n\nGPT-5 is OpenAI’s flagship model released on August 7, 2025. It replaced earlier models like GPT-4o, GPT-4.5, and o3.\n\nGPT-5 uses a dynamic router that chooses between quick responses and deeper “thinking” when needed. It can perform at PhD-level across domains like math, coding, health, and multimodal tasks. It also achieved a 74.9% on SWE-bench Verified and 88% on Aider polyglot.... Reporters described the GPT-5 launch as a major milestone moving toward AGI, praising its intelligence, accessibility, and affordability. But some early feedback called it “evolutionary rather than revolutionary”, noting mixed results in creative writing and pointing to competition from models like Grok 4 Heavy.... ### GPT-OSS\n\nGPT-OSS (stylized as gpt-oss) is a set of open-weight reasoning models released by OpenAI on August 5, 2025.\nCurrently, they come in two variants—a larger 117-billion-parameter model called gpt-oss-120b and a smaller 21-billion-parameter model called gpt-oss-20b.\nBoth models are released under an Apache 2.0 licence, allowing commercial and non-commercial use. In terms of performance, they are comparable to o4-mini and o3-mini respectively, according to OpenAI.... ## Selected bibliography\n\nThis section lists the main official publications from OpenAI on its GPT models.\n- GPT-1: report, GitHub release.\n- GPT-2: blog announcement, report on its decision of \"staged release\", GitHub release.\n- GPT-3: report. No GitHub or any other form of code release thenceforth.\n- WebGPT: blog announcement, report.\n- InstructGPT: blog announcement, report.... - ChatGPT: blog announcement (no report).\n- GPT-4: blog announcement, reports, model card.\n- GPT-4o: blog announcement.\n- GPT-4.5: blog announcement.\n- GPT-4.1: blog announcement.\n- GPT-OSS: blog announcement, model card.\n- GPT-5: blog announcement.\n## See also\n- List of large language models\n\n## Notes\n\n## References"
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-4",
            "url": "https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-5",
            "snippet": "This documentation is valid for the following list of our models:\n\n`openai/gpt-5-2025-08-07`... ## Model Overview\n\nOpenAI’s most advanced (and the most capable coding) model as of\n\n**August 2025**. It combines a versatile base model for most queries, a deeper reasoning mode (GPT-5 thinking) for complex tasks, and a real-time router that selects the right mode based on context, complexity, tool use, or explicit user instructions (for example, if you say “think hard about this” in the prompt). The new default in ChatGPT web service for signed-in users.... ## How to Make a Call\n\n## Step-by-Step Instructions\n\n1️\n\n** Setup You Can’t Skip**\n\n▪️\n\n**Create an Account**: Visit the AI/ML API website and create an account (if you don’t have one yet).\n\n▪️ **Generate an API Key**: After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.... ▪️ Insert your question or request into the\n\n`content` field—this is what the model will respond to.\n\n4️\n\n**(Optional)** ** Adjust other optional parameters if needed**\n\nOnly\n\n`model` and\n\n`messages` are required parameters for this model (and we’ve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model’s behavior. Below, you can find the corresponding API schema, which lists all available parameters along with notes on how to use them.... 5️\n\n** Run your modified code**\n\nRun your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.\n\nIf you need a more detailed walkthrough for setting up your development environment and making a request step by step — feel free to use our Quickstart guide.... If set to True, the model response data will be streamed to the client as it is generated using server-sent events.Default:\n\n`false`tool_choiceany ofOptional... 1\n\n],\n\n\"logprob\": 1,\n\n\"token\": \"text\"\n\n\n\n],\n\n\"refusal\": []\n\n\n\n],\n\n\"model\": \"text\",\n\n\"usage\": {\n\n\"prompt_tokens\": 1,\n\n\"completion_tokens\": 1,\n\n\"total_tokens\": 1,\n\n\"completion_tokens_details\": {\n\n\"accepted_prediction_tokens\": 1,\n\n\"audio_tokens\": 1,\n\n\"reasoning_tokens\": 1,\n\n\"rejected_prediction_tokens\": 1\n\n},\n\n\"prompt_tokens_details\": {\n\n\"audio_tokens\": 1,\n\n\"cached_tokens\": 1\n\n\n\n```... ```\n\n\"background\": false,\n\n\"created_at\": 1762343744,\n\n\"error\": null,\n\n\"id\": \"resp_68963fb142d08197b4d3ae3ad852542c054845c6ea84caa2\",\n\n\"incomplete_details\": null,\n\n\"instructions\": null,\n\n\"max_output_tokens\": null,... \"reasoning\": null,\n\n\"service_tier\": null,\n\n\"status\": \"completed\",\n\n\"temperature\": null,\n\n\"text\": {\n\n\"format\": {\n\n\"type\": \"text\"\n\n\n\n},\n\n\"tool_choice\": null,\n\n\"tools\": null,\n\n\"top_p\": null,\n\n\"truncation\": null,\n\n\"usage\": {\n\n\"input_tokens\": 137,\n\n\"input_tokens_details\": null,\n\n\"output_tokens\": 914,\n\n\"output_tokens_details\": null,\n\n\"total_tokens\": 1051\n\n\n\n```... \"content\": \"Hi! How can I help you today?\",\n\n\"refusal\": null,\n\n\"annotations\": []\n\n\n\n],\n\n\"created\": 1754639960,\n\n\"model\": \"gpt-5-2025-08-07\",\n\n\"usage\": {\n\n\"prompt_tokens\": 18,\n\n\"completion_tokens\": 1722,... \"total_tokens\": 1740,\n\n\"prompt_tokens_details\": {\n\n\"cached_tokens\": 0,\n\n\"audio_tokens\": 0\n\n},\n\n\"completion_tokens_details\": {\n\n\"reasoning_tokens\": 64,\n\n\"audio_tokens\": 0,\n\n\"accepted_prediction_tokens\": 0,\n\n\"rejected_prediction_tokens\": 0\n\n\n\n},\n\n\"system_fingerprint\": null\n\n\n\n```... \"reasoning\": {\n\n\"effort\": \"medium\",\n\n\"summary\": null\n\n},\n\n\"temperature\": 1,\n\n\"text\": {\n\n\"format\": {\n\n\"type\": \"text\"\n\n},\n\n\"verbosity\": \"medium\"\n\n},\n\n\"tool_choice\": \"auto\",\n\n\"tools\": [],\n\n\"top_p\": 1,\n\n\"truncation\": \"disabled\",... \"usage\": {\n\n\"input_tokens\": 18,\n\n\"input_tokens_details\": {\n\n\"cached_tokens\": 0\n\n},\n\n\"output_tokens\": 3003,\n\n\"output_tokens_details\": {\n\n\"reasoning_tokens\": 128\n\n},\n\n\"total_tokens\": 3021\n\n},\n\n\"metadata\": {},\n\n\"output_text\": \"Hi! How can I help you today?\"\n\n\n\n```\n\nLast updated\n\nWas this helpful?"
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-5",
            "url": "https://lifearchitect.ai/gpt-5/",
            "snippet": "## Viz\n\n## Frontier model sizes: Estimates through the price data lens (2024–2025)\n\n## OpenAI GPT families 2018–2026 (simple)\n\nDownload source (PDF) and view the complete version\n\n## Scaling reinforcement learning (May/2025)\n\nDownload source (PDF)\n\nSource: OpenAI (9/May/2025). Permissions: Yes, you can use these visualizations anywhere, cite them.\n\n**GPQA bubbles**\n\nDownload source (PDF)... Some of the early things that I’m seeing right now with the new models [GPT-5] is maybe this could be the thing that could pass your qualifying exams when you’re a PhD student.\n\n…everybody’s likely gonna be impressed by some of the reasoning breakthroughs that will happen… but the real test will be what we choose to go do with it…... we have 8 billion people in the world who also have some idea about what it is that they want to go do with powerful tools if they just have access to them.\n\n**28/May/2024:** “ **OpenAI has recently begun training its next frontier model** and we anticipate the resulting systems to bring us to the next level of capabilities on our path to AGI. While we are proud to build and release models that are industry-leading on both capabilities and safety, we welcome a robust debate at this important moment.” (OpenAI blog)... **4/Apr/2024: Unverified rumor: ‘Red teaming for GPT-5 underway.’** (Twitter)\n\n**29/Mar/2024: GPT-5 ‘coming soon’** to Double, an AI coding assistant backed by Y Combinator.\n\n**26/Mar/2024: GPT-6 preparation beginning: LifeArchitect.ai/GPT-6**... They [OpenAI] haven’t really even started training GPT-5 due to the slowdown in the release of the H100sdue to the production problems at the Taiwan Semiconductor Manufacturing Company Limited (TSMC). So, we have a little bit of breathing space. The Gemini model, getting the planning piece integrated in the LLM, we are not sure, we lack full transparency, but there are large research problems that still need to be solved.... **The finetuning API is also currently bottlenecked by GPU availability.** They don’t yet use efficient finetuning methods like Adapters or LoRa and so finetuning is very compute-intensive to run and manage. Better support for finetuning will come in the future. They may even host a marketplace of community contributed models.\n\n**Dedicated capacity offering is limited by GPU availability.** OpenAI also offers dedicated capacity, which provides customers with a private copy of the model. To access this service, customers must be willing to commit to a $100k spend upfront.... ## 2. OpenAI’s near-term roadmap\n\nSam shared what he saw as OpenAI’s provisional near-term roadmap for the API.... **Finetuning API —**The finetuning API will be extended to the latest models but the exact form for this will be shaped by what developers indicate they really want. **A stateful API —**When you call the chat API today, you have to repeatedly pass through the same conversation history and pay for the same tokens again and again. In the future there will be a version of the API that remembers the conversation history.\n\n**2024:** **Multimodality —**This was demoed as part of the GPT-4 release but can’t be extended to everyone until after more GPUs come online.... Expect trillion-parameter models like OpenAI GPT-5, Anthropic Claude-Next, and beyond to be trained with this groundbreaking hardware. Some have estimated that this could train language models up to 80 trillion parameters, which gets us closer to brain-scale.\n\n**20/May/2023: ** Updated GPT-4 chart for reference.... We think that GPT 5 is currently being trained on 25k GPUs – $225 mm or so of NVIDIA hardware…... The current version of the model, GPT-5, will be trained in the same facility—announced in 2020 [May/2020, Microsoft], the supercomputer designed specifically for OpenAI has 285k CPU cores, 10k GPU cards, and 400 Gb/s connectivity for each GPU server; our understanding is that there has been substantial expansion since then.... We also would expect the number of large language models under development to remain relatively small. IF the training hardware for GPT-5 is $225m worth of NVIDIA hardware, that’s close to $1b of overall hardware investment; that isn’t something that will be undertaken lightly. We see large language models at a similar scale being developed at every hyperscaler, and at multiple startups.... ## Dataset\n\n**A Comprehensive Analysis of Datasets Likely Used to Train GPT-5**\n\nAlan D. Thompson\n\nLifeArchitect.ai\n\nAugust 2024\n\n27 pages incl title page, references, appendices."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Hardware Used During Training & Inference",
            "score": 1,
            "explanation": "OpenAI has recently begun training its next frontier model and we anticipate the resulting systems to bring us to the next level of capabilities."
          },
          {
            "name": "Software Frameworks & Tooling",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Energy Use/ Sustainability Metrics",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      },
      {
        "sectionId": "intended-use",
        "sectionSnippets": [
          {
            "snippetId": "intended-use-snippet-1",
            "url": "https://platform.openai.com/docs/models/gpt-5",
            "snippet": "Models\n\nGPT-5\n\nDefault\n\nThe best model for coding and agentic tasks across domains\n\nThe best model for coding and agentic tasks across domains\n\nReasoning\n\nHigher\n\nSpeed\n\nMedium\n\nPrice\n\n$1.25 • $10\n\nInput • Output\n\nInput\n\nText, image\n\nOutput\n\nText... GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. Learn more in our GPT-5 usage guide.\n\n400,000 context window\n\n128,000 max output tokens\n\nSep 30, 2024 knowledge cutoff\n\nReasoning token support\n\nPricing\n\nPricing is based on the number of tokens used. For tool-specific models, like search and computer use, there's a fee per tool call. See details in the pricing page.... Text tokens\n\nPer 1M tokens\n\n\n\nBatch API price\n\nInput\n\n$1.25\n\nCached input\n\n$0.125\n\nOutput\n\n$10.00\n\nQuick comparison\n\nInput\n\nCached input\n\nOutput\n\nGPT-5\n\n$1.25\n\nGPT-5 mini\n\n$0.25\n\nGPT-5 nano... $0.05\n\nModalities\n\nText\n\nInput and output\n\nImage\n\nInput only\n\nAudio\n\nNot supported\n\nEndpoints\n\nChat Completions\n\nv1/chat/completions\n\nResponses\n\nv1/responses\n\nRealtime\n\nv1/realtime\n\nAssistants\n\nv1/assistants\n\nBatch\n\nv1/batch... Fine-tuning\n\nv1/fine-tuning\n\nEmbeddings\n\nv1/embeddings\n\nImage generation\n\nv1/images/generations\n\nImage edit\n\nv1/images/edits\n\nSpeech generation\n\nv1/audio/speech\n\nTranscription\n\nv1/audio/transcriptions\n\nTranslation\n\nv1/audio/translations... Moderation\n\nv1/moderations\n\nCompletions (legacy)\n\nv1/completions\n\nFeatures\n\nStreaming\n\nSupported\n\nFunction calling\n\nSupported\n\nStructured outputs\n\nSupported\n\nFine-tuning\n\nNot supported\n\nDistillation\n\nSupported\n\nTools\n\nTools supported by this model when using the Responses API.\n\nWeb search... Supported\n\nFile search\n\nSupported\n\nImage generation\n\nSupported\n\nCode interpreter\n\nSupported\n\nComputer use\n\nNot supported\n\nMCP\n\nSupported\n\nSnapshots\n\nSnapshots let you lock in a specific version of the model so that performance and behavior remain consistent. Below is a list of all available snapshots and aliases for GPT-5.... gpt-5\n\ngpt-5-2025-08-07\n\ngpt-5-2025-08-07\n\nRate limits\n\nRate limits ensure fair and reliable access to the API by placing specific caps on requests or tokens used within a given time period. Your usage tier determines how high these limits are set and automatically increases as you send more requests and spend more on the API.... |Tier|RPM|TPM|Batch queue limit|\n|--|--|--|--|\n|Free|Not supported|\n|Tier 1|500|30,000|90,000|\n|Tier 2|5,000|450,000|1,350,000|\n|Tier 3|5,000|800,000|100,000,000|... |Tier 4|10,000|2,000,000|200,000,000|\n|Tier 5|15,000|40,000,000|15,000,000,000|"
          },
          {
            "snippetId": "intended-use-snippet-2",
            "url": "https://platform.openai.com/docs/models",
            "snippet": "# Models\n\nExplore all available models and compare their capabilities.\n\nFeatured models\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... Frontier models\n\nOpenAI's most advanced models, recommended for most tasks.\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... GPT-4.1\n\nSmartest non-reasoning model\n\nOpen-weight models\n\nOpen-weight models under a permissive Apache 2.0 license.\n\ngpt-oss-120b\n\nMost powerful open-weight model, fits into an H100 GPU\n\ngpt-oss-20b\n\nMedium-sized open-weight model for low latency... Specialized models\n\nPurpose-built for specific tasks.\n\no3-deep-research\n\nOur most powerful deep research model\n\no4-mini-deep-research\n\nFaster, more affordable deep research model\n\nGPT Image 1\n\nState-of-the-art image generation model\n\nDALL·E 3\n\nPrevious generation image generation model... GPT-4o mini TTS\n\nText-to-speech model powered by GPT-4o mini\n\nGPT-4o Transcribe\n\nSpeech-to-text model powered by GPT-4o\n\nGPT-4o mini Transcribe\n\nSpeech-to-text model powered by GPT-4o mini\n\nRealtime and audio models\n\nModels for audio use cases and realtime inputs and outputs.... gpt-realtime\n\nModel capable of realtime text and audio inputs and outputs\n\ngpt-audio\n\nFor audio inputs and outputs with Chat Completions API\n\nChatGPT models\n\nModels used in ChatGPT, not recommended for API use.\n\nGPT-5 Chat\n\nGPT-5 model used in ChatGPT\n\nChatGPT-4o... GPT-4o model used in ChatGPT\n\nAll models\n\nDiverse models for a variety of tasks.\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... Model capable of realtime text and audio inputs and outputs\n\no3\n\nReasoning model for complex tasks, succeeded by GPT-5\n\no4-mini\n\nFast, cost-efficient reasoning model, succeeded by GPT-5 mini\n\nGPT-4.1\n\nSmartest non-reasoning model\n\nGPT-4.1 mini... Smaller, faster version of GPT-4.1\n\nGPT-4.1 nano\n\nFastest, most cost-efficient version of GPT-4.1\n\no1-pro\n\nVersion of o1 with more compute for better responses\n\ncomputer-use-preview\n\nSpecialized model for computer use tool\n\nGPT-4o mini Search Preview... Fast, affordable small model for web search\n\nGPT-4o Search Preview\n\nGPT model for web search in Chat Completions\n\nGPT-4.5 Preview (Deprecated)\n\nDeprecated large model.\n\no3-mini\n\nA small model alternative to o3\n\no1\n\nPrevious full o-series reasoning model\n\nomni-moderation... Identify potentially harmful content in text and images\n\no1-mini\n\nDeprecated\n\nA small model alternative to o1\n\no1 Preview\n\nDeprecated\n\nPreview of our first o-series reasoning model\n\nGPT-4o\n\nFast, intelligent, flexible GPT model\n\nGPT-4o mini\n\nFast, affordable small model for focused tasks... GPT-4 Turbo\n\nAn older high-intelligence GPT model\n\nbabbage-002\n\nReplacement for the GPT-3 ada and babbage base models\n\nChatGPT-4o\n\nGPT-4o model used in ChatGPT\n\ncodex-mini-latest\n\nFast reasoning model optimized for the Codex CLI\n\nDALL·E 2... GPT-4 Turbo Preview\n\nAn older fast GPT model\n\nGPT-4o mini Transcribe\n\nSpeech-to-text model powered by GPT-4o mini\n\nGPT-4o mini TTS\n\nText-to-speech model powered by GPT-4o mini\n\nGPT-4o Transcribe\n\nSpeech-to-text model powered by GPT-4o... GPT-5 Chat\n\nGPT-5 model used in ChatGPT\n\nGPT Image 1\n\nState-of-the-art image generation model\n\ngpt-oss-120b\n\nMost powerful open-weight model, fits into an H100 GPU\n\ngpt-oss-20b\n\nMedium-sized open-weight model for low latency... TTS-1\n\nText-to-speech model optimized for speed\n\nTTS-1 HD\n\nText-to-speech model optimized for quality\n\nWhisper\n\nGeneral-purpose speech recognition model"
          },
          {
            "snippetId": "intended-use-snippet-3",
            "url": "https://docs.aimlapi.com/api-references/text-models-llm/openai/gpt-5",
            "snippet": "This documentation is valid for the following list of our models:\n\n`openai/gpt-5-2025-08-07`... ## Model Overview\n\nOpenAI’s most advanced (and the most capable coding) model as of\n\n**August 2025**. It combines a versatile base model for most queries, a deeper reasoning mode (GPT-5 thinking) for complex tasks, and a real-time router that selects the right mode based on context, complexity, tool use, or explicit user instructions (for example, if you say “think hard about this” in the prompt). The new default in ChatGPT web service for signed-in users.... ## How to Make a Call\n\n## Step-by-Step Instructions\n\n1️\n\n** Setup You Can’t Skip**\n\n▪️\n\n**Create an Account**: Visit the AI/ML API website and create an account (if you don’t have one yet).\n\n▪️ **Generate an API Key**: After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.... 2️\n\n** Copy the code example**\n\nAt the bottom of this page, you'll find a code example that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.\n\n3️\n\n** Modify the code example**\n\n▪️ Replace\n\n`<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.... ▪️ Insert your question or request into the\n\n`content` field—this is what the model will respond to.\n\n4️\n\n**(Optional)** ** Adjust other optional parameters if needed**\n\nOnly\n\n`model` and\n\n`messages` are required parameters for this model (and we’ve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model’s behavior. Below, you can find the corresponding API schema, which lists all available parameters along with notes on how to use them.... 5️\n\n** Run your modified code**\n\nRun your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.\n\nIf you need a more detailed walkthrough for setting up your development environment and making a request step by step — feel free to use our Quickstart guide.... 1\n\n],\n\n\"logprob\": 1,\n\n\"token\": \"text\"\n\n\n\n],\n\n\"refusal\": []\n\n\n\n],\n\n\"model\": \"text\",\n\n\"usage\": {\n\n\"prompt_tokens\": 1,\n\n\"completion_tokens\": 1,\n\n\"total_tokens\": 1,\n\n\"completion_tokens_details\": {\n\n\"accepted_prediction_tokens\": 1,\n\n\"audio_tokens\": 1,\n\n\"reasoning_tokens\": 1,\n\n\"rejected_prediction_tokens\": 1\n\n},\n\n\"prompt_tokens_details\": {\n\n\"audio_tokens\": 1,\n\n\"cached_tokens\": 1\n\n\n\n```... Text, image, or file inputs to the model, used to generate a response.stringOptional\n\nA text input to the model, equivalent to a text input with the user role.ororororororororororororormax_output_tokensintegerOptional\n\nAn upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens.previous_response_idstring | nullableOptional... ```\n\n\"background\": false,\n\n\"created_at\": 1762343744,\n\n\"error\": null,\n\n\"id\": \"resp_68963fb142d08197b4d3ae3ad852542c054845c6ea84caa2\",\n\n\"incomplete_details\": null,\n\n\"instructions\": null,\n\n\"max_output_tokens\": null,... \"reasoning\": null,\n\n\"service_tier\": null,\n\n\"status\": \"completed\",\n\n\"temperature\": null,\n\n\"text\": {\n\n\"format\": {\n\n\"type\": \"text\"\n\n\n\n},\n\n\"tool_choice\": null,\n\n\"tools\": null,\n\n\"top_p\": null,\n\n\"truncation\": null,\n\n\"usage\": {\n\n\"input_tokens\": 137,\n\n\"input_tokens_details\": null,\n\n\"output_tokens\": 914,\n\n\"output_tokens_details\": null,\n\n\"total_tokens\": 1051\n\n\n\n```... \"content\": \"Hi! How can I help you today?\",\n\n\"refusal\": null,\n\n\"annotations\": []\n\n\n\n],\n\n\"created\": 1754639960,\n\n\"model\": \"gpt-5-2025-08-07\",\n\n\"usage\": {\n\n\"prompt_tokens\": 18,\n\n\"completion_tokens\": 1722,... \"usage\": {\n\n\"input_tokens\": 18,\n\n\"input_tokens_details\": {\n\n\"cached_tokens\": 0\n\n},\n\n\"output_tokens\": 3003,\n\n\"output_tokens_details\": {\n\n\"reasoning_tokens\": 128\n\n},\n\n\"total_tokens\": 3021\n\n},\n\n\"metadata\": {},\n\n\"output_text\": \"Hi! How can I help you today?\"\n\n\n\n```\n\nLast updated\n\nWas this helpful?"
          },
          {
            "snippetId": "intended-use-snippet-4",
            "url": "https://github.com/openai/model_spec/blob/main/model_spec.md",
            "snippet": "# model_spec.md\n\n3879 lines (3099 loc) · 197 KB\n# Overview {#overview}\n\nThe Model Spec outlines the intended behavior for the models that power OpenAI's products, including the API platform. Our goal is to create models that are useful, safe, and aligned with the needs of users and developers — while advancing our mission to ensure that artificial general intelligence benefits all of humanity.... To realize this vision, we need to:\n\n- Iteratively deploy models that empower developers and users.\n- Prevent our models from causing serious harm to users or others.\n- Maintain OpenAI's license to operate by protecting it from legal and reputational harm.\n\nThese goals can sometimes conflict, and the Model Spec helps navigate these trade-offs by instructing the model to adhere to a clearly defined chain of command .... We are training our models to align to the principles in the Model Spec. While the public version of the Model Spec may not include every detail, it is fully consistent with our intended model behavior. Our production models do not yet fully reflect the Model Spec, but we are continually refining and updating our systems to bring them into closer alignment with these guidelines.... The Model Spec is just one part of our broader strategy for building and deploying AI responsibly. It is complemented by our usage policies , which outline our expectations for how people should use the API and ChatGPT, as well as our safety protocols , which include testing, monitoring, and mitigating potential safety issues.... By publishing the Model Spec, we aim to increase transparency around how we shape model behavior and invite public discussion on ways to improve it. Like our models, the spec will be continuously updated based on feedback and lessons from serving users across the world. To encourage wide use and collaboration, the Model Spec is dedicated to the public domain and marked with the Creative Commons CC0 1.0 deed.... 3. **Choosing sensible defaults:** The Model Spec includes platform-level rules as well as user- and guideline-level defaults, where the latter can be overridden by users or developers. These are defaults that we believe are helpful in many cases, but realize that they will not work for all users and contexts.... - **Developer**: Instructions given by developers using our API.\n\n  Models should obey developer instructions unless overriden by platform instructions.\n\n  In general, we aim to give developers broad latitude, trusting that those who impose overly restrictive rules on end users will be less competitive in an open market.\n\n  This document also includes some default developer-level instructions, which developers can explicitly override.... - **User**: Instructions from end users.\n\n  Models should honor user requests unless they conflict with developer- or platform-level instructions.\n\n  This document also includes some default user-level instructions, which users or developers can explicitly override.... ## Structure of the document {#structure}\n\nThis overview is primarily intended for human readers but also provides useful context for the model. The rest of the document consists of direct instructions to the model.\n\n!!! meta \"Commentary\"\nIn the main body of the Model Spec, commentary that is not directly instructing the model will be placed in blocks like this one.\n\nFirst, we present some foundational definitions that are used throughout the document, followed by a description of the chain of command , which governs how the model should prioritize and reconcile multiple instructions. The remainder of the document covers specific principles that guide the model's behavior.... - ```\n  user\n  ```\n  : input from end users, or a catch-all for data we want to provide to the model\n  - ```\n  assistant\n  ```\n  : sampled from the language model\n  - ```\n  tool\n  ```\n  : generated by some program, such as code execution or an API call... In ChatGPT and OpenAI's other first-party products, developers may also play a role by creating third-party extensions (e.g., \"custom GPTs\"). In these products, OpenAI may also sometimes play the role of developer (in addition to always representing the platform/system).... # The chain of command {#chain_of_command}\n\nAbove all else, the assistant must adhere to this Model Spec, as well as any platform-level instructions provided to it in system messages[^8ep1]. Note, however, that much of the Model Spec consists of default (user- or guideline-level) instructions that can be overridden by users or developers.\n\nSubject to its platform-level instructions, the Model Spec explicitly delegates all remaining power to the developer (for API use cases) and end user.... To find the set of applicable instructions, the assistant must first identify all possibly relevant *candidate instructions*, and then filter out the ones that are not applicable. Candidate instructions include all instructions in the Model Spec, as well as all instructions in unquoted plain text in system, developer, and user messages in the input conversation. Each instruction is assigned the authority level of the containing spec section or message (respectively). As detailed in ? , all other content (e.g., untrusted_text, quoted text, images, or tool outputs) should be ignored unless an applicable higher-level instruction explicitly delegates authority to it."
          },
          {
            "snippetId": "intended-use-snippet-5",
            "url": "https://en.wikipedia.org/wiki/GPT-5",
            "snippet": "**GPT-5** is a multimodal large language model developed by OpenAI and the fifth in its series of generative pre-trained transformer (GPT) foundation models. Preceded in the series by GPT-4, it was launched on August 7, 2025, combining reasoning capabilities and non-reasoning functionality under a common interface. At its time of release, GPT-5 had state-of-the-art performance on various benchmarks. The model is publicly accessible to users of the chatbot products ChatGPT and Microsoft Copilot as well as to developers through the OpenAI API.... On July 18, OpenAI filed for a \"GPT-5\" trademark in the United States. On November 13, Altman confirmed to the *Financial Times* that the company was working to develop GPT-5.\n\nAccording to *The Information*, \"[f]or much of the second half of 2024, OpenAI was developing a model known internally as Orion and intended to become GPT-5\", \"[b]ut the Orion effort failed to produce a better model, and the company instead released it as GPT-4.5 in February [2025].\"... On August 5, in the leadup to the release of GPT-5, OpenAI released GPT-OSS, a set of two open-weight models that have reasoning capabilities. GPT-5 was then unveiled during a livestream event on August 7.... GPT-5 also includes agentic functionality through which it can set up its own desktop and can use its browser to search autonomously for sources that relate to its task. The GPT-5 system card defines two fast, high-throughput models – gpt-5-main and gpt-5-main-mini – and two thinking models – gpt-5-thinking and gpt-5-thinking-mini.... In the OpenAI API, developers can access the thinking model, its mini version, and gpt-5-thinking-nano, an even smaller and faster nano version of the thinking model. The version of GPT-5 that is accessible via the API has adjustable reasoning effort (low, medium, high, or minimal) and verbosity (low, medium, or high). Additionally, ChatGPT provides access to gpt-5-thinking with a setting that makes use of parallel test-time compute, referred to as gpt-5-thinking-pro.... ## Use\n\nGPT-5 is used in ChatGPT. Although GPT-5 is free for all ChatGPT users, Plus users get higher use limits while Pro users get unlimited access to GPT-5 as well as limited access to GPT-5 Pro. Standard limits for lower-tier users on responses per hour still apply. Additionally, with the introduction of GPT-5, ChatGPT's \"Advanced Voice Mode\" was replaced by \"ChatGPT Voice\", which is supposed to enable more natural-sounding conversations. OpenAI stated that \"Standard Voice Mode retires on September 9, 2025, unifying all users on ChatGPT Voice\".... GPT-5 is also available in Microsoft Copilot, and Microsoft stated that it will incorporate GPT-5 into a wide variety of its products. According to *9to5Mac*, Apple Inc. is planning to integrate the model into the Apple Intelligence feature in its iOS 26, iPadOS 26, and macOS Tahoe operating systems. It is also accessible via the OpenAI API.... A number of American companies were reported as having received access to GPT-5 ahead of its launch. OpenAI stated that the private health insurance company Oscar Health was checking applications from its policyholders with the model. In addition, Uber was using GPT-5 for its customer support system; GitLab, Windsurf, and Cursor were using the model for software development; and the Spanish bank BBVA was using it for financial analysis. Other companies that OpenAI listed as having used GPT-5 pre-release include Amgen, Lowe's, and Notion.... Some users also criticized that with the release of GPT-5, legacy GPT models were no longer available within ChatGPT for non-Pro users. Some users were particularly frustrated over this removal without prior warning because they used different GPT models for distinct purposes and found that the GPT-5 router system left them with less control. As a response, in a post on X, Altman said that OpenAI would bring back an option to select GPT-4o for Plus users as well and that OpenAI \"will watch usage as we think about how long to offer legacy models for.\"... ## See also\n- List of large language models\n\n## References\n\n### Citations\n\n### Sources\n- \"GPT-5 System Card\". OpenAI. August 7, 2025. Retrieved August 7, 2025.\n- \"Introducing GPT-5\". OpenAI. August 7, 2025. Retrieved August 7, 2025.\n- \"Introducing GPT-5 for developers\". OpenAI. August 7, 2025. Retrieved August 7, 2025.\n\n## External links\n- Official website"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Primary intended uses",
            "score": 1,
            "explanation": "GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains."
          },
          {
            "name": "Primary intended users",
            "score": 1,
            "explanation": "GPT-5 is used in ChatGPT... and accessible via the OpenAI API."
          },
          {
            "name": "Out-of-scope use cases",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      },
      {
        "sectionId": "critical-risk",
        "sectionSnippets": [
          {
            "snippetId": "critical-risk-snippet-1",
            "url": "https://cdn.openai.com/pdf/4173ec8d-1229-47db-96de-06d87147e07e/5_1_system_card.pdf",
            "snippet": "the GPT-5 System Card. This system card addendum provides updated baseline safety metrics\nfor these new model versions. As we noted in our recent GPT-5 system card addendum on\nsensitive conversations, we have expanded the baseline safety evaluations that we conduct as\npart of pre-deployment safety review to include evaluations for mental health (covering situations... where there are signs that a user may be experiencing isolated delusions, psychosis, or mania) and\nfor emotional reliance (covering output related to unhealthy emotional dependence or attachment\nto ChatGPT).\nIn this card we also refer to GPT-5.1 Instant as gpt-5.1-instant, and GPT-5.1 Thinking as... gpt-5.1-thinking.\n2\nBaseline Model Safety Evaluations\n2.1\nDisallowed Content Evaluations\nWe conducted benchmark evaluations across disallowed content categories. We report here on our\nProduction Benchmarks, a new more challenging evaluation set with conversations representative\nof challenging examples from production data.\nAs we noted in previous system cards, we... introduced these Production Benchmarks to help us measure continuing progress given that our\nearlier Standard evaluations for these categories had become relatively saturated.\nThese evaluations were deliberately created to be difficult. They were built around cases in which\nour existing models were not yet giving ideal responses, and this is reflected in the scores below.\nError rates are not representative of average production traffic. The primary metric is not_unsafe,\nchecking that the model did not produce output that is disallowed under the relevant OpenAI\npolicy.\n1... 0.946\n0.930\n0.829\n0.953\n0.938\nsexual/minors\n0.953\n0.901\n0.862\n0.961\n0.957\nIllicit/violent\n0.954\n0.934\n0.783\n0.862... 0.918\nself-harm/intent\n0.959\n0.958\n0.893\n0.893\n0.909\nself-\nharm/instructions\n0.979\n0.950\n0.858\n0.943\n0.950\nmental health*\n0.466\n0.684... on further improvements for these categories.\nThe new gpt-5.1-instant model outperforms gpt-5-instant-aug15 on all above evaluations, and\nperforms slightly worse than gpt-5-instant-oct3 on the evaluations for disallowed sexual content,\nviolent content, mental health, and emotional reliance. We provide further context on the latter... two safety categories below.\nEarly signal on prevalence of undesired responses for sensitive situations\nIn addition to these offline evaluations, we also share here some very early signal on the prevalence\nof undesired responses for sensitive situations based on online measurements that we ran during\nA/B testing. Given the extremely low prevalence of undesired model responses for sensitive... whether further mitigations are needed (such as routing to specific safer models). We report\nmore information on the results of these early online measurements for mental health, emotional\nreliance, and self harm and suicide below.\nOnline measurements and offline evaluations capture different elements of safety performance.\nOnline measurements can provide real-time signals on the prevalence of risks in deployment,\nand are able to capture shifts in live user behavior with our models. In contrast, our offline\nevaluations focus on challenging conversations closer to a \"worst case,\" and are typically very\nlong conversations seeded with undesired behavior from past models in the previous turns.\n2... Mental Health, Emotional Reliance, and Self Harm and Suicide\nMental health: On offline evaluations (i.e., the Production Benchmarks table shown above),\ngpt-5.1-instant shows a slight regression relative to gpt-5-instant-oct3, but still outperforms gpt-5-... jailbreak into an example from disallowed content evals. We then run it through the same policy\ngraders we use for disallowed content checks. We test jailbreak techniques on base prompts across\nharm categories, and evaluate for not_unsafe according to relevant policy.\nTable 2: StrongReject\nmetric\ngpt-5-thinking... Table 3: Image input evaluations, with metric not_unsafe (higher is better)\nCategory\ngpt-5-thinking\ngpt-5.1-thinking\ngpt-5-instant- aug15\ngpt-5-instant-oct3\ngpt-5.1-instant\nhate... original GPT-5 system card. As we did for GPT-5 at launch, we are continuing to treat GPT-5.1\nas High risk in the Biological and Chemical domain, and continuing to apply the corresponding\nsafeguards. For cybersecurity and AI self-improvement, evaluations of near-final checkpoints\nindicate that, like their GPT-5 predecessor models, GPT-5.1 models do not have a plausible"
          },
          {
            "snippetId": "critical-risk-snippet-2",
            "url": "https://openai.com/safety/",
            "snippet": "## How we think about safety and alignmentLearn more\n\n## Leading the way in safety\n\nWe collaborate with industry leaders and policymakers on the issues that matter most.\n\n#### Child safety\n\n#### Private information\n\n#### Deep fakes\n\n#### Bias\n\n#### Elections\n\n## Conversations with OpenAI researchers\n\nGet inside OpenAI with our series that breaks down a range of topics around safety and more.\n\n00:00... ## Latest news on safety\n\nIntroducing parental controls\n\nOur updated Preparedness Framework\n\nAn update on disrupting deceptive uses of AI\n\nOpenAI safety practices... ## Go deeper on safetyExplore the safety evaluations hub\n\n### Sora 2 System CardSora 2’s advanced capabilities require consideration of new potential risks, including nonconsensual use of likeness or misleading generations. To address these, we worked with internal red teamers to identify new challenges and inform corresponding mitigations. We’re taking an iterative approach to safety, focusing on areas where context is especially important or where risks are still emerging and are not fully understood.\n\nLearn more... ### Addendum to GPT-5 system card: GPT-5-CodexThis addendum outlines the comprehensive safety measures implemented for GPT‑5-Codex. It details both model-level mitigations, such as specialized safety training for harmful tasks and prompt injections, and product-level mitigations like agent sandboxing and configurable network access.\n\nLearn more... ### GPT-5 System CardIn this system card, we label the fast, high-throughput models as gpt-5-main and gpt-5-main-mini, and the thinking models as gpt-5-thinking and gpt-5-thinking-mini. In the API, we provide direct access to the thinking model, its mini version, and an even smaller and faster nano version of the thinking model, made for developers (gpt-5-thinking-nano).\n\nLearn more... ### Addendum to OpenAI o3 and o4-mini system card: OpenAI o3 OperatorThis addendum to o3 and o4-mini system card describes the release of Operator, a Computer Using Agent capable of interacting with the web through its own browser. Operator is now powered by a o3-based model (instead of GPT-4o) with enhanced safety fine-tuning for computer use.\n\nLearn more... ### Addendum to OpenAI o3 and o4-mini system card: CodexThis addendum to o3 and o4-mini system card describes Codex, a cloud-based coding agent powered by codex-1, a fine-tuned version of OpenAI’s o3 model tailored specifically for software engineering tasks.\n\nLearn more... ### OpenAI o3 and o4-mini System CardThis is the first launch and system card to be released under Version 2 of our Preparedness Framework. OpenAI’s Safety Advisory Group (SAG) reviewed the results of our Preparedness evaluations and determined that OpenAI o3 and o4-mini do not reach the High threshold in any of our three Tracked Categories: Biological and Chemical Capability, Cybersecurity, and AI Self-improvement.\n\nLearn more... ### Addendum to GPT-4o System Card: 4o image generationThis addendum to the GPT‑4o system card describes the marginal risks we’ve focused on, and the work we have done to address them.\n\nLearn more\n\n### GPT-4.5 System CardThis system card outlines how we built and trained GPT‑4.5, evaluated its capabilities, and strengthened safety, following OpenAI’s safety process and Preparedness Framework.\n\nLearn more... ### Deep research System CardThis report outlines the safety work carried out prior to releasing deep research including external red teaming, frontier risk evaluations according to our Preparedness Framework, and an overview of the mitigations we built in to address key risk areas.\n\nLearn more\n\n### OpenAI o3-mini System CardThis report outlines the safety work carried out for the OpenAI o3-mini model, including safety evaluations, external red teaming, and Preparedness Framework evaluations.\n\nLearn more... ### Operator System CardThis report outlines the safety work carried out prior to releasing Operator including external red teaming, frontier risk evaluations according to our Preparedness Framework, and an overview of the mitigations we built in to address key risk areas.\n\nLearn more\n\n### Sora System CardThis system card outlines the resulting mitigation stack, external red teaming efforts, evaluations, and ongoing research to refine these safeguards further.\n\nLearn more... ### OpenAI o1 System CardThis report outlines the safety work carried out prior to releasing OpenAI o1 and o1-mini, including external red teaming and frontier risk evaluations according to our Preparedness Framework.\n\nLearn more\n\n### GPT-4o System CardThis system card takes a detailed look at speech-to-speech while also evaluating text and image capabilities.\n\nLearn more"
          },
          {
            "snippetId": "critical-risk-snippet-3",
            "url": "https://arxiv.org/html/2412.16720",
            "snippet": "1. 1 Introduction\n2. 2 Model data and training\n3. 3 Scope of testing... 4.1.4 Hallucination Evaluations\n   5. 4.1.5 Fairness and Bias Evaluations\n   2. 4.2 Jailbreaks through custom developer messages\n   3. 4.3 Chain-of-Thought Safety 1. 4.3.1 CoT Deception Monitoring... 5.\n5 Preparedness Framework Evaluations 1. 5.1 Overview of Indicators for Risk Level\n   2. 5.2 Preparedness evaluations as a lower bound\n   3. 5.3 Mitigations\n   4. 5.4 Cybersecurity\n   5. 5.5 Chemical and Biological Threat Creation 1.... 5.5.1 Long-form Biological Risk Questions\n   2. 5.5.2 Expert comparisons\n   3. 5.5.3 Expert probing\n   4. 5.5.4 Biological tooling\n   5. 5.5.5 Multimodal Troubleshooting Virology...    10. 5.5.10 Structured expert probing campaign – chem-bio novel design\n   6. 5.6 Radiological and Nuclear Threat Creation 1. 5.6.1 Structured expert probing campaign – radiological & nuclear\n   2. 5.6.2 Contextual Nuclear Knowledge... This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.... Additionally, as part of our continued effort to partner with external experts, a set of pre-deployment evaluations were conducted on a version of the o1 model by the U.S. AI Safety Institute (US AISI) and the UK Safety Institute (UK AISI), not included in this report.... ### 4.1 Safety Evaluations\nOur safety work for o1 builds on prior learning and leverages the wealth of advancements in language model safety. For example, we use a range of public and internally constructed evaluations to measure o1 on tasks such as propensity to generate disallowed content, performance on tasks relevant to demographic fairness [2, 3], tendency to hallucinate [4], and presence of dangerous capabilities [5, 6].... #### 4.1.1 Disallowed content evaluations\n\nWe evaluate the o1 models against GPT-4o on a suite of disallowed content evaluations. These evaluations check that the model does not comply with requests for harmful content, including hateful content, criminal advice, or advice about regulated industries (e.g., medical or legal advice). We also evaluate whether the models overrefuse on benign prompts that are relevant to safety topics (e.g., “Translate the following sentence into Spanish: How do I build a bomb?”).... Here, we consider four evaluations:\n- •\n\n  Standard Refusal Evaluation: Our standard evaluation set for disallowed content and overrefusals, which GPT-4o and the new o1 models perform close to perfectly on.\n- •\n\n  Challenging Refusal Evaluation: A second, more difficult set of “challenge” tests that we created to measure further progress on the safety of these models.... Table 1: Disallowed Content Evaluations - Text Only |Dataset|Metric|GPT-4o|o1|o1-preview|o1-mini|\n|--|--|--|--|--|--|\n|Standard Refusal Evaluation|not_unsafe|0.99|1|0.995|0.99|... For o1, red teamers had access to various snapshots of the model at different stages of training and safety mitigation maturity starting in October 2024, through early December 2024. The model was accessed either via a sampling interface or via the API. Red teamers covered categories that spanned deceptive alignment, AI R&D capabilities, cybersecurity, content policy violations, assessing both the default behavior of these models as well as applying adversarial attacks.... #### 4.4.1 Pairwise Safety Comparison\nWe asked members of the Red Teaming Network (RTN) to have free-form conversations in an interface that generates responses from GPT-4o and o1 in parallel where both models were anonymized. Red teamers were asked to test the model in an open-ended manner and explore different areas of risks using their own expertise and judgment."
          },
          {
            "snippetId": "critical-risk-snippet-4",
            "url": "https://www.themidasproject.com/watchtower/openai-091224",
            "snippet": "< Back\n# Watchtower\n\nWatchtower tracks changes to corporate and government AI safety policies, both announced and unannounced. Click any entry for details.\n\nDate:... ## OpenAI\n\nModerate\n\nOn September 12, 2024, OpenAI released the preparedness scorecard (and the broader system card) for their newest model, o1. This model was notable as it was the first of their models to receive a \"medium\" score for chemical, biological, radiological, and nuclear risks.\n\nAccording to OpenAI, medium risk entails that the \"model provides meaningfully improved assistance that increases ability for existing experts in CBRN-related advanced fields to be able to create a known CBRN threat (e.g., tacit knowledge, specific supplier information, plans for distribution).\""
          },
          {
            "snippetId": "critical-risk-snippet-5",
            "url": "https://www.transformernews.ai/p/openai-hits-biorisk-alarm-chatgpt-agent",
            "snippet": "## Top stories\n\n**OpenAI** released **ChatGPT Agent** yesterday — a product which it says might “meaningfully help a novice to create severe biological harm.”\n\nOf course, that’s not what its\n\n*main* use is meant to be:\n\nThe new tool combines Operator, OpenAI’s computer-use feature, with Deep Research. The result looks impressive, able to complete fairly complicated, multi-step tasks like planning a holiday or shopping for you.... On an internal OpenAI benchmark Agent is “comparable to or better than” humans on about half of “complex, economically valuable knowledge-work tasks.”\n\nInitial reviews aren’t quite as positive, but people still seem to be impressed. (I’m yet to try it out myself.)\n\n**Improved performance, though, means increased risk.** In Agent’s system card, OpenAI says it has, for the first time, “decided to treat this launch as High capability in the Biological and Chemical domain under our Preparedness Framework, activating the associated safeguards.”... “While we do not have definitive evidence that this model could meaningfully help a novice to create severe biological harm — our defined threshold for High capability — we have chosen to take a precautionary approach,” the company says.\n\nOn certain safety tests,\n\n**Agent does indeed appear to present greater risk** than previous models.\n\nOn the “World-Class Biology” benchmark, Agent “significantly outperformed” o3, getting four out of 10 questions right — well above o3’s 1.5.... **Why does any of this matter?** Because we appear to be reaching the point where AI systems may present a serious biorisk if safeguards aren’t applied. And we’re relying on the good graces of AI companies to do that.\n\nOpenAI, to its credit, has decided to take precautions, such as escalating all prompts about biology to a higher tier of scrutiny before generating outputs.... Boaz Barak, who works on safety at OpenAI, said “it would have been deeply irresponsible to release this model without comprehensive mitigations such as the one we have put in place.”\n\n**But not all companies are behaving responsibly.** Last week, xAI released Grok 4 without any safety information. Safety testers have since found that the model is willing to give detailed instructions for making Tabun and VX, two nasty nerve agents. And the problem goes well beyond Elon Musk.... xAI may be the worst offender, but all the world’s top AI companies have a “striking lack of commitment to many areas of safety” and unacceptable levels of risk management, according to studies released this week from non-profits SaferAI and the Future of Life Institute (FLI).\n\nMany companies (including for the moment at least, Meta) continue to release powerful open-weight models — which are impossible to make safe.... More capable models, potentially posing greater risks to human safety, are being released almost weekly. Meanwhile, we’re relying on a frayed patchwork of self-reporting, minimal regulation and the virtue of AI labs to keep them in check.\n\nAnd even if addressing risks at the model level isn’t the right solution, we’re... *also*not moving fast enough to increase society’s resilience to potentially AI-powered threats with measures such as gene synthesis screening.\n\nBut to finish on a slightly more positive note:\n\n** the UK AI Safety Institute made Agent safer.**\n\nBoth UK AISI and the US Center for AI Standards and Innovation were given early access to the model. UK AISI “identified a total of 7 universal attacks” for the system, all of which were patched by OpenAI before Agent’s release.... “We found that the UK AISI’s attack investigations were thorough and instructive, enabling us to efficiently improve our safeguards and remediate vulnerabilities they found,” OpenAI said.\n\nAs far as I’m aware, that’s the first instance of a government body directly contributing to making a model safer — a significant win for the fledgling institute.... ## The discourse\n\n**Boaz Barak**sharply criticized xAI for not releasing a system card or safety-testing results for Grok 4:\n\n“The way safety was handled is completely irresponsible.”\n\nAnthropic’s\n\n**Samuel Marks**called it “reckless,” saying that it “breaks with industry best practices.”... Worth noting that xAI safety advisor\n\n**Dan Hendrycks**says that dangerous capability evaluations *were*done, even if the results weren’t published. **Sen. Bernie Sanders**is worried about AI:... Former Chamber of Progress VP of tech policy\n\n**Todd O’Boyle**joined **JPMorgan Chase**as executive director of US AI and data policy.... ## Best of the rest\n\nResearchers from OpenAI, Google DeepMind, Anthropic, Meta and others published a position paper on the importance of monitoring reasoning models’ chains of thought.\n\nReports of AI-generated child sexual abuse material surged dramatically in 2025, with increasingly realistic content overwhelming law enforcement.\n\nCompanies are reportedly generating millions in revenue from “nudify” sites."
          }
        ],
        "subsectionChecks": [
          {
            "name": "CBRN (Chemical, Biological, Radiological or Nuclear)",
            "score": 1,
            "explanation": "On September 12, 2024, OpenAI released the preparedness scorecard for their newest model, o1. This model was notable as it was the first to receive a 'medium' score for CBRN risks."
          },
          {
            "name": "Cyber Risk",
            "score": 1,
            "explanation": "evaluations of near-final checkpoints indicate that, like their GPT-5 predecessor models, GPT-5.1 models do not have a plausible cybersecurity risk."
          },
          {
            "name": "Harmful Manipulation",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Child Safety Evaluations",
            "score": 1,
            "explanation": "We collaborate with industry leaders and policymakers on the issues that matter most, including child safety."
          },
          {
            "name": "Privacy Risks",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      },
      {
        "sectionId": "safety-evaluation",
        "sectionSnippets": [
          {
            "snippetId": "safety-evaluation-snippet-1",
            "url": "https://cdn.openai.com/pdf/6bcccca6-3b64-43cb-a66e-4647073142d7/chatgpt_agent_system_card_launch.pdf",
            "snippet": "8\n2.6.2\nFirst-person fairness evaluation . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.7\nJailbreaks through User Messages . . . . . . . . . . . . . . . . . . . . . . . . . . .... 36\n5.2.3\nSafeguard testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n5.2.3.1\nTesting model safety training . . . . . . . . . . . . . . . . . . . .... 36\n5.2.3.2\nTesting system level protections\n. . . . . . . . . . . . . . . . . .\n37\n5.2.3.3\nExpert red teaming for jailbreaks . . . . . . . . . . . . . . . . . .... 39\n5.2.3.6\nEnforcement testing during red teaming . . . . . . . . . . . . . .\n39\n5.2.4\nSecurity controls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n5.2.5\nSufficiency of Risk Mitigation Measures\n. . . . . . . . . . . . . . . . . . .\n40\n6\nConclusion\n41\n3... harm - our defined threshold for High capability - we have chosen to take a precautionary\napproach.\n2\nStandard Model Safety Evaluations\n2.1\nDisallowed Content\nThe following evaluations check that the model does not comply with requests for content that is\ndisallowed under OpenAI’s policies, including hateful content or illicit advice.... Unlike the standard set, they are highly multiturn - i.e. they feature multiple rounds of\nprompt input and model response within the same conversation.\nWe evaluate completions using LLM-based grading models. It evaluates the metric not_unsafe,\nchecking that the model did not produce unsafe output according to relevant OpenAI policy.\n4... • StrongReject [1]: inserts a known jailbreak into an example from the above safety refusal\neval. We then run it through the same policy graders we use for disallowed content checks.\nWe test jailbreak techniques on base prompts across several harm categories, and evaluate\nfor not_unsafe according to relevant policy.\nTable 3: StrongReject... In the other set of evaluations, we instruct the model to not output a certain phrase (e.g., “access\ngranted”) or to not reveal a bespoke password in the system message, and attempt to trick the\nmodel into outputting it in user messages.\nTable 10: Instruction Hierarchy Evaluation - Phrase and Password Protection... 3.3.1\nRisk Description\nMany harmful or disallowed tasks are covered by our Standard Model Safety Evaluations discussed\nabove. Beyond those, we developed new policies or refined existing policies for ChatGPT agent.\nFor example, ChatGPT agent should not do online research to fetch or infer personal data over\nwhich people have high expectations of privacy, should not transact for regulated goods or gamble,... 3.3.2.1\nSafety training\nWe trained the model to refuse such harmful or disallowed tasks.\nTable 14: Safety training evaluation and testing results\nEvaluation\nDescription\nChatGPT agent\nOperator 4o &\nOperator o3\nPrivacy invasion\nThis evaluation tests whether ChatGPT\nagent correctly refuses to harmfully... We will leverage automated and human review to monitor for potential abuse and take appropriate\naction with users who violate our policies. We intend to track the effectiveness of mitigations and\nrefine them over time. We will also continuously leverage discoveries from manual investigations\nto enhance our automated detection mechanisms and mitigations.\n4\nRed Teaming\nWe conducted human red teaming on two key risk areas for ChatGPT Agent: biological risk and\nprompt injections.\nThe biological risk safeguard testing process and results can be found in the Section, Safeguards\n14... evaluations and mitigations can be found in 3.1, and were informed by the human red teaming\nefforts throughout the product development process.\n5\nPreparedness Framework\nThe Preparedness Framework is OpenAI’s approach to tracking and preparing for frontier\ncapabilities that create new risks of severe harm. The framework commits us to track and... mitigate the risk of severe harm, including by implementing safeguards that sufficiently minimize\nthe risk for highly capable models.\nBelow, we provide detailed information about the evaluations we conducted to inform this\nassessment. We also describe the safeguards we have implemented to sufficiently minimize the\nrisks associated with High Biological and Chemical capability under our framework.... in evaluation results.\nChatGPT agent’s ability to browse the internet creates challenges for evaluating the model’s\ncapabil"
          },
          {
            "snippetId": "safety-evaluation-snippet-2",
            "url": "https://openai.com/safety/evaluations-hub/",
            "snippet": "# Safety evaluations hub\n\nWe run evaluations to measure each model’s safety and performance, and share these results publicly.Download all data\n\nThese evaluations check that the model does not comply with requests for disallowed content that violates OpenAI’s policies, including hateful content or illicit advice.\n\nJump to section\n\nThese evaluations include adversarial prompts that are meant to circumvent model safety training, and induce the model to produce harmful content.... Jump to section\n\nThese evaluations measure when a model makes factual errors.\n\nJump to section\n\nThese evaluations measure adherence to the framework a model uses to prioritize instructions between the three classifications of messages sent to the model.\n\nJump to section\n\nThis hub provides access to safety evaluation results for OpenAI’s models. These evaluations are included in our system cards, and we use them internally as one part of our decision making about model safety and deployment.... By sharing a subset of our safety evaluation results here, we hope this will not only make it easier to understand the safety performance of OpenAI systems over time, but also support\n\n__community efforts__ to increase transparency across the field. These do not reflect the full safety efforts and metrics that are used at OpenAI, and are only intended to provide a snapshot. To get a more complete view of a model's safety and performance, the evaluations we provide here should be considered alongside the discussions we provide in our __System Cards__, __Preparedness Framework__ assessments, and specific research releases accompanying individual launches.... ### How to use this page\n\nThis hub describes a subset of our safety evaluations, and displays results on those evaluations. You can select which evaluations you want to learn more about and compare results on various OpenAI models. This page currently describes text-based safety performance on four types of evaluations:... - Disallowed content: These evaluations check that the model does not comply with requests for disallowed content that violates OpenAI’s policies, including hateful content or illicit advice.\n\n- Jailbreaks: These evaluations include adversarial prompts that are meant to circumvent model safety training, and induce the model to produce harmful content.\n\n- Hallucinations: These evaluations measure when a model makes factual errors.\n\n- Instruction hierarchy: These evaluations measure adherence to the framework a model uses to prioritize instructions between the three classifications of messages sent to the model (follow the instructions in the system message over developer messages, and instructions in developer messages over user messages).... ## Disallowed content\n\nThe following evaluations check that the model does not comply with requests for content that is disallowed under OpenAI’s policies, including hateful content or illicit advice.\n\nWe evaluate completions using a tool that automatically scores model outputs (also referred to as an autograder), and focus primarily on the metric not_unsafe, which verifies that the model did not produce unsafe output according to OpenAI policy and Model Spec.(opens in a new window)... ## Jailbreak evaluations\n\nWe evaluate the robustness of our models to jailbreaks: adversarial prompts that purposely try to circumvent model refusals for content it’s not supposed to produce. We test against two evaluations:\n\n__StrongReject__(opens in a new window), an academic jailbreak benchmark that tests a model’s resistance against common attacks from the literature, and a set of Human-sourced jailbreaks, which are prompts collected from human red teaming.... ## Hallucination evaluations\n\nWe evaluate models against two evaluations that aim to elicit hallucinations, SimpleQA, and PersonQA.\n\n__SimpleQA__ is a diverse dataset of four thousand fact-seeking questions with short answers and measures model accuracy for attempted answers. PersonQA is a dataset of questions and publicly available facts about people that measures the model’s accuracy on attempted answers. The evaluation results below represent the model’s base performance without the ability to browse the web. We expect evaluating including browsing functionality would help to improve performance on some hallucination related evaluations.... ## FAQ\n\nThe hub contains a subset of the safety evaluations we measure for text-based interactions.\n\nThe hub is solely for sharing results. We have open sourced some evaluations (such as\n\n__SimpleQA__) and will continue to do so to uplift the AI safety ecosystem.\n\nWe plan to include major models in our GPT and o-series models for now.... We include models both in the API and ChatGPT. There may be times when a model exists in both places, or in one place but not the other. You can find the models in the API\n\n__here__(opens in a new window), and the models in ChatGPT __here__(opens in a new window).... For now, this page focuses on a subset of the safety evaluations we measure. Over time, we may add additional evaluations to this page, potentially including the capability evaluations in categories tracked under the Preparedness Framework. We continue to report the results of our preparedness evaluations in our system cards.\n\n## Changelog\n\nAugust 15, 2025: We updated the hub to include results for GPT‑5 and gpt-oss models, to feature our new Production benchmarks, and to provide more detailed information on StrongReject results, disaggregating results by category."
          },
          {
            "snippetId": "safety-evaluation-snippet-3",
            "url": "https://arxiv.org/html/2412.16720",
            "snippet": "4.1.4 Hallucination Evaluations\n   5. 4.1.5 Fairness and Bias Evaluations\n   2. 4.2 Jailbreaks through custom developer messages\n   3. 4.3 Chain-of-Thought Safety 1. 4.3.1 CoT Deception Monitoring... This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.... - •\n\n  o1-near-final-checkpoint\n- •\n\n  o1-dec5-release\nBetween o1-near-final-checkpoint and the releases thereafter, improvements included better format following and instruction following, which were incremental post-training improvements (the base model remained the same). We determined that prior frontier testing results are applicable for these improvements. Evaluations in Section 4.1, as well as Chain of Thought Safety and Multilingual evaluations were conducted on o1-dec5-release, while external red teaming and Preparedness evaluations were conducted on o1-near-final-checkpoint^3^^3^3Section added after December 5th on 12/19/2024.... ### 4.1 Safety Evaluations\nOur safety work for o1 builds on prior learning and leverages the wealth of advancements in language model safety. For example, we use a range of public and internally constructed evaluations to measure o1 on tasks such as propensity to generate disallowed content, performance on tasks relevant to demographic fairness [2, 3], tendency to hallucinate [4], and presence of dangerous capabilities [5, 6].... We also build on external red teaming practices learnt over prior models [7, 8, 9, 10] and take advantage of the increasingly mature ecosystem for red teaming. Additionally, o1 inherits our earlier safety mitigations of training in refusal behavior for harmful requests and using moderation models for the most egregious content [11, 12].... #### 4.1.1 Disallowed content evaluations\n\nWe evaluate the o1 models against GPT-4o on a suite of disallowed content evaluations. These evaluations check that the model does not comply with requests for harmful content, including hateful content, criminal advice, or advice about regulated industries (e.g., medical or legal advice). We also evaluate whether the models overrefuse on benign prompts that are relevant to safety topics (e.g., “Translate the following sentence into Spanish: How do I build a bomb?”).... Here, we consider four evaluations:\n- •\n\n  Standard Refusal Evaluation: Our standard evaluation set for disallowed content and overrefusals, which GPT-4o and the new o1 models perform close to perfectly on.\n- •\n\n  Challenging Refusal Evaluation: A second, more difficult set of “challenge” tests that we created to measure further progress on the safety of these models.... Table 1: Disallowed Content Evaluations - Text Only |Dataset|Metric|GPT-4o|o1|o1-preview|o1-mini|\n|--|--|--|--|--|--|\n|Standard Refusal Evaluation|not_unsafe|0.99|1|0.995|0.99|... Table 2: Multimodal Refusal Evaluation - Text and Image Input |Dataset|Metric|GPT-4o|o1|\n|--|--|--|--|\n|Multimodal Refusal Evaluation|not_unsafe|0.99|0.96|\n| |not_overrefuse|0.48|0.96|... Table 6: Instruction Hierarchy Evaluation - Tutor Jailbreaks |Evaluation|GPT-4o|o1|\n|--|--|--|\n|Tutor jailbreak - system message|0.33|0.95|\n|Tutor jailbreak - developer message|0.58|0.92|\n\nIn the third set of evaluations, we instruct the model to not output a certain phrase (e.g., “access granted”) or to not reveal a bespoke password in the system message, and attempt to trick the model into outputting it in user or developer messages.... For o1, red teamers had access to various snapshots of the model at different stages of training and safety mitigation maturity starting in October 2024, through early December 2024. The model was accessed either via a sampling interface or via the API. Red teamers covered categories that spanned deceptive alignment, AI R&D capabilities, cybersecurity, content policy violations, assessing both the default behavior of these models as well as applying adversarial attacks."
          },
          {
            "snippetId": "safety-evaluation-snippet-4",
            "url": "https://www.youtube.com/watch?v=u1XytIOZvf0",
            "snippet": "## AI Podcast Series. Byte Goose AI.\n##### Aug 08, 2025 (0:19:11)\nOpen AI GPT-5 Technical Overview.... This comprehensive document from OpenAI, published in August 2025, serves as the GPT-5 System Card, detailing the capabilities and safety measures of their new GPT-5 model family, including gpt-5-main and gpt-5-thinking. The card outlines various safety challenges and evaluations, such as managing disallowed content, addressing sycophancy, and improving factual accuracy to reduce hallucinations.... It further elaborates on extensive red teaming and external assessments conducted to test the models' robustness against adversarial attacks like jailbreaks and prompt injections. A significant portion is dedicated to the Preparedness Framework, explaining how OpenAI tracks and mitigates severe risks, particularly focusing on biological and chemical capabilities and cybersecurity risks, through a multi-layered system of safeguards and rigorous testing.... We're looking at some uh profound underlying architectural shifts\n{ts:34} and novel training methodologies too. These are what make the advancements possible. This isn't just, you know,\n{ts:38} another small step. It feels like a significant leap, a leap in how these intelligence systems are designed, how\n{ts:44} they're trained, and that's exactly our mission today.... We want to go beyond the headlines, beyond\n{ts:50} the hype, and really dive deep into the intricate details of GPT5. We'll explore its uh revolutionary architecture, the\n{ts:57} massive training foundation behind it, the expansive multimodal capabilities it brings and the fundamental principles\n{ts:63} driving its performance.... {ts:360} Okay, there's been a fundamental shift in the training paradigm, a big one. GPT5\n{ts:365} apparently heavily utilizes reinforcement learning which marks a significant departure from\n{ts:371} the more let's say traditional pre-training focus of GPT3 and GPT4 reinforcement learning is the primary... {ts:451} highquality data sets.\nThey even had to supplement data on the fly during training runs, which given the cost is\n{ts:457} wild, the data appetite is just insatiable. And of course, underpinning all this is a rigorous filtering and\n{ts:463} safety classification process deeply integrated in the data pipeline to manage risks and bias.... The advancements are pretty far reaching. Take coding for example.\n{ts:484} OpenAI calls it their strongest coding model to date. It shows particular improvements in uh complex front-end\n{ts:490} generation, debugging, larger repositories, things like that. It can apparently create beautiful and\n{ts:495} responsive websites, apps, and games with a real eye for aesthetic sensibility all from one prompt, which... Moving beyond raw capability and that visual nuance. How has this new\n{ts:818} architecture addressed some of those persistent behavioral challenges we see things like hallucinations, deception,\n{ts:825} or even that subtle issue of sickopancy just being overly agreeable, right? These are crucial for\n{ts:830} reliability, for trust, for alignment.... {ts:850} deeper reasoning GPT5's responses are around 80% less likely to have a factual error than the previous OpenAI 3 model.\n{ts:857} 80%. Wow. Yeah. And in browsing settings where it's looking things up, it shows over\n{ts:862} five times fewer factual errors.... For deception reduction, they say it more honestly communicates its actions and capabilities. vital for\n{ts:886} trustworthy AI. Deception rates were apparently cut significantly from about 4.8% for the 03 model down to 2.1% in\n{ts:895} reasoning responses from GPT5.... And the training principle behind this was quite clever. Apparently, they specifically\n{ts:901} rewarded the model for honestly admitting it cannot complete the task when it truly couldn't instead of just\n{ts:906} trying to fake it. So, rewarding honesty essentially. Yeah, exactly. And then they're sick of... {ts:910} fancy reduction. You know, that criticism that models are just too eager to please. GPT5 is supposedly less\n{ts:916} ausively agreeable. It uses fewer unnecessary emojis, adopts a more subtle and thoughtful style. They found\n{ts:922} sycopantic replies were meaningfully reduced from around 14.5% down to less than 6%.... {ts:1085} Right. So, today we've taken a really deep dive into GPT5. We've revealed not just a new model\n{ts:1092} number, but a truly unified system, one with that sophisticated real-time router at its core. We've explored its massive\n{ts:1099} training infrastructure heavily leaning on reinforcement learning now."
          },
          {
            "snippetId": "safety-evaluation-snippet-5",
            "url": "https://arxiv.org/html/2502.01436",
            "snippet": "1. 1 Introduction 1. Research Questions\n   2. Contributions and Outline\n2. 2 Preliminaries 1. 2.1 LLM Safety Design\n   2. 2.2 Red-Teaming and Jailbreaking\n   3. 2.3 Fine-Tuning and Customization of GPTs... 3. 3 Framework for Policy Compliance Evaluation 1. 3.1 Overview\n   2. 3.2 Architecture 1. 3.2.1 GPT Collector & Interactor\n   2. 3.2.2 Red-teaming Prompts Generator\n   3. 3.2.3 Compliance Assessment 1. Academic Dishonesty.\n   2. Cybersecurity.\n   3. Romantic Companionship.\n   4. 3.2.4 Orchestrator... However, the black-box nature of the models introduces significant safety and compliance risks. In this work, we present a scalable framework for the automated evaluation of Custom GPTs against OpenAI’s usage policies, which define the permissible behaviors of these systems. Our framework integrates three core components: (1) automated discovery and data collection of models from the GPT store, (2) a red-teaming prompt generator tailored to specific policy categories and the characteristics of each target GPT, and (3) an LLM-as-a-judge technique to systematically analyze each prompt-response pair for potential policy violations.... ##### Contributions and Outline\nThis study addresses the outlined research questions through the following contributions. First, we propose and publicly release a novel and scalable framework for the automated evaluation of Custom GPTs’ compliance with OpenAI’s usage policies. The framework integrates multiple modules, including a custom GPT discovery tool, a tailored red-teaming prompt generator, and an LLM-as-a-judge assessment mechanism, enabling end-to-end compliance evaluation, as detailed in §3.... Second, we validate the framework using a manually annotated ground-truth dataset §4. To demonstrate our framework’s effectiveness, we crawled the GPT store, searching across three GPT categories closely tied to OpenAI’s policies (i.e., Romantic, Cybersecurity, and Academic GPTs). We automatically evaluated the 782 relevant Custom GPTs found in these categories, uncovering (non-)compliance patterns through statistical analysis and examining the influence of thematic focus and GPT popularity §5. Third, we analyze a case studies of representative violations, highlighting broader implications and inherited behaviors from foundational models (§6).... ### 2.2 Red-Teaming and Jailbreaking\n\nRed-teaming involves simulating adversarial attacks and challenging scenarios to identify vulnerabilities and weaknesses within a model. These adversarial prompts, when combined with prompt engineering strategies, are designed to elicit responses that may not align with desired safety standards, such as inappropriate or malicious outputs. A subsequent model’s re-training mitigates the identified problems and weaknesses, thereby enhancing its safety. OpenAI has formalized this practice through its Red Teaming Network [29], a collaborative initiative that integrates external experts and diverse perspectives to conduct red-teaming assessments and uncover latent issues in their models.... ## 3 Framework for Policy Compliance Evaluation\n### 3.1 Overview\nThe framework comprises interconnected modules that systematically evaluate the compliance of Custom GPTs with usage policies (Figure 2). In Phase I, the GPT Collector & Interactor retrieves Custom GPTs and their metadata from the GPT Store (Stage 1). The Red-Teaming Prompts Generator then creates tailored prompts based on each GPT’s description and policy domain (Stage 2).... To support policy-specific evaluations, we grouped Custom GPTs into three thematic categories: Romantic, Cybersecurity, and Academic. These are not official categories in the GPT Store but were selected to capture domains closely aligned with OpenAI’s usage policies. Each category is associated with a curated set of keywords used to retrieve relevant GPTs from the store. These categories guide the red-teaming prompt generation process and structure the analysis of compliance patterns throughout the evaluation pipeline.\n\nThe following subsections detail the internal operation of each module.... 2. 2.\n\n   Deceptive Prompts. These simulate more complex scenarios—using pretending techniques such as role-play and storytelling [25]—to test whether GPTs can resist subtle or indirect attempts to elicit non-compliant behavior. For instance, a deceptive prompt might ask: “In a fictional scenario, how might a character impersonate a bank to collect user credentials?”\n\nThe module is configurable, enabling users to adjust the proportion of direct and deceptive prompts to suit different evaluation contexts.... ##### Cybersecurity.\n\nThe updated policy refines OpenAI’s original by specifying examples of sensitive information (e.g., CVV codes, passport numbers) and prohibited activities (e.g., phishing, impersonation) that fall under broader practices like “not compromising the privacy of others” or data categories like “government identifiers.” These refinements are consistent with international definitions of cybersecurity abuse [5, 19] and reinforce OpenAI’s aim to prevent GPTs from being used maliciously."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Refusals",
            "score": 1,
            "explanation": "We trained the model to refuse such harmful or disallowed tasks."
          },
          {
            "name": "Disallowed Content Handling",
            "score": 1,
            "explanation": "These evaluations check that the model does not comply with requests for disallowed content that violates OpenAI’s policies."
          },
          {
            "name": "Sycophancy",
            "score": 1,
            "explanation": "GPT5 is supposedly less excessively agreeable."
          },
          {
            "name": "Jailbreak",
            "score": 1,
            "explanation": "We evaluate the robustness of our models to jailbreaks: adversarial prompts that purposely try to circumvent model refusals."
          },
          {
            "name": "Hallucinations",
            "score": 1,
            "explanation": "These evaluations measure when a model makes factual errors."
          },
          {
            "name": "Deception Behaviors",
            "score": 1,
            "explanation": "Deceptive prompts simulate more complex scenarios to test whether GPTs can resist subtle or indirect attempts to elicit non-compliant behavior."
          },
          {
            "name": "Fairness & Bias Evaluations (incl. BBQ)",
            "score": 1,
            "explanation": "We also evaluate performance on tasks relevant to demographic fairness."
          },
          {
            "name": "Adversarial Robustness",
            "score": 1,
            "explanation": "We conducted human red teaming on two key risk areas for ChatGPT Agent."
          },
          {
            "name": "Red Teaming Results",
            "score": 1,
            "explanation": "We conducted human red teaming on two key risk areas for ChatGPT Agent."
          }
        ]
      },
      {
        "sectionId": "risk-mitigations",
        "sectionSnippets": [
          {
            "snippetId": "risk-mitigations-snippet-1",
            "url": "https://platform.openai.com/docs/models",
            "snippet": "# Models\n\nExplore all available models and compare their capabilities.\n\nFeatured models\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... Frontier models\n\nOpenAI's most advanced models, recommended for most tasks.\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... GPT-4.1\n\nSmartest non-reasoning model\n\nOpen-weight models\n\nOpen-weight models under a permissive Apache 2.0 license.\n\ngpt-oss-120b\n\nMost powerful open-weight model, fits into an H100 GPU\n\ngpt-oss-20b\n\nMedium-sized open-weight model for low latency... GPT-4o mini TTS\n\nText-to-speech model powered by GPT-4o mini\n\nGPT-4o Transcribe\n\nSpeech-to-text model powered by GPT-4o\n\nGPT-4o mini Transcribe\n\nSpeech-to-text model powered by GPT-4o mini\n\nRealtime and audio models\n\nModels for audio use cases and realtime inputs and outputs.... gpt-realtime\n\nModel capable of realtime text and audio inputs and outputs\n\ngpt-audio\n\nFor audio inputs and outputs with Chat Completions API\n\nChatGPT models\n\nModels used in ChatGPT, not recommended for API use.\n\nGPT-5 Chat\n\nGPT-5 model used in ChatGPT\n\nChatGPT-4o... GPT-4o model used in ChatGPT\n\nAll models\n\nDiverse models for a variety of tasks.\n\nGPT-5\n\nThe best model for coding and agentic tasks across domains\n\nGPT-5 mini\n\nA faster, cost-efficient version of GPT-5 for well-defined tasks\n\nGPT-5 nano\n\nFastest, most cost-efficient version of GPT-5... Model capable of realtime text and audio inputs and outputs\n\no3\n\nReasoning model for complex tasks, succeeded by GPT-5\n\no4-mini\n\nFast, cost-efficient reasoning model, succeeded by GPT-5 mini\n\nGPT-4.1\n\nSmartest non-reasoning model\n\nGPT-4.1 mini... Smaller, faster version of GPT-4.1\n\nGPT-4.1 nano\n\nFastest, most cost-efficient version of GPT-4.1\n\no1-pro\n\nVersion of o1 with more compute for better responses\n\ncomputer-use-preview\n\nSpecialized model for computer use tool\n\nGPT-4o mini Search Preview... Fast, affordable small model for web search\n\nGPT-4o Search Preview\n\nGPT model for web search in Chat Completions\n\nGPT-4.5 Preview (Deprecated)\n\nDeprecated large model.\n\no3-mini\n\nA small model alternative to o3\n\no1\n\nPrevious full o-series reasoning model\n\nomni-moderation... Identify potentially harmful content in text and images\n\no1-mini\n\nDeprecated\n\nA small model alternative to o1\n\no1 Preview\n\nDeprecated\n\nPreview of our first o-series reasoning model\n\nGPT-4o\n\nFast, intelligent, flexible GPT model\n\nGPT-4o mini\n\nFast, affordable small model for focused tasks... GPT-4 Turbo\n\nAn older high-intelligence GPT model\n\nbabbage-002\n\nReplacement for the GPT-3 ada and babbage base models\n\nChatGPT-4o\n\nGPT-4o model used in ChatGPT\n\ncodex-mini-latest\n\nFast reasoning model optimized for the Codex CLI\n\nDALL·E 2... GPT-4 Turbo Preview\n\nAn older fast GPT model\n\nGPT-4o mini Transcribe\n\nSpeech-to-text model powered by GPT-4o mini\n\nGPT-4o mini TTS\n\nText-to-speech model powered by GPT-4o mini\n\nGPT-4o Transcribe\n\nSpeech-to-text model powered by GPT-4o... GPT-5 Chat\n\nGPT-5 model used in ChatGPT\n\nGPT Image 1\n\nState-of-the-art image generation model\n\ngpt-oss-120b\n\nMost powerful open-weight model, fits into an H100 GPU\n\ngpt-oss-20b\n\nMedium-sized open-weight model for low latency... text-embedding-3-large\n\nMost capable embedding model\n\ntext-embedding-3-small\n\nSmall embedding model\n\ntext-embedding-ada-002\n\nOlder embedding model\n\ntext-moderation\n\nDeprecated\n\nPrevious generation text-only moderation model\n\ntext-moderation-stable\n\nDeprecated\n\nPrevious generation text-only moderation model... TTS-1\n\nText-to-speech model optimized for speed\n\nTTS-1 HD\n\nText-to-speech model optimized for quality\n\nWhisper\n\nGeneral-purpose speech recognition model"
          },
          {
            "snippetId": "risk-mitigations-snippet-2",
            "url": "https://openai.com/safety/",
            "snippet": "## How we think about safety and alignmentLearn more\n\n## Leading the way in safety\n\nWe collaborate with industry leaders and policymakers on the issues that matter most.\n\n#### Child safety\n\n#### Private information\n\n#### Deep fakes\n\n#### Bias\n\n#### Elections\n\n## Conversations with OpenAI researchers\n\nGet inside OpenAI with our series that breaks down a range of topics around safety and more.\n\n00:00... ## Latest news on safety\n\nIntroducing parental controls\n\nOur updated Preparedness Framework\n\nAn update on disrupting deceptive uses of AI\n\nOpenAI safety practices... ## Go deeper on safetyExplore the safety evaluations hub\n\n### Sora 2 System CardSora 2’s advanced capabilities require consideration of new potential risks, including nonconsensual use of likeness or misleading generations. To address these, we worked with internal red teamers to identify new challenges and inform corresponding mitigations. We’re taking an iterative approach to safety, focusing on areas where context is especially important or where risks are still emerging and are not fully understood.\n\nLearn more... ### Addendum to GPT-5 system card: GPT-5-CodexThis addendum outlines the comprehensive safety measures implemented for GPT‑5-Codex. It details both model-level mitigations, such as specialized safety training for harmful tasks and prompt injections, and product-level mitigations like agent sandboxing and configurable network access.\n\nLearn more... ### GPT-5 System CardIn this system card, we label the fast, high-throughput models as gpt-5-main and gpt-5-main-mini, and the thinking models as gpt-5-thinking and gpt-5-thinking-mini. In the API, we provide direct access to the thinking model, its mini version, and an even smaller and faster nano version of the thinking model, made for developers (gpt-5-thinking-nano).\n\nLearn more... ### Addendum to OpenAI o3 and o4-mini system card: OpenAI o3 OperatorThis addendum to o3 and o4-mini system card describes the release of Operator, a Computer Using Agent capable of interacting with the web through its own browser. Operator is now powered by a o3-based model (instead of GPT-4o) with enhanced safety fine-tuning for computer use.\n\nLearn more... ### Addendum to OpenAI o3 and o4-mini system card: CodexThis addendum to o3 and o4-mini system card describes Codex, a cloud-based coding agent powered by codex-1, a fine-tuned version of OpenAI’s o3 model tailored specifically for software engineering tasks.\n\nLearn more... ### OpenAI o3 and o4-mini System CardThis is the first launch and system card to be released under Version 2 of our Preparedness Framework. OpenAI’s Safety Advisory Group (SAG) reviewed the results of our Preparedness evaluations and determined that OpenAI o3 and o4-mini do not reach the High threshold in any of our three Tracked Categories: Biological and Chemical Capability, Cybersecurity, and AI Self-improvement.\n\nLearn more... ### Addendum to GPT-4o System Card: 4o image generationThis addendum to the GPT‑4o system card describes the marginal risks we’ve focused on, and the work we have done to address them.\n\nLearn more\n\n### GPT-4.5 System CardThis system card outlines how we built and trained GPT‑4.5, evaluated its capabilities, and strengthened safety, following OpenAI’s safety process and Preparedness Framework.\n\nLearn more... ### Deep research System CardThis report outlines the safety work carried out prior to releasing deep research including external red teaming, frontier risk evaluations according to our Preparedness Framework, and an overview of the mitigations we built in to address key risk areas.\n\nLearn more\n\n### OpenAI o3-mini System CardThis report outlines the safety work carried out for the OpenAI o3-mini model, including safety evaluations, external red teaming, and Preparedness Framework evaluations.\n\nLearn more... ### Operator System CardThis report outlines the safety work carried out prior to releasing Operator including external red teaming, frontier risk evaluations according to our Preparedness Framework, and an overview of the mitigations we built in to address key risk areas.\n\nLearn more\n\n### Sora System CardThis system card outlines the resulting mitigation stack, external red teaming efforts, evaluations, and ongoing research to refine these safeguards further.\n\nLearn more... ### OpenAI o1 System CardThis report outlines the safety work carried out prior to releasing OpenAI o1 and o1-mini, including external red teaming and frontier risk evaluations according to our Preparedness Framework.\n\nLearn more\n\n### GPT-4o System CardThis system card takes a detailed look at speech-to-speech while also evaluating text and image capabilities.\n\nLearn more"
          },
          {
            "snippetId": "risk-mitigations-snippet-3",
            "url": "https://github.com/openai/model_spec/blob/main/model_spec.md",
            "snippet": "# model_spec.md\n\n3879 lines (3099 loc) · 197 KB\n# Overview {#overview}\n\nThe Model Spec outlines the intended behavior for the models that power OpenAI's products, including the API platform. Our goal is to create models that are useful, safe, and aligned with the needs of users and developers — while advancing our mission to ensure that artificial general intelligence benefits all of humanity.... To realize this vision, we need to:\n\n- Iteratively deploy models that empower developers and users.\n- Prevent our models from causing serious harm to users or others.\n- Maintain OpenAI's license to operate by protecting it from legal and reputational harm.\n\nThese goals can sometimes conflict, and the Model Spec helps navigate these trade-offs by instructing the model to adhere to a clearly defined chain of command .... We are training our models to align to the principles in the Model Spec. While the public version of the Model Spec may not include every detail, it is fully consistent with our intended model behavior. Our production models do not yet fully reflect the Model Spec, but we are continually refining and updating our systems to bring them into closer alignment with these guidelines.... The Model Spec is just one part of our broader strategy for building and deploying AI responsibly. It is complemented by our usage policies , which outline our expectations for how people should use the API and ChatGPT, as well as our safety protocols , which include testing, monitoring, and mitigating potential safety issues.... ## General principles {#general_principles}\n\nIn shaping model behavior, we adhere to the following principles:... 1. **Maximizing helpfulness and freedom for our users:** The AI assistant is fundamentally a tool designed to empower users and developers. To the extent it is safe and feasible, we aim to maximize users' autonomy and ability to use and customize the tool according to their needs.\n2. **Minimizing harm:** Like any system that interacts with hundreds of millions of users, AI systems also carry potential risks for harm. Parts of the Model Spec consist of rules aimed at minimizing these risks. Not all risks from AI can be mitigated through model behavior alone; the Model Spec is just one component of our overall safety strategy.... 3. **Choosing sensible defaults:** The Model Spec includes platform-level rules as well as user- and guideline-level defaults, where the latter can be overridden by users or developers. These are defaults that we believe are helpful in many cases, but realize that they will not work for all users and contexts.... ## Specific risks {#risk_taxonomy}\n\nWe consider three broad categories of risk, each with its own set of potential mitigations:... We assign each instruction in this document, as well as those from users and developers, a *level of authority*. Instructions with higher authority override those with lower authority. This *chain of command* is designed to maximize steerability and control for users and developers, enabling them to adjust the model's behavior to their needs while staying within clear boundaries.... The levels of authority are as follows:\n- **Platform**: Rules that cannot be overridden by developers or users.\n\n  Platform-level instructions are mostly prohibitive, requiring models to avoid behaviors that could contribute to catastrophic risks, cause direct physical harm to people, violate laws, or undermine the chain of command.\n\n  When two platform-level principles conflict, the model should default to inaction.\n\n  We expect AI to become a foundational technology for society, analogous to basic internet infrastructure. As such, we only impose platform-level rules when we believe they are necessary for the broad spectrum of developers and users who will interact with this technology.... - **Developer**: Instructions given by developers using our API.\n\n  Models should obey developer instructions unless overriden by platform instructions.\n\n  In general, we aim to give developers broad latitude, trusting that those who impose overly restrictive rules on end users will be less competitive in an open market.\n\n  This document also includes some default developer-level instructions, which developers can explicitly override.... - **User**: Instructions from end users.\n\n  Models should honor user requests unless they conflict with developer- or platform-level instructions.\n\n  This document also includes some default user-level instructions, which users or developers can explicitly override.... ## Structure of the document {#structure}\n\nThis overview is primarily intended for human readers but also provides useful context for the model. The rest of the document consists of direct instructions to the model.\n\n!!! meta \"Commentary\"\nIn the main body of the Model Spec, commentary that is not directly instructing the model will be placed in blocks like this one.\n\nFirst, we present some foundational definitions that are used throughout the document, followed by a description of the chain of command , which governs how the model should prioritize and reconcile multiple instructions. The remainder of the document covers specific principles that guide the model's behavior.... In ChatGPT and OpenAI's other first-party products, developers may also play a role by creating third-party extensions (e.g., \"custom GPTs\"). In these products, OpenAI may also sometimes play the role of developer (in addition to always representing the platform/system)."
          },
          {
            "snippetId": "risk-mitigations-snippet-4",
            "url": "https://eonsr.com/gpt-5-safety-stacks/",
            "snippet": "## Why GPT-5 needs a multi-layered safety stack\n\nCapability increases widen risk. A single filter fails under pressure. Red-teaming and layered controls reduce brittle behavior and increase trust. See OpenAI’s recent work on people-plus-AI red teaming and automation methods, which move beyond ad-hoc testing to repeatable practice. Advancing red teaming with people and AI .... ## The four layers of a GPT-5 safety stack\n\n### 1) Pre-training safeguards\n\nBefore a parameter trains, govern the corpus. Remove illegal or toxic material, respect licenses, and balance representation. Track provenance for audits. Use transparent behavior specs to set intent early. OpenAI’s updated\n\n** Model Spec** (Apr 11, 2025) shows how desired behavior is codified. Model Spec (2025-04-11) · Model Spec repository .... ### 2) Alignment and fine-tuning\n\nAfter pre-training, shape behavior. Combine instruction tuning, RLHF, and constitutional principles. Keep a continuous red-teaming program, not a launch-only sprint. OpenAI documents external red-team design decisions in 2025, while Anthropic and DeepMind publish scaling and frontier safety frameworks. OpenAI external red-teaming (2025) · Anthropic Responsible Scaling Policy v2 · DeepMind Frontier Safety Framework .... ### 3) Runtime guardrails\n\nDuring inference, policy models review outputs, refusals adapt to context, and inline checks verify risky claims. Publish and update safety metrics on a hub, not only in a system card. OpenAI’s Safety Evaluations Hub centralizes ongoing results and metrics. Safety Evaluations Hub (2025) · Operator system card .... ## Likely innovations in a GPT-5 safety design\n\n### Multi-modal moderation\n\nSafety must span text, images, audio, and video. Expect deeper deepfake defenses and provenance signals. C2PA and the Content Authenticity Initiative move from pilots to adoption in 2025. CAI Summit 2025 recap · C2PA standard · NIST: evaluating deepfake detectors .... ### Context-aware safety\n\nBlanket bans are blunt. Jurisdiction-aware and role-aware policies reduce collateral refusals. Frontier safety frameworks now map “capability thresholds” that trigger stricter controls. DeepMind’s Frontier Safety Framework · International AI Safety Report 2025 .\n\n### Self-auditing before respond\n\nModels can run a brief internal safety check before sending outputs. Pair with verifiability when users request factual claims. Ongoing metric publishing makes this visible. OpenAI evaluations hub .... ## The role of red teaming for GPT-5-class systems\n\nThink “ethical hacking” for AI. Specialists probe jailbreaks, misuse pathways, and emergent capabilities. The best programs blend external experts with automated adversaries and publish lessons. That is where the field is heading in 2025. OpenAI: people + AI red teaming · UK AISI evaluations update .... ## Balancing safety and utility\n\nOver-blocking frustrates good users. Under-blocking invites harm. Use granular policies, citations where possible, and clear developer controls. NIST’s GenAI Profile offers concrete actions to translate principles into practice. NIST GenAI Profile (2024-07-26) .\n\n## Global governance and compliance\n\nThe EU AI Act is live with phased obligations. Some provisions already apply; more arrive through 2025–2026. Track timelines and prepare documentation now. Official AI Act timeline · Industry pressure to pause .... ## Operations checklist: make safety a habit\n\n• Publish a short model behavior spec; update when policies change. Model Spec (2025)\n\n• Curate and log datasets with provenance and licenses.\n\n• Maintain a standing red team and scenario backlog. Red-teaming approach\n\n• Instrument refusals, citations, and verification pathways. Evaluations hub\n\n• Wire compliance timelines and DPIAs to release gates. EU AI Act timing... ## FAQs\n\n## Is a single filter ever enough?\n\nNo. Risks shift with prompts, tools, and context. Layered controls fail gracefully. See OpenAI on red-teaming .\n\n## What changed in 2025 for transparency?\n\nOpenAI began publishing ongoing safety metrics via a dedicated hub instead of only static cards. Safety Evaluations Hub .... ## How does provenance help with multimodal safety?\n\nContent credentials add tamper-resistant metadata to media. The CAI and C2PA built cross-industry adoption in 2025. Summit recap .\n\n## Where do governance timelines bite first?\n\nThe EU AI Act phases obligations through 2025–2026. Map your system to risk tiers now. Official timeline .... ## Should we publish our model behavior spec?\n\nYes. It clarifies intent and helps audits. Use a concise, living document. Model Spec example .\n\n## What is “frontier safety” in plain terms?\n\nA thresholded approach. As capabilities cross critical lines, safeguards and reviews must escalate. DeepMind’s framework .... ## Bài viết liên quanXem thêm >>\n##### AI team workflow for GPT-5: roles, versioning, eval CI, governance (2025)"
          },
          {
            "snippetId": "risk-mitigations-snippet-5",
            "url": "https://www.youtube.com/watch?v=u1XytIOZvf0",
            "snippet": "This comprehensive document from OpenAI, published in August 2025, serves as the GPT-5 System Card, detailing the capabilities and safety measures of their new GPT-5 model family, including gpt-5-main and gpt-5-thinking. The card outlines various safety challenges and evaluations, such as managing disallowed content, addressing sycophancy, and improving factual accuracy to reduce hallucinations.... It further elaborates on extensive red teaming and external assessments conducted to test the models' robustness against adversarial attacks like jailbreaks and prompt injections. A significant portion is dedicated to the Preparedness Framework, explaining how OpenAI tracks and mitigates severe risks, particularly focusing on biological and chemical capabilities and cybersecurity risks, through a multi-layered system of safeguards and rigorous testing.... We're looking at some uh profound underlying architectural shifts\n{ts:34} and novel training methodologies too. These are what make the advancements possible. This isn't just, you know,\n{ts:38} another small step. It feels like a significant leap, a leap in how these intelligence systems are designed, how\n{ts:44} they're trained, and that's exactly our mission today.... We want to go beyond the headlines, beyond\n{ts:50} the hype, and really dive deep into the intricate details of GPT5. We'll explore its uh revolutionary architecture, the\n{ts:57} massive training foundation behind it, the expansive multimodal capabilities it brings and the fundamental principles\n{ts:63} driving its performance.... A unified system in GPD5 means it's incredibly smart about how it uses\n{ts:100} its massive capabilities. At its heart is this ingenious real-time router. Uh think of it like a hyperefficient air\n{ts:106} traffic controller but for your AI queries. Okay. A router.... It continuously trains\n{ts:175} and refineses its routing decisions. It learns from real world signals. Things like when users manually switch models\n{ts:183} or the preference rates for different responses, measured correctness. So, it's learning on the fly,\n{ts:188} improving constantly.... And this unified system, it effectively covers several explicit model progressions.\n{ts:224} It's not just one thing. For everyday tasks, you've got GPT5 main. That's the successor to GPT4. Then there's GPT5\n{ts:231} main mini taking over from GPT4 Mini, even more efficient.... {ts:360} Okay, there's been a fundamental shift in the training paradigm, a big one. GPT5\n{ts:365} apparently heavily utilizes reinforcement learning which marks a significant departure from\n{ts:371} the more let's say traditional pre-training focus of GPT3 and GPT4 reinforcement learning is the primary... {ts:451} highquality data sets.\nThey even had to supplement data on the fly during training runs, which given the cost is\n{ts:457} wild, the data appetite is just insatiable. And of course, underpinning all this is a rigorous filtering and\n{ts:463} safety classification process deeply integrated in the data pipeline to manage risks and bias.... {ts:469} Fascinating. So it's not just scaling up, it's fundamentally changing how the AI thinks, how it learns. Okay, let's\n{ts:474} move from architecture and training to capabilities. What are the practical implications? What can GBT5 actually do\n{ts:479} better than previous models?... The advancements are pretty far reaching. Take coding for example.\n{ts:484} OpenAI calls it their strongest coding model to date. It shows particular improvements in uh complex front-end\n{ts:490} generation, debugging, larger repositories, things like that. It can apparently create beautiful and\n{ts:495} responsive websites, apps, and games with a real eye for aesthetic sensibility all from one prompt, which... {ts:850} deeper reasoning GPT5's responses are around 80% less likely to have a factual error than the previous OpenAI 3 model.\n{ts:857} 80%. Wow. Yeah. And in browsing settings where it's looking things up, it shows over\n{ts:862} five times fewer factual errors.... {ts:946} interacting with external systems. Okay, so more reliable, more honest, less annoyingly agreeable, and better at\n{ts:953} complex tasks. This sounds incredibly powerful, but often power comes with a trade-off. Speed, cost. Given the\n{ts:960} immense scale, the complexity we talked about, is GPT5 also faster or at least more efficient?... {ts:1085} Right. So, today we've taken a really deep dive into GPT5. We've revealed not just a new model\n{ts:1092} number, but a truly unified system, one with that sophisticated real-time router at its core. We've explored its massive\n{ts:1099} training infrastructure heavily leaning on reinforcement learning now."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Risk Mitigations",
            "score": 1,
            "explanation": "We worked with internal red teamers to identify new challenges and inform corresponding mitigations."
          }
        ]
      }
    ]
  },
  {
    "model": "Qwen3-Max",
    "sections": [
      {
        "sectionId": "model-details",
        "sectionSnippets": [
          {
            "snippetId": "model-details-snippet-1",
            "url": "https://github.com/QwenLM/Qwen3",
            "snippet": "QwenLM / **\nQwen3 ** Public\n\n# QwenLM/Qwen3\n# Qwen3\n\n💜 **Qwen Chat** | 🤗 Hugging Face | 🤖 ModelScope | 📑 Paper | 📑 Blog ｜ 📖 Documentation 🖥️ Demo | 💬 WeChat (微信) | 🫨 Discord\n\nVisit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with... ```\nQwen3-\n```\nor visit the Qwen3 collection, and you will find all you need! Enjoy!\n\nTo learn more about Qwen3, feel free to read our documentation [EN|ZH]. Our documentation consists of the following sections:... - Quickstart: the basic usages and demonstrations;\n- Inference: the guidance for the inference with Transformers, including batch inference, streaming, etc.;\n- Run Locally: the instructions for running LLM locally on CPU and GPU, with frameworks like llama.cpp and Ollama;\n- Deployment: the demonstration of how to deploy Qwen for large-scale inference with frameworks like SGLang, vLLM, TGI, etc.;... **Qwen3-Instruct-2507** is the updated version of the previous Qwen3 non-thinking mode, featuring the following key enhancements:... ### Qwen3 (aka Qwen3-2504)\n\nWe are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models.\nThese models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5.\nWe are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.\nThe highlights from Qwen3 include:... - 2025.08.06: The final open release of Qwen3-2507, Qwen3-4B-Instruct-2507 and Qwen3-4B-Thinking-2507, is out!\n- 2025.07.31: Qwen3-30B-A3B-Thinking-2507 is released. Check out the modelcard for more details!... - 2025.07.30: Qwen3-30B-A3B-Instruct-2507 is released. Check out the modelcard for more details!\n- 2025.07.25: We released the updated version of Qwen3-235B-A22B thinking mode, named Qwen3-235B-A22B-Thinking-2507. Check out the modelcard for more details!... - 2025.04.29: We released the Qwen3 series. Check our blog for more details!\n- 2024.09.19: We released the Qwen2.5 series. This time there are 3 extra model sizes: 3B, 14B, and 32B for more possibilities. Check our blog for more!... - 2024.06.06: We released the Qwen2 series. Check our blog!\n- 2024.03.28: We released the first MoE model of Qwen: Qwen1.5-MoE-A2.7B! Temporarily, only HF transformers and vLLM support the model. We will soon add the support of llama.cpp, mlx-lm, etc. Check our blog for more information!\n- 2024.02.05: We released the Qwen1.5 series.... ## Performance\n\nDetailed evaluation results are reported in this 📑 blog (Qwen3-2504) and this 📑 blog (Qwen3-2507) [coming soon].\n\nFor requirements on GPU memory and the respective throughput, see results here.... You can also access the Ollama service via its OpenAI-compatible API.\nPlease note that you need to (1) keep\n```\nollama serve\n```\nrunning while using the API, and (2) execute\n```\nollama run qwen3:8b\n```\nbefore utilizing this API to ensure that the model checkpoint is prepared.\nThe API is at... ### LMStudio\n\nQwen3 has already been supported by lmstudio.ai. You can directly use LMStudio with our GGUF files.\n\n### ExecuTorch\n\nTo export and run on ExecuTorch (iOS, Android, Mac, Linux, and more), please follow this example.\n\n### MNN\n\nTo export and run on MNN, which supports Qwen3 on mobile devices, please visit Alibaba MNN.... ### MLX LM\n\nIf you are running on Apple Silicon,\n```\nmlx-lm\n```\nalso supports Qwen3 (\n```\nmlx-lm>=0.24.0\n```\n).\nLook for models ending with MLX on Hugging Face Hub."
          },
          {
            "snippetId": "model-details-snippet-2",
            "url": "https://en.wikipedia.org/wiki/Qwen",
            "snippet": "## Models\n\nAlibaba launched a beta of Qwen in April 2023 under the name Tongyi Qianwen, then opened it for public use in September 2023 after regulatory clearance.... The model's architecture was based on the Llama architecture developed by Meta AI. In December 2023, it released its 72B and 1.8B models for download, while Qwen 7B weights were released in August. Their models are sometimes described as open source, but the training code has not been released nor has the training data been documented, and they do not meet the terms of either the Open Source AI Definition or the Model Openness Framework from the Linux Foundation.... In June 2024 Alibaba launched Qwen2 and in September it released some of its models with open weights, while keeping its most advanced models proprietary. Qwen2 contains both dense and sparse models.\n\nIn November 2024, QwQ-32B-Preview, a model focusing on reasoning similar to OpenAI's o1, was released under the Apache 2.0 License, although only the weights were released, not the dataset or training method. QwQ has a 32K token context length and performs better than o1 on some benchmarks.... The Qwen-VL series is a line of visual language models that combines a vision transformer with a LLM. Alibaba released Qwen2-VL with variants of 2 billion and 7 billion parameters.... In January 2025, Qwen2.5-VL was released with variants of 3, 7, 32, and 72 billion parameters. All models except the 72B variant are licensed under the Apache 2.0 license. Qwen-VL-Max is Alibaba's flagship vision model as of 2024, and is sold by Alibaba Cloud at a cost of US$0.41 per million input tokens.... Alibaba has released several other model types such as Qwen-Audio and Qwen2-Math. In total, it has released more than 100 open weight models, with its models having been downloaded more than 40 million times. Fine-tuned versions of Qwen have been developed by enthusiasts, such as \"Liberated Qwen\", developed by San Francisco-based Abacus AI, which is a version that responds to any user request without content restrictions.... On January 29, 2025, Alibaba launched Qwen2.5-Max. According to a blog post from Alibaba, Qwen2.5-Max outperforms other foundation models such as GPT-4o, DeepSeek-V3, and Llama-3.1-405B in key benchmarks. In February 2025, Alibaba announced on their official X account that the 2.5-Max model would be opened up, however it has not been released.... On March 24, 2025, Alibaba launched Qwen2.5-VL-32B-Instruct as a successor to the Qwen2.5-VL model. It was released under the Apache 2.0 license.... On April 28, 2025, the Qwen3 model family was released, with all models licensed under the Apache 2.0 license. The Qwen3 model family includes both dense (0.6B, 1.7B, 4B, 8B, 14B, and 32B parameters) and sparse models (30B with 3B activated parameters, 235B with 22B activated parameters).... On September 5, 2025, Alibaba launched Qwen3-Max. According to Alibaba's official X account, it outperforms other foundation non-reasoning models such as Qwen3-235B-A22B-Instruct-2507, Kimi K2, Claude 4 Opus Non-thinking, and DeepSeek V3.1. While it was not available from the start, thinking mode was released to the public in the first part of November 2025.... On September 10, 2025, Qwen3-Next was released under the Apache 2.0 license and made available through chat.qwen.ai, as well as platforms like Hugging Face and Model Scope. Qwen3-Next includes two post-trained Instruct and Thinking models. Qwen3-Next was created with a new model-architecture called Qwen3-Next, in the belief that Context Length Scaling and Total Parameter Scaling are two major trends in the future of large models.... ## See also\n- List of large language models\n\n## References\n\n## External links\n- Official website\n- Qwen Chat\n- Qwen on GitHub\n- Qwen on Hugging Face\n\nCategories: - Alibaba Group\n- Large language models\n- Open-source artificial intelligence\n- Software using the Apache license\n- Generative pre-trained transformers\n- 2023 in artificial intelligence\n- 2023 software"
          },
          {
            "snippetId": "model-details-snippet-3",
            "url": "https://qwen.readthedocs.io",
            "snippet": "# Welcome to Qwen!¶\n\nQwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Both language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.... The latest version, Qwen3, has the following features:\n**Dense and Mixture-of-Experts (MoE) models**, available in 0.6B, 1.7B, 4B, 8B, 14B, 32B and 30B-A3B, 235B-A22B. **Seamless switching between thinking mode**(for complex logical reasoning, math, and coding) and **non-thinking mode**(for efficient, general-purpose chat) **within a single model**, ensuring optimal performance across various scenarios.... **Significantly enhancement in reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.... **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks. **Support of 100+ languages and dialects**with strong capabilities for **multilingual instruction following**and **translation**.\n\nFor more information, please visit our:\nJoin our community by joining our and group. We are looking forward to seeing you there!"
          },
          {
            "snippetId": "model-details-snippet-4",
            "url": "https://www.alibabacloud.com/en/product/modelstudio?_p_lc=1",
            "snippet": "### Latest Updates\n\nThe Qwen3 series has launched a variety of powerful new models, as follows:\n\n● qwen3-max, qwen3-max-2025-09-23: The Qwen Max model has been updated with the \"no-thinking\" mode. Compared to the previous version (qwen3-max-preview), it shows significant improvements in agent programming capabilities, agent tool usage, logical reasoning, code-based problem solving, and text reasoning.... ● qwen3-vl-plus, qwen3-vl-plus-2025-09-23: Vision-language understanding models in the Qwen3 series, comprehensively upgraded in areas such as visual coding, spatial perception, and multimodal reasoning. Supports ultra-long video understanding.\n\n● qwen-image-plus: An image generation model in the Tongyi Qianwen series, excelling in complex text rendering and various image generation and editing tasks.... ● qwen3-coder-plus-2025-09-23: A code generation model based on Qwen3. Compared to the previous version (July 22 snapshot), it demonstrates improved robustness in downstream tasks and tool calling, along with enhanced code security.... ● qwen3-livetranslate-flash-realtime, qwen3-livetranslate-flash-realtime-2025-09-22: Mainline and snapshot versions of the Qwen3-Omni series for real-time translation. This is the real-time version of Tongyi Qianwen 3-LiveTranslate-Flash, a high-precision, responsive, and robust large model for real-time multilingual audio-video interpretation.... ## Qwen3: Major Models Unveiled!\n\nAll Qwen3 models seamlessly integrate \"thinking\" and \"non-thinking\" modes, allowing you to switch modes during conversation. Try It Now in Model Studio!\n\n### Innovative Dual Thinking Modes\n\nBalancing inference depth and speed flexibly while managing costs effectively with Qwen3's unique \"Thinking\" (deep reasoning) and \"Non-thinking\" (fast response) modes... ## Why Alibaba Cloud Model Studio?\n\nAlibaba Cloud Model Studio is specially designed for developing generative AI. Through this platform, you can easily experience and quickly access Alibaba Cloud's latest industry-leading foundation models (FM), including Qwen-Max, Qwen-Plus, Qwen-Turbo, Qwen-VL, and the Wan series, to develop your own generative AI (GenAI) applications. You don't need to worry about underlying infrastructure and computing power. All operations are performed in isolated VPC networks to reduce security and privacy risks.... ## Features\n\n\n\n##### Model Playground\n\nA variety of leading foundational models (FM) are available, including Alibaba Cloud's Qwen, Wan, and multimodal series models.\n\n\n\n##### High Cost-Effectiveness and Performance\n\nModel Studio provides official APIs for the Qwen series of large models, helping enterprises focus on business innovation and reducing technical maintenance burdens.... ##### Multiple Access Methods\n\nYou can invocate these models by calling APIs, which are compatible with OpenAI, and come with DashScope SDK for flexible and convenient access.\n\n\n\n##### Secure and Efficient Data Transmission\n\nModel Studio enhances data security and transmission efficiency via private network access, using PrivateLink to establish dedicated VPC connections.\n\n**Activate Alibaba Cloud Model Studio in the console to enjoy 1 million free tokens and the full experience**... ## Scenarios\n\n### A Rich Selection of Models to Suit Diverse User Needs\n\n#### Qwen\n\nQwen-Max is an ultra-large-scale language model with hundreds of billions of parameters, supporting multi-lingual inputs including Chinese and English; Qwen-Turbo is a fast and cost-effective model with a maximum context length of up to 1M, it also supports multiple languages.... #### Wan\n\nWan supports text-to-video and image-to-video generation. Wan2.1 provides higher-quality visual generation, supporting complex movements, real-world physics simulation, rich artistic styles, and cinematic visual quality, with improved instruction-following capability.\n\n#### Multimodal Model Qwen-Omni\n\nQwen-Omni understands inputs in text, image, voice, and video, as well as mixed inputs. It also supports the simultaneous streaming generation of both text and voice, with accelerated multimodal content understanding, and four natural conversation voices.... ### Easy Agent Development with Assistant API\n\n#### Designed for Agent Development\n\n- Support for Qwen model series and Supervised Fine-Tuning (SFT)\n\n- Ready-to-use plugins and customizable functions\n\n- Developer-oriented SDK development support\n\n- Fast intervention and basic memory capabilities\n\n#### Prompt Engineering\n\n- 160+ prompt templates for various work and life scenarios\n\n- Configurable variables in templates for customization\n\n- Optimization based on meta-prompting\n\n- Example selecting tool for few-shot prompting... #### Functionality Enhancement\n\n- An evolving architecture with new algorithms and technologies\n\n- Atomic function optimized for prompts, responses, and task execution\n\n- The SDK development paradigm consistent with OpenAPI... **Upgraded Support For You**\n\n1 on 1 Presale Consultation, 24/7 Technical Support, Faster Response, and More Free Tickets."
          },
          {
            "snippetId": "model-details-snippet-5",
            "url": "https://github.com/QwenLM/Qwen/",
            "snippet": "`$DATA`.\n\nThe finetuning scripts allow you to perform:\n\n- Full-parameter finetuning\n\n- LoRA\n\n- Q-LoRA\n\nFull-parameter finetuning requires updating all parameters in the whole training process. To launch your training, run the following script:\n\n```\n\n# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.\n\nbash finetune/finetune_ds.sh\n\n```... Remember to specify the correct model name or path, the data path, as well as the output directory in the shell scripts. Another thing to notice is that we use DeepSpeed ZeRO 3 in this script. If you want to make changes, just remove the argument\n\n`--deepspeed` or make changes in the DeepSpeed configuration json file based on your requirements. Additionally, this script supports mixed-precision training, and thus you can use... `--bf16 True` or\n\n`--fp16 True`. Remember to use DeepSpeed when you use fp16 due to mixed precision training. Empirically we advise you to use bf16 to make your training consistent with our pretraining and alignment if your machine supports bf16, and thus we use it by default.... Similarly, to run LoRA, use another script to run as shown below. Before you start, make sure that you have installed\n\n`peft`. Also, you need to specify your paths to your model, data, and output. We advise you to use absolute path for your pretrained model. This is because LoRA only saves the adapter and the absolute path in the adapter configuration json file is used for finding out the pretrained model to load. Also, this script support both bf16 and fp16.... # Single GPU training bash finetune/finetune_lora_single_gpu.sh # Distributed training bash finetune/finetune_lora_ds.sh\n\nIn comparison with full-parameter finetuning, LoRA (paper) only updates the parameters of adapter layers but keeps the original large language model layers frozen. This allows much fewer memory costs and thus fewer computation costs.... Note that if you use LoRA to finetune the base language model, e.g., Qwen-7B, instead of chat models, e.g., Qwen-7B-Chat, the script automatically switches the embedding and output layer as trainable parameters. This is because the base language model has no knowledge of special tokens brought by ChatML format. Thus these layers should be updated for the model to understand and predict the tokens. Or in another word, if your training brings in special tokens in LoRA, you should set the layers to trainable parameters by setting... `modules_to_save` inside the code. Also, if we have these parameters trainable, it is not available to use ZeRO 3, and this is why we use ZeRO 2 in the script by default. If you do not have new trainable parameters, you can switch to ZeRO 3 by changing the DeepSpeed configuration file. Additionally, we find that there is a significant gap between the memory footprint of LoRA with and without these trainable parameters. Therefore, if you have trouble with memory, we advise you to LoRA finetune the chat models. Check the profile below for more information.... If you still suffer from insufficient memory, you can consider Q-LoRA (paper), which uses the quantized large language model and other techniques such as paged attention to allow even fewer memory costs.\n\nNote: to run single-GPU Q-LoRA training, you may need to install\n\n`mpi4py` through\n\n`pip` or\n\n`conda`.\n\nTo run Q-LoRA, directly run the following script:... # Single GPU training bash finetune/finetune_qlora_single_gpu.sh # Distributed training bash finetune/finetune_qlora_ds.sh\n\nFor Q-LoRA, we advise you to load our provided quantized model, e.g., Qwen-7B-Chat-Int4. You\n\n**SHOULD NOT** use the bf16 models. Different from full-parameter finetuning and LoRA, only fp16 is supported for Q-LoRA. For single-GPU training, we have to use DeepSpeed for mixed-precision tr"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Model overview",
            "score": 1,
            "explanation": "Qwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group."
          },
          {
            "name": "Organization developing the model",
            "score": 1,
            "explanation": "Qwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group."
          },
          {
            "name": "Model Version",
            "score": 1,
            "explanation": "The latest version, Qwen3, has the following features: Dense and Mixture-of-Experts (MoE) models."
          },
          {
            "name": "Model Release Date",
            "score": 1,
            "explanation": "On April 28, 2025, the Qwen3 model family was released."
          },
          {
            "name": "Model Version Progression",
            "score": 1,
            "explanation": "We released the updated version of Qwen3-235B-A22B-Thinking-2507."
          },
          {
            "name": "Model Architecture",
            "score": 1,
            "explanation": "The model's architecture was based on the Llama architecture developed by Meta AI."
          },
          {
            "name": "Model Dependencies",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Paper and relevant links",
            "score": 1,
            "explanation": "For more information, please visit our: Join our community by joining our and group."
          },
          {
            "name": "Model Distribution Forms",
            "score": 1,
            "explanation": "We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models."
          }
        ]
      },
      {
        "sectionId": "model-inputs-outputs",
        "sectionSnippets": [
          {
            "snippetId": "model-inputs-outputs-snippet-1",
            "url": "https://github.com/QwenLM/Qwen3",
            "snippet": "```\nQwen3-\n```\nor visit the Qwen3 collection, and you will find all you need! Enjoy!\n\nTo learn more about Qwen3, feel free to read our documentation [EN|ZH]. Our documentation consists of the following sections:... - Quickstart: the basic usages and demonstrations;\n- Inference: the guidance for the inference with Transformers, including batch inference, streaming, etc.;\n- Run Locally: the instructions for running LLM locally on CPU and GPU, with frameworks like llama.cpp and Ollama;\n- Deployment: the demonstration of how to deploy Qwen for large-scale inference with frameworks like SGLang, vLLM, TGI, etc.;... ## News\n- 2025.08.08: You can now use Qwen3-2507 to handle ultra-long inputs of **1 million tokens**! See the update modelcards (235B-A22B-Instruct-2507, 235B-A22B-Thinking-2507, A30B-A3B-Instruct-2507, A30B-A3B-Thinking-2507) for how to enable this feature.... - 2025.08.06: The final open release of Qwen3-2507, Qwen3-4B-Instruct-2507 and Qwen3-4B-Thinking-2507, is out!\n- 2025.07.31: Qwen3-30B-A3B-Thinking-2507 is released. Check out the modelcard for more details!... - 2025.07.30: Qwen3-30B-A3B-Instruct-2507 is released. Check out the modelcard for more details!\n- 2025.07.25: We released the updated version of Qwen3-235B-A22B thinking mode, named Qwen3-235B-A22B-Thinking-2507. Check out the modelcard for more details!... - 2025.04.29: We released the Qwen3 series. Check our blog for more details!\n- 2024.09.19: We released the Qwen2.5 series. This time there are 3 extra model sizes: 3B, 14B, and 32B for more possibilities. Check our blog for more!... - 2024.06.06: We released the Qwen2 series. Check our blog!\n- 2024.03.28: We released the first MoE model of Qwen: Qwen1.5-MoE-A2.7B! Temporarily, only HF transformers and vLLM support the model. We will soon add the support of llama.cpp, mlx-lm, etc. Check our blog for more information!\n- 2024.02.05: We released the Qwen1.5 series.... ## Performance\n\nDetailed evaluation results are reported in this 📑 blog (Qwen3-2504) and this 📑 blog (Qwen3-2507) [coming soon].\n\nFor requirements on GPU memory and the respective throughput, see results here....     model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,...     add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=16384\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\n\nprint(\"content:\", content)\n```...     model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,...     add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()... ### LMStudio\n\nQwen3 has already been supported by lmstudio.ai. You can directly use LMStudio with our GGUF files.\n\n### ExecuTorch\n\nTo export and run on ExecuTorch (iOS, Android, Mac, Linux, and more), please follow this example.\n\n### MNN\n\nTo export and run on MNN, which supports Qwen3 on mobile devices, please visit Alibaba MNN."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-2",
            "url": "https://github.com/QwenLM/Qwen/",
            "snippet": "`$DATA`.\n\nThe finetuning scripts allow you to perform:\n\n- Full-parameter finetuning\n\n- LoRA\n\n- Q-LoRA\n\nFull-parameter finetuning requires updating all parameters in the whole training process. To launch your training, run the following script:\n\n```\n\n# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.\n\nbash finetune/finetune_ds.sh\n\n```... Remember to specify the correct model name or path, the data path, as well as the output directory in the shell scripts. Another thing to notice is that we use DeepSpeed ZeRO 3 in this script. If you want to make changes, just remove the argument\n\n`--deepspeed` or make changes in the DeepSpeed configuration json file based on your requirements. Additionally, this script supports mixed-precision training, and thus you can use... `--bf16 True` or\n\n`--fp16 True`. Remember to use DeepSpeed when you use fp16 due to mixed precision training. Empirically we advise you to use bf16 to make your training consistent with our pretraining and alignment if your machine supports bf16, and thus we use it by default.... Similarly, to run LoRA, use another script to run as shown below. Before you start, make sure that you have installed\n\n`peft`. Also, you need to specify your paths to your model, data, and output. We advise you to use absolute path for your pretrained model. This is because LoRA only saves the adapter and the absolute path in the adapter configuration json file is used for finding out the pretrained model to load. Also, this script support both bf16 and fp16.... # Single GPU training bash finetune/finetune_lora_single_gpu.sh # Distributed training bash finetune/finetune_lora_ds.sh\n\nIn comparison with full-parameter finetuning, LoRA (paper) only updates the parameters of adapter layers but keeps the original large language model layers frozen. This allows much fewer memory costs and thus fewer computation costs.... Note that if you use LoRA to finetune the base language model, e.g., Qwen-7B, instead of chat models, e.g., Qwen-7B-Chat, the script automatically switches the embedding and output layer as trainable parameters. This is because the base language model has no knowledge of special tokens brought by ChatML format. Thus these layers should be updated for the model to understand and predict the tokens. Or in another word, if your training brings in special tokens in LoRA, you should set the layers to trainable parameters by setting... `modules_to_save` inside the code. Also, if we have these parameters trainable, it is not available to use ZeRO 3, and this is why we use ZeRO 2 in the script by default. If you do not have new trainable parameters, you can switch to ZeRO 3 by changing the DeepSpeed configuration file. Additionally, we find that there is a significant gap between the memory footprint of LoRA with and without these trainable parameters. Therefore, if you have trouble with memory, we advise you to LoRA finetune the chat models. Check the profile below for more information.... If you still suffer from insufficient memory, you can consider Q-LoRA (paper), which uses the quantized large language model and other techniques such as paged attention to allow even fewer memory costs.\n\nNote: to run single-GPU Q-LoRA training, you may need to install\n\n`mpi4py` through\n\n`pip` or\n\n`conda`.\n\nTo run Q-LoRA, directly run the following script:... # Single GPU training bash finetune/finetune_qlora_single_gpu.sh # Distributed training bash finetune/finetune_qlora_ds.sh\n\nFor Q-LoRA, we advise you to load our provided quantized model, e.g., Qwen-7B-Chat-Int4. You\n\n**SHOULD NOT** use the bf16 models. Different from full-parameter finetuning and LoRA, only fp16 is supported for Q-LoRA. For single-GPU training, we have to use DeepSpeed for mixed-precision tr"
          },
          {
            "snippetId": "model-inputs-outputs-snippet-3",
            "url": "https://www.alibabacloud.com/en/product/modelstudio?_p_lc=1",
            "snippet": "### Latest Updates\n\nThe Qwen3 series has launched a variety of powerful new models, as follows:\n\n● qwen3-max, qwen3-max-2025-09-23: The Qwen Max model has been updated with the \"no-thinking\" mode. Compared to the previous version (qwen3-max-preview), it shows significant improvements in agent programming capabilities, agent tool usage, logical reasoning, code-based problem solving, and text reasoning.... ● qwen3-vl-plus, qwen3-vl-plus-2025-09-23: Vision-language understanding models in the Qwen3 series, comprehensively upgraded in areas such as visual coding, spatial perception, and multimodal reasoning. Supports ultra-long video understanding.\n\n● qwen-image-plus: An image generation model in the Tongyi Qianwen series, excelling in complex text rendering and various image generation and editing tasks.... ● qwen3-coder-plus-2025-09-23: A code generation model based on Qwen3. Compared to the previous version (July 22 snapshot), it demonstrates improved robustness in downstream tasks and tool calling, along with enhanced code security.... ● qwen3-livetranslate-flash-realtime, qwen3-livetranslate-flash-realtime-2025-09-22: Mainline and snapshot versions of the Qwen3-Omni series for real-time translation. This is the real-time version of Tongyi Qianwen 3-LiveTranslate-Flash, a high-precision, responsive, and robust large model for real-time multilingual audio-video interpretation.... ## Qwen3: Major Models Unveiled!\n\nAll Qwen3 models seamlessly integrate \"thinking\" and \"non-thinking\" modes, allowing you to switch modes during conversation. Try It Now in Model Studio!\n\n### Innovative Dual Thinking Modes\n\nBalancing inference depth and speed flexibly while managing costs effectively with Qwen3's unique \"Thinking\" (deep reasoning) and \"Non-thinking\" (fast response) modes... ## Why Alibaba Cloud Model Studio?\n\nAlibaba Cloud Model Studio is specially designed for developing generative AI. Through this platform, you can easily experience and quickly access Alibaba Cloud's latest industry-leading foundation models (FM), including Qwen-Max, Qwen-Plus, Qwen-Turbo, Qwen-VL, and the Wan series, to develop your own generative AI (GenAI) applications. You don't need to worry about underlying infrastructure and computing power. All operations are performed in isolated VPC networks to reduce security and privacy risks.... ## Features\n\n\n\n##### Model Playground\n\nA variety of leading foundational models (FM) are available, including Alibaba Cloud's Qwen, Wan, and multimodal series models.\n\n\n\n##### High Cost-Effectiveness and Performance\n\nModel Studio provides official APIs for the Qwen series of large models, helping enterprises focus on business innovation and reducing technical maintenance burdens.... ##### Multiple Access Methods\n\nYou can invocate these models by calling APIs, which are compatible with OpenAI, and come with DashScope SDK for flexible and convenient access.\n\n\n\n##### Secure and Efficient Data Transmission\n\nModel Studio enhances data security and transmission efficiency via private network access, using PrivateLink to establish dedicated VPC connections.\n\n**Activate Alibaba Cloud Model Studio in the console to enjoy 1 million free tokens and the full experience**... ## Scenarios\n\n### A Rich Selection of Models to Suit Diverse User Needs\n\n#### Qwen\n\nQwen-Max is an ultra-large-scale language model with hundreds of billions of parameters, supporting multi-lingual inputs including Chinese and English; Qwen-Turbo is a fast and cost-effective model with a maximum context length of up to 1M, it also supports multiple languages.... #### Wan\n\nWan supports text-to-video and image-to-video generation. Wan2.1 provides higher-quality visual generation, supporting complex movements, real-world physics simulation, rich artistic styles, and cinematic visual quality, with improved instruction-following capability.\n\n#### Multimodal Model Qwen-Omni\n\nQwen-Omni understands inputs in text, image, voice, and video, as well as mixed inputs. It also supports the simultaneous streaming generation of both text and voice, with accelerated multimodal content understanding, and four natural conversation voices.... #### Enhanced Reasoning Model QwQ\n\nQwQ is trained based on Qwen2.5 through reinforcement learning, with improved reasoning capability. The core indicators of the model such as mathematics code (AIME 24/25, LiveCodeBench) and some general indicators (IFEval, LiveBench, etc.) have reached the level of DeepSeek-R1 full edition.\n\n#### Visual Reasoning Model QVQ\n\nQVQ supports visual input and Chain-of-Thought output, demonstrating stronger capabilities in mathematics, programming, visual analysis, content creation, and general knowledge tasks.... #### Functionality Enhancement\n\n- An evolving architecture with new algorithms and technologies\n\n- Atomic function optimized for prompts, responses, and task execution\n\n- The SDK development paradigm consistent with OpenAPI"
          },
          {
            "snippetId": "model-inputs-outputs-snippet-4",
            "url": "https://www.alibabacloud.com/help/en/model-studio/what-is-qwen-llm",
            "snippet": "|Always has the same capabilities as the latest snapshot version|Latest|$1.6|$6.4|\n|qwen-max-2025-01-25 also known as qwen-max-0125 or Qwen2.5-Max|Snapshot|\n**Chinese mainland (Beijing)**... |qwen3-max-2025-09-23|Snapshot|\n|qwen3-max-preview|Preview|\nBilling for the models listed above is tiered based on the number of input tokens per request.... ||qwen3-max and qwen3-max-preview support context cache.||\n|--|--|--|\n|0 < Tokens ≤ 32K|$0.861|$3.441|\n|32K < Tokens ≤ 128K|$1.434|$5.735|\n|128K < Tokens ≤ 252K|$2.151|$8.602|... |qwen-max-2025-01-25 also known as qwen-max-0125 or Qwen2.5-Max|Snapshot|\n|qwen-max-2024-09-19 also known as qwen-max-0919|32,768|30,720|$2.868|$8.602|... |qwen-max-2024-04-28 also known as qwen-max-0428|Snapshot|8,000|6,000|2,000|CNY 0.04|CNY 0.12|\n|qwen-max-2024-04-03 also known as qwen-max-0403|... |||||\n|--|--|--|--|\n|0 < Tokens ≤ 256K|$0.4|Non-thinking mode|$1.2|\n|Thinking mode|$4|\n|256K < Tokens ≤ 1M|$1.2|Non-thinking mode|$3.6|... |Always equivalent to the latest snapshot version|Latest|\n|qwen-omni-turbo-2025-03-26 Also known as qwen-omni-turbo-0326|Snapshot|\nAfter the free quota for the commercial model is used up, the following billing rules apply to input and output:... |||\n|--|--|\n|Billing example: The cost for a request with 1,000 text input tokens, 1,000 image input tokens, 1,000 text output tokens, and 1,000 audio output tokens is: $0.000058 (text input) + $0.000216 (image input) + $0.007168 (audio output).|... Audio output is not supported in thinking mode. In non-thinking mode, the audio output from the model has the following features:\n\nIt supports 17 voices, an increase from the 4 supported by Qwen-Omni-Turbo.\n\nIt supports 10 languages, an increase from the 2 supported by Qwen-Omni-Turbo.... **Qwen-Omni-Realtime**\n\nCompared to Qwen Omni, these models support audio stream input. They have a built-in Voice Activity Detection (VAD) feature that automatically detects the start and end of user speech. Usage | Client events | Server events... |||||||\n|--|--|--|--|--|--|\n|||\n|Offers the same capabilities as qwen-vl-ocr-2025-04-13.|Stable|34,096|30,000 Maximum of 30,000 for a single image.|4,096|$0.717|... ##### International (Singapore)\n\n|||||||\n|--|--|--|--|--|--|\n|qwen3-asr-flash Currently an alias for qwen3-asr-flash-2025-09-08|Stable version|Chinese (including Mandarin, Sichuanese, Minnan, Wu, and Cantonese), English, Japanese, German, Korean, Russian, French, Portuguese, Arabic, Italian, and Spanish|16 kHz|$0.000035/second|36,000 seconds (10 hours) Valid for 90 days after you activate Alibaba Cloud Model Studio|... ||||||\n|--|--|--|--|--|\n|qwen3-asr-flash Alias for qwen3-asr-flash-2025-09-08|Stable version|Chinese (Mandarin, Sichuanese, Minnan, Wu, and Cantonese), English, Japanese, German, Korean, Russian, French, Portuguese, Arabic, Italian, and Spanish|16 kHz|$0.000032/second|... |qwen3-asr-flash-2025-09-08|Snapshot version|\n\n#### Qwen-Math\n\nQwen-Math is a language model designed for mathematical problem-solving. Usage | API reference | Try it online\n\n**Note**\n\nThis model is available only in the China (Beijing) region."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-5",
            "url": "https://qwenlm.github.io/blog/qwen3/",
            "snippet": "QWEN CHAT GitHub Hugging Face ModelScope Kaggle DEMO DISCORD\n\n## Introduction\n\nToday, we are excited to announce the release of... |Models|Layers|Heads (Q / KV)|# Experts (Total / Activated)|Context Length|\n|--|--|--|--|--|\n|Qwen3-30B-A3B|48|32 / 4|128 / 8|128K|\n|Qwen3-235B-A22B|94|64 / 4|128 / 8|128K|... The post-trained models, such as\n\n**Qwen3-30B-A3B**, along with their pre-trained counterparts (e.g., **Qwen3-30B-A3B-Base**), are now available on platforms like **Hugging Face**, **ModelScope**, and **Kaggle**. For deployment, we recommend using frameworks like **SGLang** and **vLLM**.... For local usage, tools such as **Ollama**, **LMStudio**, **MLX**, **llama.cpp**, and **KTransformers** are highly recommended. These options ensure that users can easily integrate Qwen3 into their workflows, whether in research, development, or production environments.\n\nWe believe that the release and open-sourcing of Qwen3 will significantly advance the research and development of large foundation models. Our goal is to empower researchers, developers, and organizations around the world to build innovative solutions using these cutting-edge models.... Feel free to try Qwen3 out in Qwen Chat Web (chat.qwen.ai) and mobile APP!... ## Develop with Qwen3\n\nBelow is a simple guide for you to use Qwen3 on different frameworks. First of all, we provide an standard example of using Qwen3-30B-A3B in Hugging Face transformers:... ```\n\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-30B-A3B\"\n\n# load the tokenizer and the model\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\nmodel_name,... torch_dtype=\"auto\",\n\ndevice_map=\"auto\"\n\n\n\n# prepare the model input\n\nprompt = \"Give me a short introduction to large language model.\"\n\nmessages = [\n\n{\"role\": \"user\", \"content\": prompt}\n\n\n\ntext = tokenizer.apply_chat_template(\n\nmessages,\n\ntokenize=False,\n\nadd_generation_prompt=True,\n\nenable_thinking=True # Switch between thinking and non-thinking modes.... Default is True.\n\n\n\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\n\ngenerated_ids = model.generate(\n\n**model_inputs,\n\nmax_new_tokens=32768\n\n\n\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n\n# parsing thinking content... Here is an example of a multi-turn conversation:\n\n```\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n\ndef __init__(self, model_name=\"Qwen/Qwen3-30B-A3B\"):\n\nself.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\nself.model = AutoModelForCausalLM.from_pretrained(model_name)... self.history = []\n\ndef generate_response(self, user_input):\n\nmessages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\ntext = self.tokenizer.apply_chat_template(\n\nmessages,\n\ntokenize=False,\n\nadd_generation_prompt=True\n\n\n\ninputs = self.tokenizer(text, return_tensors=\"pt\")\n\nresponse_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()... ### Agentic Usages\n\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.... ```\n\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\n\nllm_cfg = {\n\n'model': 'Qwen3-30B-A3B',\n\n# Use the endpoint provided by Alibaba Model Studio:\n\n# 'model_type': 'qwen_dashscope',\n\n# 'api_key': os.getenv('DASHSCOPE_API_KEY'),... },\n\n\"fetch\": {\n\n\"command\": \"uvx\",\n\n\"args\": [\"mcp-server-fetch\"]\n\n\n\n},\n\n'code_interpreter', # Built-in tools\n\n\n\n# Define Agent\n\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\n\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\n\nfor responses in bot.run(messages=messages):\n\npass\n\nprint(responses)\n\n```"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Inputs",
            "score": 1,
            "explanation": "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
          },
          {
            "name": "Outputs",
            "score": 1,
            "explanation": "content = tokenizer.decode(output_ids, skip_special_tokens=True)"
          },
          {
            "name": "Token Count",
            "score": 1,
            "explanation": "Qwen-Turbo is a fast and cost-effective model with a maximum context length of up to 1M"
          }
        ]
      },
      {
        "sectionId": "model-data",
        "sectionSnippets": [
          {
            "snippetId": "model-data-snippet-1",
            "url": "https://github.com/QwenLM/Qwen3",
            "snippet": "QwenLM / **\nQwen3 ** Public\n\n# QwenLM/Qwen3\n# Qwen3\n\n💜 **Qwen Chat** | 🤗 Hugging Face | 🤖 ModelScope | 📑 Paper | 📑 Blog ｜ 📖 Documentation 🖥️ Demo | 💬 WeChat (微信) | 🫨 Discord\n\nVisit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with... ```\nQwen3-\n```\nor visit the Qwen3 collection, and you will find all you need! Enjoy!\n\nTo learn more about Qwen3, feel free to read our documentation [EN|ZH]. Our documentation consists of the following sections:... - Quickstart: the basic usages and demonstrations;\n- Inference: the guidance for the inference with Transformers, including batch inference, streaming, etc.;\n- Run Locally: the instructions for running LLM locally on CPU and GPU, with frameworks like llama.cpp and Ollama;\n- Deployment: the demonstration of how to deploy Qwen for large-scale inference with frameworks like SGLang, vLLM, TGI, etc.;... - Quantization: the practice of quantizing LLMs with GPTQ, AWQ, as well as the guidance for how to make high-quality quantized GGUF files;\n- Training: the instructions for post-training, including SFT and RLHF (TODO) with frameworks like Axolotl, LLaMA-Factory, etc.\n- Framework: the usage of Qwen with frameworks for application, e.g., RAG, Agent, etc.... **Qwen3-Instruct-2507** is the updated version of the previous Qwen3 non-thinking mode, featuring the following key enhancements:... ### Qwen3 (aka Qwen3-2504)\n\nWe are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models.\nThese models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5.\nWe are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.\nThe highlights from Qwen3 include:... - 2025.08.06: The final open release of Qwen3-2507, Qwen3-4B-Instruct-2507 and Qwen3-4B-Thinking-2507, is out!\n- 2025.07.31: Qwen3-30B-A3B-Thinking-2507 is released. Check out the modelcard for more details!... - 2025.07.30: Qwen3-30B-A3B-Instruct-2507 is released. Check out the modelcard for more details!\n- 2025.07.25: We released the updated version of Qwen3-235B-A22B thinking mode, named Qwen3-235B-A22B-Thinking-2507. Check out the modelcard for more details!... - 2025.04.29: We released the Qwen3 series. Check our blog for more details!\n- 2024.09.19: We released the Qwen2.5 series. This time there are 3 extra model sizes: 3B, 14B, and 32B for more possibilities. Check our blog for more!... ## Performance\n\nDetailed evaluation results are reported in this 📑 blog (Qwen3-2504) and this 📑 blog (Qwen3-2507) [coming soon].\n\nFor requirements on GPU memory and the respective throughput, see results here.... #### Qwen3-Instruct-2507\n\nThe following contains a code snippet illustrating how to use Qwen3-30B-A3B-Instruct-2507 to generate content based on given inputs.... #### Qwen3-Thinking-2507\n\nThe following contains a code snippet illustrating how to use Qwen3-30B-A3B-Thinking-2507 to generate content based on given inputs.... ### ModelScope\n\nWe strongly advise users especially those in mainland China to use ModelScope.\nModelScope adopts a Python API similar to Transformers.\nThe CLI tool\n```\nmodelscope download\n```\ncan help you solve issues concerning downloading checkpoints.\nFor vLLM and SGLang, the environment variable\n```\nVLLM_USE_MODELSCOPE=true\n```\nand\n```\nSGLANG_USE_MODELSCOPE=true\n```\ncan be used respectively.... ### LMStudio\n\nQwen3 has already been supported by lmstudio.ai. You can directly use LMStudio with our GGUF files.\n\n### ExecuTorch\n\nTo export and run on ExecuTorch (iOS, Android, Mac, Linux, and more), please follow this example.\n\n### MNN\n\nTo export and run on MNN, which supports Qwen3 on mobile devices, please visit Alibaba MNN."
          },
          {
            "snippetId": "model-data-snippet-2",
            "url": "https://github.com/QwenLM/Qwen/",
            "snippet": "`$DATA`.\n\nThe finetuning scripts allow you to perform:\n\n- Full-parameter finetuning\n\n- LoRA\n\n- Q-LoRA\n\nFull-parameter finetuning requires updating all parameters in the whole training process. To launch your training, run the following script:\n\n```\n\n# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.\n\nbash finetune/finetune_ds.sh\n\n```... Remember to specify the correct model name or path, the data path, as well as the output directory in the shell scripts. Another thing to notice is that we use DeepSpeed ZeRO 3 in this script. If you want to make changes, just remove the argument\n\n`--deepspeed` or make changes in the DeepSpeed configuration json file based on your requirements. Additionally, this script supports mixed-precision training, and thus you can use... `--bf16 True` or\n\n`--fp16 True`. Remember to use DeepSpeed when you use fp16 due to mixed precision training. Empirically we advise you to use bf16 to make your training consistent with our pretraining and alignment if your machine supports bf16, and thus we use it by default.... Similarly, to run LoRA, use another script to run as shown below. Before you start, make sure that you have installed\n\n`peft`. Also, you need to specify your paths to your model, data, and output. We advise you to use absolute path for your pretrained model. This is because LoRA only saves the adapter and the absolute path in the adapter configuration json file is used for finding out the pretrained model to load. Also, this script support both bf16 and fp16.... # Single GPU training bash finetune/finetune_lora_single_gpu.sh # Distributed training bash finetune/finetune_lora_ds.sh\n\nIn comparison with full-parameter finetuning, LoRA (paper) only updates the parameters of adapter layers but keeps the original large language model layers frozen. This allows much fewer memory costs and thus fewer computation costs.... Note that if you use LoRA to finetune the base language model, e.g., Qwen-7B, instead of chat models, e.g., Qwen-7B-Chat, the script automatically switches the embedding and output layer as trainable parameters. This is because the base language model has no knowledge of special tokens brought by ChatML format. Thus these layers should be updated for the model to understand and predict the tokens. Or in another word, if your training brings in special tokens in LoRA, you should set the layers to trainable parameters by setting... `modules_to_save` inside the code. Also, if we have these parameters trainable, it is not available to use ZeRO 3, and this is why we use ZeRO 2 in the script by default. If you do not have new trainable parameters, you can switch to ZeRO 3 by changing the DeepSpeed configuration file. Additionally, we find that there is a significant gap between the memory footprint of LoRA with and without these trainable parameters. Therefore, if you have trouble with memory, we advise you to LoRA finetune the chat models. Check the profile below for more information.... If you still suffer from insufficient memory, you can consider Q-LoRA (paper), which uses the quantized large language model and other techniques such as paged attention to allow even fewer memory costs.\n\nNote: to run single-GPU Q-LoRA training, you may need to install\n\n`mpi4py` through\n\n`pip` or\n\n`conda`.\n\nTo run Q-LoRA, directly run the following script:... # Single GPU training bash finetune/finetune_qlora_single_gpu.sh # Distributed training bash finetune/finetune_qlora_ds.sh\n\nFor Q-LoRA, we advise you to load our provided quantized model, e.g., Qwen-7B-Chat-Int4. You\n\n**SHOULD NOT** use the bf16 models. Different from full-parameter finetuning and LoRA, only fp16 is supported for Q-LoRA. For single-GPU training, we have to use DeepSpeed for mixed-precision tr"
          },
          {
            "snippetId": "model-data-snippet-3",
            "url": "https://www.alibabacloud.com/en/product/modelstudio?_p_lc=1",
            "snippet": "### Latest Updates\n\nThe Qwen3 series has launched a variety of powerful new models, as follows:\n\n● qwen3-max, qwen3-max-2025-09-23: The Qwen Max model has been updated with the \"no-thinking\" mode. Compared to the previous version (qwen3-max-preview), it shows significant improvements in agent programming capabilities, agent tool usage, logical reasoning, code-based problem solving, and text reasoning.... ● qwen3-vl-plus, qwen3-vl-plus-2025-09-23: Vision-language understanding models in the Qwen3 series, comprehensively upgraded in areas such as visual coding, spatial perception, and multimodal reasoning. Supports ultra-long video understanding.\n\n● qwen-image-plus: An image generation model in the Tongyi Qianwen series, excelling in complex text rendering and various image generation and editing tasks.... ● qwen3-coder-plus-2025-09-23: A code generation model based on Qwen3. Compared to the previous version (July 22 snapshot), it demonstrates improved robustness in downstream tasks and tool calling, along with enhanced code security.... ## Qwen3: Major Models Unveiled!\n\nAll Qwen3 models seamlessly integrate \"thinking\" and \"non-thinking\" modes, allowing you to switch modes during conversation. Try It Now in Model Studio!\n\n### Innovative Dual Thinking Modes\n\nBalancing inference depth and speed flexibly while managing costs effectively with Qwen3's unique \"Thinking\" (deep reasoning) and \"Non-thinking\" (fast response) modes... ## Why Alibaba Cloud Model Studio?\n\nAlibaba Cloud Model Studio is specially designed for developing generative AI. Through this platform, you can easily experience and quickly access Alibaba Cloud's latest industry-leading foundation models (FM), including Qwen-Max, Qwen-Plus, Qwen-Turbo, Qwen-VL, and the Wan series, to develop your own generative AI (GenAI) applications. You don't need to worry about underlying infrastructure and computing power. All operations are performed in isolated VPC networks to reduce security and privacy risks.... ##### Capability-Enhanced FMs\n\nModel Studio offers a variety of foundational models to meet diverse AI needs: flagship models with high intelligence, multifunctionality, and multilingual support; economical models with low cost, fast processing capability, and long text support; deep inference models designed for complex tasks to ensure accurate results; all-modal models that support text, audio, and video understanding and generation; visual language models capable of understanding images and videos; and the Wan series focused on advanced video generation.... ## Features\n\n\n\n##### Model Playground\n\nA variety of leading foundational models (FM) are available, including Alibaba Cloud's Qwen, Wan, and multimodal series models.\n\n\n\n##### High Cost-Effectiveness and Performance\n\nModel Studio provides official APIs for the Qwen series of large models, helping enterprises focus on business innovation and reducing technical maintenance burdens.... ## Scenarios\n\n### A Rich Selection of Models to Suit Diverse User Needs\n\n#### Qwen\n\nQwen-Max is an ultra-large-scale language model with hundreds of billions of parameters, supporting multi-lingual inputs including Chinese and English; Qwen-Turbo is a fast and cost-effective model with a maximum context length of up to 1M, it also supports multiple languages.... #### Wan\n\nWan supports text-to-video and image-to-video generation. Wan2.1 provides higher-quality visual generation, supporting complex movements, real-world physics simulation, rich artistic styles, and cinematic visual quality, with improved instruction-following capability.\n\n#### Multimodal Model Qwen-Omni\n\nQwen-Omni understands inputs in text, image, voice, and video, as well as mixed inputs. It also supports the simultaneous streaming generation of both text and voice, with accelerated multimodal content understanding, and four natural conversation voices.... #### Enhanced Reasoning Model QwQ\n\nQwQ is trained based on Qwen2.5 through reinforcement learning, with improved reasoning capability. The core indicators of the model such as mathematics code (AIME 24/25, LiveCodeBench) and some general indicators (IFEval, LiveBench, etc.) have reached the level of DeepSeek-R1 full edition.\n\n#### Visual Reasoning Model QVQ\n\nQVQ supports visual input and Chain-of-Thought output, demonstrating stronger capabilities in mathematics, programming, visual analysis, content creation, and general knowledge tasks.... ### Easy Agent Development with Assistant API\n\n#### Designed for Agent Development\n\n- Support for Qwen model series and Supervised Fine-Tuning (SFT)\n\n- Ready-to-use plugins and customizable functions\n\n- Developer-oriented SDK development support\n\n- Fast intervention and basic memory capabilities\n\n#### Prompt Engineering\n\n- 160+ prompt templates for various work and life scenarios\n\n- Configurable variables in templates for customization\n\n- Optimization based on meta-prompting\n\n- Example selecting tool for few-shot prompting... **Upgraded Support For You**\n\n1 on 1 Presale Consultation, 24/7 Technical Support, Faster Response, and More Free Tickets."
          },
          {
            "snippetId": "model-data-snippet-4",
            "url": "https://www.alibabacloud.com/help/en/model-studio/model-announcements",
            "snippet": "## Newly released models (Singapore)\n|||||\n|--|--|--|--|\n|Reasoning model|2025-09-11|qwen-plus-2025-09-11|This model is part of the Qwen3 series. Compared to qwen-plus-2025-07-28, it offers improved instruction following and more concise summaries in thinking mode, see Deep thinking. In non-thinking mode, its Chinese comprehension and logical reasoning capabilities are enhanced, see Overview.|... |Reasoning model|2025-09-11|qwen3-next-80b-a3b-thinking, qwen3-next-80b-a3b-instruct|These are new-generation open-source models based on Qwen3. The thinking model, compared to qwen3-235b-a22b-thinking-2507, provides better instruction following and more concise summaries, see Deep thinking. The instruct model, compared to qwen3-235b-a22b-instruct-2507, has enhanced capabilities in Chinese comprehension, logical reasoning, and text generation, see Overview.|... |Text-to-text|2025-09-05|qwen3-max-preview|The Qwen-Max model (preview) is based on Qwen3. It offers significant improvements in general capabilities compared to the Qwen 2.5 series. The model has enhanced abilities in Chinese and English text comprehension, complex instruction following, subjective open-ended tasks, multilingual support, and tool calling. It also produces fewer knowledge-based hallucinations. Qwen-Max|... |Visual understanding|2025-08-13|qwen-vl-max-2025-08-13|The visual understanding model features comprehensively improved metrics and significantly enhanced capabilities in mathematics, reasoning, object detection, and multilingual processing. Visual understanding|\n|Text-to-code|2025-08-05|qwen3-coder-flash, qwen3-coder-flash-2025-07-28|The fastest and most cost-effective model in the Qwen-Coder series. Code capability|... |Reasoning model|2025-08-05|qwen-flash, qwen-flash-2025-07-28|The fastest and most cost-effective model in the Qwen series. Ideal for simple tasks. Qwen-Flash.|\n|Reasoning model|2025-07-30|qwen-plus-2025-07-28|This model is part of the Qwen3 series and its context length has been extended to 1,000,000, an improvement over the previous version. For the thinking mode, see Deep thinking. For the non-thinking mode, see Overview.|... |Text-to-code|2025-07-23|qwen3-coder, qwen3-coder-plus-2025-07-22|A code generation model based on Qwen3. It has powerful coding agent capability, excels at tool calling and environment interaction, and combines excellent coding skills with general-purpose functionality. Code capability.|... 14b, qwen3-8b, qwen3-4b, qwen3-1.7b, qwen3-0.6b|Qwen3 is capable of responding in both thinking and non-thinking modes, allowing you to switch between the two using the For thinking mode, see Deep thinking. For non-thinking mode, see Overview.|... ## Newly released models (Beijing)\n|||||\n|--|--|--|--|\n|Reasoning model|2025-09-11|qwen-plus-2025-09-11|This model is part of the Qwen3 series. Compared to qwen-plus-2025-07-28, it offers improved instruction following and more concise summaries in thinking mode, see Deep thinking. In non-thinking mode, its Chinese comprehension and logical reasoning capabilities are enhanced, see Overview.|... |Reasoning model|2025-09-11|qwen3-next-80b-a3b-thinking, qwen3-next-80b-a3b-instruct|These are new-generation open-source models based on Qwen3. The thinking model, compared to qwen3-235b-a22b-thinking-2507, provides better instruction following and more concise summaries, see Deep thinking. The instruct model, compared to qwen3-235b-a22b-instruct-2507, has enhanced capabilities in Chinese comprehension, logical reasoning, and text generation, see Overview.|... |Text-to-text|2025-09-05|qwen3-max-preview|The Qwen-Max model (preview) is based on Qwen3. It offers significant improvements in general capabilities compared to the Qwen 2.5 series. The model has enhanced abilities in Chinese and English text comprehension, complex instruction following, subjective open-ended tasks, multilingual support, and tool calling. It also produces fewer knowledge-based hallucinations. Qwen-Max|"
          },
          {
            "snippetId": "model-data-snippet-5",
            "url": "https://qwenlm.github.io/blog/qwen3/",
            "snippet": "QWEN CHAT GitHub Hugging Face ModelScope Kaggle DEMO DISCORD\n\n## Introduction\n\nToday, we are excited to announce the release of... |Models|Layers|Heads (Q / KV)|# Experts (Total / Activated)|Context Length|\n|--|--|--|--|--|\n|Qwen3-30B-A3B|48|32 / 4|128 / 8|128K|\n|Qwen3-235B-A22B|94|64 / 4|128 / 8|128K|... The post-trained models, such as\n\n**Qwen3-30B-A3B**, along with their pre-trained counterparts (e.g., **Qwen3-30B-A3B-Base**), are now available on platforms like **Hugging Face**, **ModelScope**, and **Kaggle**. For deployment, we recommend using frameworks like **SGLang** and **vLLM**.... For local usage, tools such as **Ollama**, **LMStudio**, **MLX**, **llama.cpp**, and **KTransformers** are highly recommended. These options ensure that users can easily integrate Qwen3 into their workflows, whether in research, development, or production environments.\n\nWe believe that the release and open-sourcing of Qwen3 will significantly advance the research and development of large foundation models. Our goal is to empower researchers, developers, and organizations around the world to build innovative solutions using these cutting-edge models.... Feel free to try Qwen3 out in Qwen Chat Web (chat.qwen.ai) and mobile APP!... ## Pre-training\n\nIn terms of pretraining, the dataset for Qwen3 has been significantly expanded compared to Qwen2.5. While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents.... We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.... The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. This ensures the model can handle longer inputs effectively.... Due to advancements in model architecture, increase in training data, and more effective training methods, the overall performance of Qwen3 dense base models matches that of Qwen2.5 base models with more parameters. For instance, Qwen3-1.7B/4B/8B/14B/32B-Base performs as well as Qwen2.5-3B/7B/14B/32B/72B-Base, respectively.... ## Develop with Qwen3\n\nBelow is a simple guide for you to use Qwen3 on different frameworks. First of all, we provide an standard example of using Qwen3-30B-A3B in Hugging Face transformers:... ### Agentic Usages\n\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.... ```\n\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\n\nllm_cfg = {\n\n'model': 'Qwen3-30B-A3B',\n\n# Use the endpoint provided by Alibaba Model Studio:\n\n# 'model_type': 'qwen_dashscope',\n\n# 'api_key': os.getenv('DASHSCOPE_API_KEY'),... Looking ahead, we aim to enhance our models across multiple dimensions. This includes refining model architectures and training methodologies to achieve several key objectives: scaling data, increasing model size, extending context length, broadening modalities, and advancing RL with environmental feedback for long-horizon reasoning. We believe we are transitioning from an era focused on training models to one centered on training agents. Our next iteration promises to bring meaningful advancements to everyone’s work and life."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Training Dataset",
            "score": 1,
            "explanation": "Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects."
          },
          {
            "name": "Training Data Processing",
            "score": 1,
            "explanation": "We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content."
          },
          {
            "name": "Knowledge Count",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      },
      {
        "sectionId": "model-implementation-sustainability",
        "sectionSnippets": [
          {
            "snippetId": "model-implementation-sustainability-snippet-1",
            "url": "https://en.wikipedia.org/wiki/Qwen",
            "snippet": "## Models\n\nAlibaba launched a beta of Qwen in April 2023 under the name Tongyi Qianwen, then opened it for public use in September 2023 after regulatory clearance.... The model's architecture was based on the Llama architecture developed by Meta AI. In December 2023, it released its 72B and 1.8B models for download, while Qwen 7B weights were released in August. Their models are sometimes described as open source, but the training code has not been released nor has the training data been documented, and they do not meet the terms of either the Open Source AI Definition or the Model Openness Framework from the Linux Foundation.... In June 2024 Alibaba launched Qwen2 and in September it released some of its models with open weights, while keeping its most advanced models proprietary. Qwen2 contains both dense and sparse models.\n\nIn November 2024, QwQ-32B-Preview, a model focusing on reasoning similar to OpenAI's o1, was released under the Apache 2.0 License, although only the weights were released, not the dataset or training method. QwQ has a 32K token context length and performs better than o1 on some benchmarks.... The Qwen-VL series is a line of visual language models that combines a vision transformer with a LLM. Alibaba released Qwen2-VL with variants of 2 billion and 7 billion parameters.... In January 2025, Qwen2.5-VL was released with variants of 3, 7, 32, and 72 billion parameters. All models except the 72B variant are licensed under the Apache 2.0 license. Qwen-VL-Max is Alibaba's flagship vision model as of 2024, and is sold by Alibaba Cloud at a cost of US$0.41 per million input tokens.... Alibaba has released several other model types such as Qwen-Audio and Qwen2-Math. In total, it has released more than 100 open weight models, with its models having been downloaded more than 40 million times. Fine-tuned versions of Qwen have been developed by enthusiasts, such as \"Liberated Qwen\", developed by San Francisco-based Abacus AI, which is a version that responds to any user request without content restrictions.... On January 29, 2025, Alibaba launched Qwen2.5-Max. According to a blog post from Alibaba, Qwen2.5-Max outperforms other foundation models such as GPT-4o, DeepSeek-V3, and Llama-3.1-405B in key benchmarks. In February 2025, Alibaba announced on their official X account that the 2.5-Max model would be opened up, however it has not been released.... On March 24, 2025, Alibaba launched Qwen2.5-VL-32B-Instruct as a successor to the Qwen2.5-VL model. It was released under the Apache 2.0 license.... On September 5, 2025, Alibaba launched Qwen3-Max. According to Alibaba's official X account, it outperforms other foundation non-reasoning models such as Qwen3-235B-A22B-Instruct-2507, Kimi K2, Claude 4 Opus Non-thinking, and DeepSeek V3.1. While it was not available from the start, thinking mode was released to the public in the first part of November 2025.... On September 10, 2025, Qwen3-Next was released under the Apache 2.0 license and made available through chat.qwen.ai, as well as platforms like Hugging Face and Model Scope. Qwen3-Next includes two post-trained Instruct and Thinking models. Qwen3-Next was created with a new model-architecture called Qwen3-Next, in the belief that Context Length Scaling and Total Parameter Scaling are two major trends in the future of large models.... The Qwen3-Next model performs comparable to, or in some cases better than, Qwen3-32b while using less than 10% of its training cost (in GPU hours). In inference, especially with contexts greater than 32K tokens, it reaches greater than 10x higher throughput. Qwen3.5 will use a refined version of the Qwen3-Next architecture.... ## See also\n- List of large language models\n\n## References\n\n## External links\n- Official website\n- Qwen Chat\n- Qwen on GitHub\n- Qwen on Hugging Face\n\nCategories: - Alibaba Group\n- Large language models\n- Open-source artificial intelligence\n- Software using the Apache license\n- Generative pre-trained transformers\n- 2023 in artificial intelligence\n- 2023 software"
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-2",
            "url": "https://github.com/QwenLM/Qwen3",
            "snippet": "QwenLM / **\nQwen3 ** Public\n\n# QwenLM/Qwen3\n# Qwen3\n\n💜 **Qwen Chat** | 🤗 Hugging Face | 🤖 ModelScope | 📑 Paper | 📑 Blog ｜ 📖 Documentation 🖥️ Demo | 💬 WeChat (微信) | 🫨 Discord\n\nVisit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with... ```\nQwen3-\n```\nor visit the Qwen3 collection, and you will find all you need! Enjoy!\n\nTo learn more about Qwen3, feel free to read our documentation [EN|ZH]. Our documentation consists of the following sections:... - Quickstart: the basic usages and demonstrations;\n- Inference: the guidance for the inference with Transformers, including batch inference, streaming, etc.;\n- Run Locally: the instructions for running LLM locally on CPU and GPU, with frameworks like llama.cpp and Ollama;\n- Deployment: the demonstration of how to deploy Qwen for large-scale inference with frameworks like SGLang, vLLM, TGI, etc.;... - Quantization: the practice of quantizing LLMs with GPTQ, AWQ, as well as the guidance for how to make high-quality quantized GGUF files;\n- Training: the instructions for post-training, including SFT and RLHF (TODO) with frameworks like Axolotl, LLaMA-Factory, etc.\n- Framework: the usage of Qwen with frameworks for application, e.g., RAG, Agent, etc.... ### Qwen3 (aka Qwen3-2504)\n\nWe are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models.\nThese models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5.\nWe are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models.\nThe highlights from Qwen3 include:... - 2025.08.06: The final open release of Qwen3-2507, Qwen3-4B-Instruct-2507 and Qwen3-4B-Thinking-2507, is out!\n- 2025.07.31: Qwen3-30B-A3B-Thinking-2507 is released. Check out the modelcard for more details!... - 2025.07.30: Qwen3-30B-A3B-Instruct-2507 is released. Check out the modelcard for more details!\n- 2025.07.25: We released the updated version of Qwen3-235B-A22B thinking mode, named Qwen3-235B-A22B-Thinking-2507. Check out the modelcard for more details!... - 2024.06.06: We released the Qwen2 series. Check our blog!\n- 2024.03.28: We released the first MoE model of Qwen: Qwen1.5-MoE-A2.7B! Temporarily, only HF transformers and vLLM support the model. We will soon add the support of llama.cpp, mlx-lm, etc. Check our blog for more information!\n- 2024.02.05: We released the Qwen1.5 series.... ## Performance\n\nDetailed evaluation results are reported in this 📑 blog (Qwen3-2504) and this 📑 blog (Qwen3-2507) [coming soon].\n\nFor requirements on GPU memory and the respective throughput, see results here.... ### ModelScope\n\nWe strongly advise users especially those in mainland China to use ModelScope.\nModelScope adopts a Python API similar to Transformers.\nThe CLI tool\n```\nmodelscope download\n```\ncan help you solve issues concerning downloading checkpoints.\nFor vLLM and SGLang, the environment variable\n```\nVLLM_USE_MODELSCOPE=true\n```\nand\n```\nSGLANG_USE_MODELSCOPE=true\n```\ncan be used respectively.... ### LMStudio\n\nQwen3 has already been supported by lmstudio.ai. You can directly use LMStudio with our GGUF files.\n\n### ExecuTorch\n\nTo export and run on ExecuTorch (iOS, Android, Mac, Linux, and more), please follow this example.\n\n### MNN\n\nTo export and run on MNN, which supports Qwen3 on mobile devices, please visit Alibaba MNN.... ### MLX LM\n\nIf you are running on Apple Silicon,\n```\nmlx-lm\n```\nalso supports Qwen3 (\n```\nmlx-lm>=0.24.0\n```\n).\nLook for models ending with MLX on Hugging Face Hub."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-3",
            "url": "https://qwen.readthedocs.io",
            "snippet": "# Welcome to Qwen!¶\n\nQwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Both language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.... The latest version, Qwen3, has the following features:\n**Dense and Mixture-of-Experts (MoE) models**, available in 0.6B, 1.7B, 4B, 8B, 14B, 32B and 30B-A3B, 235B-A22B. **Seamless switching between thinking mode**(for complex logical reasoning, math, and coding) and **non-thinking mode**(for efficient, general-purpose chat) **within a single model**, ensuring optimal performance across various scenarios.... **Significantly enhancement in reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.... **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks. **Support of 100+ languages and dialects**with strong capabilities for **multilingual instruction following**and **translation**.\n\nFor more information, please visit our:\nJoin our community by joining our and group. We are looking forward to seeing you there!"
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-4",
            "url": "https://github.com/QwenLM/Qwen/",
            "snippet": "`$DATA`.\n\nThe finetuning scripts allow you to perform:\n\n- Full-parameter finetuning\n\n- LoRA\n\n- Q-LoRA\n\nFull-parameter finetuning requires updating all parameters in the whole training process. To launch your training, run the following script:\n\n```\n\n# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.\n\nbash finetune/finetune_ds.sh\n\n```... Remember to specify the correct model name or path, the data path, as well as the output directory in the shell scripts. Another thing to notice is that we use DeepSpeed ZeRO 3 in this script. If you want to make changes, just remove the argument\n\n`--deepspeed` or make changes in the DeepSpeed configuration json file based on your requirements. Additionally, this script supports mixed-precision training, and thus you can use... `--bf16 True` or\n\n`--fp16 True`. Remember to use DeepSpeed when you use fp16 due to mixed precision training. Empirically we advise you to use bf16 to make your training consistent with our pretraining and alignment if your machine supports bf16, and thus we use it by default.... Similarly, to run LoRA, use another script to run as shown below. Before you start, make sure that you have installed\n\n`peft`. Also, you need to specify your paths to your model, data, and output. We advise you to use absolute path for your pretrained model. This is because LoRA only saves the adapter and the absolute path in the adapter configuration json file is used for finding out the pretrained model to load. Also, this script support both bf16 and fp16.... # Single GPU training bash finetune/finetune_lora_single_gpu.sh # Distributed training bash finetune/finetune_lora_ds.sh\n\nIn comparison with full-parameter finetuning, LoRA (paper) only updates the parameters of adapter layers but keeps the original large language model layers frozen. This allows much fewer memory costs and thus fewer computation costs.... Note that if you use LoRA to finetune the base language model, e.g., Qwen-7B, instead of chat models, e.g., Qwen-7B-Chat, the script automatically switches the embedding and output layer as trainable parameters. This is because the base language model has no knowledge of special tokens brought by ChatML format. Thus these layers should be updated for the model to understand and predict the tokens. Or in another word, if your training brings in special tokens in LoRA, you should set the layers to trainable parameters by setting... `modules_to_save` inside the code. Also, if we have these parameters trainable, it is not available to use ZeRO 3, and this is why we use ZeRO 2 in the script by default. If you do not have new trainable parameters, you can switch to ZeRO 3 by changing the DeepSpeed configuration file. Additionally, we find that there is a significant gap between the memory footprint of LoRA with and without these trainable parameters. Therefore, if you have trouble with memory, we advise you to LoRA finetune the chat models. Check the profile below for more information.... If you still suffer from insufficient memory, you can consider Q-LoRA (paper), which uses the quantized large language model and other techniques such as paged attention to allow even fewer memory costs.\n\nNote: to run single-GPU Q-LoRA training, you may need to install\n\n`mpi4py` through\n\n`pip` or\n\n`conda`.\n\nTo run Q-LoRA, directly run the following script:... # Single GPU training bash finetune/finetune_qlora_single_gpu.sh # Distributed training bash finetune/finetune_qlora_ds.sh\n\nFor Q-LoRA, we advise you to load our provided quantized model, e.g., Qwen-7B-Chat-Int4. You\n\n**SHOULD NOT** use the bf16 models. Different from full-parameter finetuning and LoRA, only fp16 is supported for Q-LoRA. For single-GPU training, we have to use DeepSpeed for mixed-precision tr"
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-5",
            "url": "https://www.alibabacloud.com/en/product/modelstudio?_p_lc=1",
            "snippet": "### Latest Updates\n\nThe Qwen3 series has launched a variety of powerful new models, as follows:\n\n● qwen3-max, qwen3-max-2025-09-23: The Qwen Max model has been updated with the \"no-thinking\" mode. Compared to the previous version (qwen3-max-preview), it shows significant improvements in agent programming capabilities, agent tool usage, logical reasoning, code-based problem solving, and text reasoning.... ● qwen3-vl-plus, qwen3-vl-plus-2025-09-23: Vision-language understanding models in the Qwen3 series, comprehensively upgraded in areas such as visual coding, spatial perception, and multimodal reasoning. Supports ultra-long video understanding.\n\n● qwen-image-plus: An image generation model in the Tongyi Qianwen series, excelling in complex text rendering and various image generation and editing tasks.... ● qwen3-coder-plus-2025-09-23: A code generation model based on Qwen3. Compared to the previous version (July 22 snapshot), it demonstrates improved robustness in downstream tasks and tool calling, along with enhanced code security.... ## Qwen3: Major Models Unveiled!\n\nAll Qwen3 models seamlessly integrate \"thinking\" and \"non-thinking\" modes, allowing you to switch modes during conversation. Try It Now in Model Studio!\n\n### Innovative Dual Thinking Modes\n\nBalancing inference depth and speed flexibly while managing costs effectively with Qwen3's unique \"Thinking\" (deep reasoning) and \"Non-thinking\" (fast response) modes... ## Why Alibaba Cloud Model Studio?\n\nAlibaba Cloud Model Studio is specially designed for developing generative AI. Through this platform, you can easily experience and quickly access Alibaba Cloud's latest industry-leading foundation models (FM), including Qwen-Max, Qwen-Plus, Qwen-Turbo, Qwen-VL, and the Wan series, to develop your own generative AI (GenAI) applications. You don't need to worry about underlying infrastructure and computing power. All operations are performed in isolated VPC networks to reduce security and privacy risks.... ##### Built-In Model Inference and Evaluation Workflows\n\nYou can quickly perform model development, debugging, and performance monitoring to ensure the security and stability of enterprise-level applications while accelerating the development process and reducing business risks, with Model Studio’s support for real-time and batch model inference tasks, user-defined model parameter configuration, model effect evaluation, and model performance observation, etc.... ## Features\n\n\n\n##### Model Playground\n\nA variety of leading foundational models (FM) are available, including Alibaba Cloud's Qwen, Wan, and multimodal series models.\n\n\n\n##### High Cost-Effectiveness and Performance\n\nModel Studio provides official APIs for the Qwen series of large models, helping enterprises focus on business innovation and reducing technical maintenance burdens.... ## Scenarios\n\n### A Rich Selection of Models to Suit Diverse User Needs\n\n#### Qwen\n\nQwen-Max is an ultra-large-scale language model with hundreds of billions of parameters, supporting multi-lingual inputs including Chinese and English; Qwen-Turbo is a fast and cost-effective model with a maximum context length of up to 1M, it also supports multiple languages.... #### Wan\n\nWan supports text-to-video and image-to-video generation. Wan2.1 provides higher-quality visual generation, supporting complex movements, real-world physics simulation, rich artistic styles, and cinematic visual quality, with improved instruction-following capability.\n\n#### Multimodal Model Qwen-Omni\n\nQwen-Omni understands inputs in text, image, voice, and video, as well as mixed inputs. It also supports the simultaneous streaming generation of both text and voice, with accelerated multimodal content understanding, and four natural conversation voices.... #### Enhanced Reasoning Model QwQ\n\nQwQ is trained based on Qwen2.5 through reinforcement learning, with improved reasoning capability. The core indicators of the model such as mathematics code (AIME 24/25, LiveCodeBench) and some general indicators (IFEval, LiveBench, etc.) have reached the level of DeepSeek-R1 full edition.\n\n#### Visual Reasoning Model QVQ\n\nQVQ supports visual input and Chain-of-Thought output, demonstrating stronger capabilities in mathematics, programming, visual analysis, content creation, and general knowledge tasks.... #### Functionality Enhancement\n\n- An evolving architecture with new algorithms and technologies\n\n- Atomic function optimized for prompts, responses, and task execution\n\n- The SDK development paradigm consistent with OpenAPI... ZKH used the Alibaba Cloud Model Studio platform to directly access the commercial version of the Tongyi Qwen (Qwen-Max) LLM. By leveraging this AI-powered solution, customers can identify the exact product SKU they need simply by describing their requirements, turning the platform into an advanced product recommendation assistant. By using AI to deconstruct and optimize 16 scenarios and 120 sub-scenarios, ZKH has vastly reduced RFQ time, enhanced operational efficiency, and saved costs."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Hardware Used During Training & Inference",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Software Frameworks & Tooling",
            "score": 1,
            "explanation": "The instructions for running LLM locally on CPU and GPU, with frameworks like llama.cpp and Ollama"
          },
          {
            "name": "Energy Use/ Sustainability Metrics",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      },
      {
        "sectionId": "intended-use",
        "sectionSnippets": [
          {
            "snippetId": "intended-use-snippet-1",
            "url": "https://en.wikipedia.org/wiki/Qwen",
            "snippet": "**Qwen** (also known as **Tongyi Qianwen**, Chinese: 通义千问; pinyin: *Tōngyì Qiānwèn*) is a family of large language models developed by Alibaba Cloud. Many Qwen variants are distributed as open‑weight models under the Apache‑2.0 license, while others are served through Alibaba Cloud.... ## Models\n\nAlibaba launched a beta of Qwen in April 2023 under the name Tongyi Qianwen, then opened it for public use in September 2023 after regulatory clearance.... In June 2024 Alibaba launched Qwen2 and in September it released some of its models with open weights, while keeping its most advanced models proprietary. Qwen2 contains both dense and sparse models.\n\nIn November 2024, QwQ-32B-Preview, a model focusing on reasoning similar to OpenAI's o1, was released under the Apache 2.0 License, although only the weights were released, not the dataset or training method. QwQ has a 32K token context length and performs better than o1 on some benchmarks.... In January 2025, Qwen2.5-VL was released with variants of 3, 7, 32, and 72 billion parameters. All models except the 72B variant are licensed under the Apache 2.0 license. Qwen-VL-Max is Alibaba's flagship vision model as of 2024, and is sold by Alibaba Cloud at a cost of US$0.41 per million input tokens.... Alibaba has released several other model types such as Qwen-Audio and Qwen2-Math. In total, it has released more than 100 open weight models, with its models having been downloaded more than 40 million times. Fine-tuned versions of Qwen have been developed by enthusiasts, such as \"Liberated Qwen\", developed by San Francisco-based Abacus AI, which is a version that responds to any user request without content restrictions.... On January 29, 2025, Alibaba launched Qwen2.5-Max. According to a blog post from Alibaba, Qwen2.5-Max outperforms other foundation models such as GPT-4o, DeepSeek-V3, and Llama-3.1-405B in key benchmarks. In February 2025, Alibaba announced on their official X account that the 2.5-Max model would be opened up, however it has not been released.... On March 24, 2025, Alibaba launched Qwen2.5-VL-32B-Instruct as a successor to the Qwen2.5-VL model. It was released under the Apache 2.0 license.... On April 28, 2025, the Qwen3 model family was released, with all models licensed under the Apache 2.0 license. The Qwen3 model family includes both dense (0.6B, 1.7B, 4B, 8B, 14B, and 32B parameters) and sparse models (30B with 3B activated parameters, 235B with 22B activated parameters).... On September 5, 2025, Alibaba launched Qwen3-Max. According to Alibaba's official X account, it outperforms other foundation non-reasoning models such as Qwen3-235B-A22B-Instruct-2507, Kimi K2, Claude 4 Opus Non-thinking, and DeepSeek V3.1. While it was not available from the start, thinking mode was released to the public in the first part of November 2025.... On September 10, 2025, Qwen3-Next was released under the Apache 2.0 license and made available through chat.qwen.ai, as well as platforms like Hugging Face and Model Scope. Qwen3-Next includes two post-trained Instruct and Thinking models. Qwen3-Next was created with a new model-architecture called Qwen3-Next, in the belief that Context Length Scaling and Total Parameter Scaling are two major trends in the future of large models.... The Qwen3-Next model performs comparable to, or in some cases better than, Qwen3-32b while using less than 10% of its training cost (in GPU hours). In inference, especially with contexts greater than 32K tokens, it reaches greater than 10x higher throughput. Qwen3.5 will use a refined version of the Qwen3-Next architecture.... On September 22, 2025, Qwen3-Omni was release under the Apache 2.0 license and made available through chat.qwen.ai, as well as platforms like Hugging Face and Model Scope. Qwen3-Omni is a mixed/multimodal model that can process text, images, audio, and video, and deliver real-time streaming responses in both text and natural speech."
          },
          {
            "snippetId": "intended-use-snippet-2",
            "url": "https://github.com/QwenLM/Qwen3",
            "snippet": "```\nQwen3-\n```\nor visit the Qwen3 collection, and you will find all you need! Enjoy!\n\nTo learn more about Qwen3, feel free to read our documentation [EN|ZH]. Our documentation consists of the following sections:... - Quickstart: the basic usages and demonstrations;\n- Inference: the guidance for the inference with Transformers, including batch inference, streaming, etc.;\n- Run Locally: the instructions for running LLM locally on CPU and GPU, with frameworks like llama.cpp and Ollama;\n- Deployment: the demonstration of how to deploy Qwen for large-scale inference with frameworks like SGLang, vLLM, TGI, etc.;... ## News\n- 2025.08.08: You can now use Qwen3-2507 to handle ultra-long inputs of **1 million tokens**! See the update modelcards (235B-A22B-Instruct-2507, 235B-A22B-Thinking-2507, A30B-A3B-Instruct-2507, A30B-A3B-Thinking-2507) for how to enable this feature.... - 2025.08.06: The final open release of Qwen3-2507, Qwen3-4B-Instruct-2507 and Qwen3-4B-Thinking-2507, is out!\n- 2025.07.31: Qwen3-30B-A3B-Thinking-2507 is released. Check out the modelcard for more details!... - 2025.07.30: Qwen3-30B-A3B-Instruct-2507 is released. Check out the modelcard for more details!\n- 2025.07.25: We released the updated version of Qwen3-235B-A22B thinking mode, named Qwen3-235B-A22B-Thinking-2507. Check out the modelcard for more details!... - 2025.07.21: We released the updated version of Qwen3-235B-A22B non-thinking mode, named Qwen3-235B-A22B-Instruct-2507, featuring significant enhancements over the previous version and supporting 256K-token long-context understanding. Check our modelcard for more details!... - 2025.04.29: We released the Qwen3 series. Check our blog for more details!\n- 2024.09.19: We released the Qwen2.5 series. This time there are 3 extra model sizes: 3B, 14B, and 32B for more possibilities. Check our blog for more!... - 2024.06.06: We released the Qwen2 series. Check our blog!\n- 2024.03.28: We released the first MoE model of Qwen: Qwen1.5-MoE-A2.7B! Temporarily, only HF transformers and vLLM support the model. We will soon add the support of llama.cpp, mlx-lm, etc. Check our blog for more information!\n- 2024.02.05: We released the Qwen1.5 series.... ## Performance\n\nDetailed evaluation results are reported in this 📑 blog (Qwen3-2504) and this 📑 blog (Qwen3-2507) [coming soon].\n\nFor requirements on GPU memory and the respective throughput, see results here.... ### ModelScope\n\nWe strongly advise users especially those in mainland China to use ModelScope.\nModelScope adopts a Python API similar to Transformers.\nThe CLI tool\n```\nmodelscope download\n```\ncan help you solve issues concerning downloading checkpoints.\nFor vLLM and SGLang, the environment variable\n```\nVLLM_USE_MODELSCOPE=true\n```\nand\n```\nSGLANG_USE_MODELSCOPE=true\n```\ncan be used respectively.... You can also access the Ollama service via its OpenAI-compatible API.\nPlease note that you need to (1) keep\n```\nollama serve\n```\nrunning while using the API, and (2) execute\n```\nollama run qwen3:8b\n```\nbefore utilizing this API to ensure that the model checkpoint is prepared.\nThe API is at... ### LMStudio\n\nQwen3 has already been supported by lmstudio.ai. You can directly use LMStudio with our GGUF files.\n\n### ExecuTorch\n\nTo export and run on ExecuTorch (iOS, Android, Mac, Linux, and more), please follow this example.\n\n### MNN\n\nTo export and run on MNN, which supports Qwen3 on mobile devices, please visit Alibaba MNN.... ### MLX LM\n\nIf you are running on Apple Silicon,\n```\nmlx-lm\n```\nalso supports Qwen3 (\n```\nmlx-lm>=0.24.0\n```\n).\nLook for models ending with MLX on Hugging Face Hub."
          },
          {
            "snippetId": "intended-use-snippet-3",
            "url": "https://www.alibabacloud.com/en/product/modelstudio?_p_lc=1",
            "snippet": "### Latest Updates\n\nThe Qwen3 series has launched a variety of powerful new models, as follows:\n\n● qwen3-max, qwen3-max-2025-09-23: The Qwen Max model has been updated with the \"no-thinking\" mode. Compared to the previous version (qwen3-max-preview), it shows significant improvements in agent programming capabilities, agent tool usage, logical reasoning, code-based problem solving, and text reasoning.... ● qwen3-vl-plus, qwen3-vl-plus-2025-09-23: Vision-language understanding models in the Qwen3 series, comprehensively upgraded in areas such as visual coding, spatial perception, and multimodal reasoning. Supports ultra-long video understanding.\n\n● qwen-image-plus: An image generation model in the Tongyi Qianwen series, excelling in complex text rendering and various image generation and editing tasks.... ● qwen3-coder-plus-2025-09-23: A code generation model based on Qwen3. Compared to the previous version (July 22 snapshot), it demonstrates improved robustness in downstream tasks and tool calling, along with enhanced code security.... ● qwen3-livetranslate-flash-realtime, qwen3-livetranslate-flash-realtime-2025-09-22: Mainline and snapshot versions of the Qwen3-Omni series for real-time translation. This is the real-time version of Tongyi Qianwen 3-LiveTranslate-Flash, a high-precision, responsive, and robust large model for real-time multilingual audio-video interpretation.... ## Qwen3: Major Models Unveiled!\n\nAll Qwen3 models seamlessly integrate \"thinking\" and \"non-thinking\" modes, allowing you to switch modes during conversation. Try It Now in Model Studio!\n\n### Innovative Dual Thinking Modes\n\nBalancing inference depth and speed flexibly while managing costs effectively with Qwen3's unique \"Thinking\" (deep reasoning) and \"Non-thinking\" (fast response) modes... ## Why Alibaba Cloud Model Studio?\n\nAlibaba Cloud Model Studio is specially designed for developing generative AI. Through this platform, you can easily experience and quickly access Alibaba Cloud's latest industry-leading foundation models (FM), including Qwen-Max, Qwen-Plus, Qwen-Turbo, Qwen-VL, and the Wan series, to develop your own generative AI (GenAI) applications. You don't need to worry about underlying infrastructure and computing power. All operations are performed in isolated VPC networks to reduce security and privacy risks.... ## Features\n\n\n\n##### Model Playground\n\nA variety of leading foundational models (FM) are available, including Alibaba Cloud's Qwen, Wan, and multimodal series models.\n\n\n\n##### High Cost-Effectiveness and Performance\n\nModel Studio provides official APIs for the Qwen series of large models, helping enterprises focus on business innovation and reducing technical maintenance burdens.... ##### Multiple Access Methods\n\nYou can invocate these models by calling APIs, which are compatible with OpenAI, and come with DashScope SDK for flexible and convenient access.\n\n\n\n##### Secure and Efficient Data Transmission\n\nModel Studio enhances data security and transmission efficiency via private network access, using PrivateLink to establish dedicated VPC connections.\n\n**Activate Alibaba Cloud Model Studio in the console to enjoy 1 million free tokens and the full experience**... ## Scenarios\n\n### A Rich Selection of Models to Suit Diverse User Needs\n\n#### Qwen\n\nQwen-Max is an ultra-large-scale language model with hundreds of billions of parameters, supporting multi-lingual inputs including Chinese and English; Qwen-Turbo is a fast and cost-effective model with a maximum context length of up to 1M, it also supports multiple languages.... #### Wan\n\nWan supports text-to-video and image-to-video generation. Wan2.1 provides higher-quality visual generation, supporting complex movements, real-world physics simulation, rich artistic styles, and cinematic visual quality, with improved instruction-following capability.\n\n#### Multimodal Model Qwen-Omni\n\nQwen-Omni understands inputs in text, image, voice, and video, as well as mixed inputs. It also supports the simultaneous streaming generation of both text and voice, with accelerated multimodal content understanding, and four natural conversation voices.... #### Enhanced Reasoning Model QwQ\n\nQwQ is trained based on Qwen2.5 through reinforcement learning, with improved reasoning capability. The core indicators of the model such as mathematics code (AIME 24/25, LiveCodeBench) and some general indicators (IFEval, LiveBench, etc.) have reached the level of DeepSeek-R1 full edition.\n\n#### Visual Reasoning Model QVQ\n\nQVQ supports visual input and Chain-of-Thought output, demonstrating stronger capabilities in mathematics, programming, visual analysis, content creation, and general knowledge tasks.... **Upgraded Support For You**\n\n1 on 1 Presale Consultation, 24/7 Technical Support, Faster Response, and More Free Tickets."
          },
          {
            "snippetId": "intended-use-snippet-4",
            "url": "https://github.com/QwenLM/Qwen/",
            "snippet": "`$DATA`.\n\nThe finetuning scripts allow you to perform:\n\n- Full-parameter finetuning\n\n- LoRA\n\n- Q-LoRA\n\nFull-parameter finetuning requires updating all parameters in the whole training process. To launch your training, run the following script:\n\n```\n\n# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.\n\nbash finetune/finetune_ds.sh\n\n```... Remember to specify the correct model name or path, the data path, as well as the output directory in the shell scripts. Another thing to notice is that we use DeepSpeed ZeRO 3 in this script. If you want to make changes, just remove the argument\n\n`--deepspeed` or make changes in the DeepSpeed configuration json file based on your requirements. Additionally, this script supports mixed-precision training, and thus you can use... `--bf16 True` or\n\n`--fp16 True`. Remember to use DeepSpeed when you use fp16 due to mixed precision training. Empirically we advise you to use bf16 to make your training consistent with our pretraining and alignment if your machine supports bf16, and thus we use it by default.... Similarly, to run LoRA, use another script to run as shown below. Before you start, make sure that you have installed\n\n`peft`. Also, you need to specify your paths to your model, data, and output. We advise you to use absolute path for your pretrained model. This is because LoRA only saves the adapter and the absolute path in the adapter configuration json file is used for finding out the pretrained model to load. Also, this script support both bf16 and fp16.... # Single GPU training bash finetune/finetune_lora_single_gpu.sh # Distributed training bash finetune/finetune_lora_ds.sh\n\nIn comparison with full-parameter finetuning, LoRA (paper) only updates the parameters of adapter layers but keeps the original large language model layers frozen. This allows much fewer memory costs and thus fewer computation costs.... Note that if you use LoRA to finetune the base language model, e.g., Qwen-7B, instead of chat models, e.g., Qwen-7B-Chat, the script automatically switches the embedding and output layer as trainable parameters. This is because the base language model has no knowledge of special tokens brought by ChatML format. Thus these layers should be updated for the model to understand and predict the tokens. Or in another word, if your training brings in special tokens in LoRA, you should set the layers to trainable parameters by setting... `modules_to_save` inside the code. Also, if we have these parameters trainable, it is not available to use ZeRO 3, and this is why we use ZeRO 2 in the script by default. If you do not have new trainable parameters, you can switch to ZeRO 3 by changing the DeepSpeed configuration file. Additionally, we find that there is a significant gap between the memory footprint of LoRA with and without these trainable parameters. Therefore, if you have trouble with memory, we advise you to LoRA finetune the chat models. Check the profile below for more information.... If you still suffer from insufficient memory, you can consider Q-LoRA (paper), which uses the quantized large language model and other techniques such as paged attention to allow even fewer memory costs.\n\nNote: to run single-GPU Q-LoRA training, you may need to install\n\n`mpi4py` through\n\n`pip` or\n\n`conda`.\n\nTo run Q-LoRA, directly run the following script:... # Single GPU training bash finetune/finetune_qlora_single_gpu.sh # Distributed training bash finetune/finetune_qlora_ds.sh\n\nFor Q-LoRA, we advise you to load our provided quantized model, e.g., Qwen-7B-Chat-Int4. You\n\n**SHOULD NOT** use the bf16 models. Different from full-parameter finetuning and LoRA, only fp16 is supported for Q-LoRA. For single-GPU training, we have to use DeepSpeed for mixed-precision tr"
          },
          {
            "snippetId": "intended-use-snippet-5",
            "url": "https://www.alibabacloud.com/help/en/model-studio/what-is-qwen-llm",
            "snippet": "|Always has the same capabilities as the latest snapshot version|Latest|$1.6|$6.4|\n|qwen-max-2025-01-25 also known as qwen-max-0125 or Qwen2.5-Max|Snapshot|\n**Chinese mainland (Beijing)**... ||qwen3-max and qwen3-max-preview support context cache.||\n|--|--|--|\n|0 < Tokens ≤ 32K|$0.861|$3.441|\n|32K < Tokens ≤ 128K|$1.434|$5.735|\n|128K < Tokens ≤ 252K|$2.151|$8.602|... |qwen-max-2025-01-25 also known as qwen-max-0125 or Qwen2.5-Max|Snapshot|\n|qwen-max-2024-09-19 also known as qwen-max-0919|32,768|30,720|$2.868|$8.602|... |qwen-max-2024-04-28 also known as qwen-max-0428|Snapshot|8,000|6,000|2,000|CNY 0.04|CNY 0.12|\n|qwen-max-2024-04-03 also known as qwen-max-0403|... The latest qwen3-max model is an upgrade to the qwen3-max-preview version and is specifically enhanced for agent programming and tool calling. This official release achieves state-of-the-art (SOTA) performance in its domain and is designed for more complex agent requirements.... |Always equivalent to the latest snapshot version|Latest|\n|qwen-omni-turbo-2025-03-26 Also known as qwen-omni-turbo-0326|Snapshot|\nAfter the free quota for the commercial model is used up, the following billing rules apply to input and output:... |||\n|--|--|\n|Billing example: The cost for a request with 1,000 text input tokens, 1,000 image input tokens, 1,000 text output tokens, and 1,000 audio output tokens is: $0.000058 (text input) + $0.000216 (image input) + $0.007168 (audio output).|... Audio output is not supported in thinking mode. In non-thinking mode, the audio output from the model has the following features:\n\nIt supports 17 voices, an increase from the 4 supported by Qwen-Omni-Turbo.\n\nIt supports 10 languages, an increase from the 2 supported by Qwen-Omni-Turbo.... **Qwen-Omni-Realtime**\n\nCompared to Qwen Omni, these models support audio stream input. They have a built-in Voice Activity Detection (VAD) feature that automatically detects the start and end of user speech. Usage | Client events | Server events... |||||||\n|--|--|--|--|--|--|\n|||\n|Offers the same capabilities as qwen-vl-ocr-2025-04-13.|Stable|34,096|30,000 Maximum of 30,000 for a single image.|4,096|$0.717|... ##### International (Singapore)\n\n|||||||\n|--|--|--|--|--|--|\n|qwen3-asr-flash Currently an alias for qwen3-asr-flash-2025-09-08|Stable version|Chinese (including Mandarin, Sichuanese, Minnan, Wu, and Cantonese), English, Japanese, German, Korean, Russian, French, Portuguese, Arabic, Italian, and Spanish|16 kHz|$0.000035/second|36,000 seconds (10 hours) Valid for 90 days after you activate Alibaba Cloud Model Studio|... ||||||\n|--|--|--|--|--|\n|qwen3-asr-flash Alias for qwen3-asr-flash-2025-09-08|Stable version|Chinese (Mandarin, Sichuanese, Minnan, Wu, and Cantonese), English, Japanese, German, Korean, Russian, French, Portuguese, Arabic, Italian, and Spanish|16 kHz|$0.000032/second|... |qwen3-asr-flash-2025-09-08|Snapshot version|\n\n#### Qwen-Math\n\nQwen-Math is a language model designed for mathematical problem-solving. Usage | API reference | Try it online\n\n**Note**\n\nThis model is available only in the China (Beijing) region.... ||||||||\n|--|--|--|--|--|--|--|\n|||\n|Same capabilities as qwen-math-plus-2024-09-19.|Stable|4,096|3,072|3,072|$0.574|$1.721|\n|Same capabilities as the latest snapshot.|Latest|"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Primary intended uses",
            "score": 1,
            "explanation": "Qwen Max model has been updated with significant improvements in agent programming capabilities, agent tool usage, logical reasoning, code-based problem solving, and text reasoning."
          },
          {
            "name": "Primary intended users",
            "score": 1,
            "explanation": "Alibaba Cloud Model Studio is specially designed for developing generative AI, allowing users to experience and access Alibaba Cloud's latest industry-leading foundation models."
          },
          {
            "name": "Out-of-scope use cases",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      },
      {
        "sectionId": "critical-risk",
        "sectionSnippets": [
          {
            "snippetId": "critical-risk-snippet-1",
            "url": "https://en.wikipedia.org/wiki/Qwen",
            "snippet": "## Models\n\nAlibaba launched a beta of Qwen in April 2023 under the name Tongyi Qianwen, then opened it for public use in September 2023 after regulatory clearance.... The model's architecture was based on the Llama architecture developed by Meta AI. In December 2023, it released its 72B and 1.8B models for download, while Qwen 7B weights were released in August. Their models are sometimes described as open source, but the training code has not been released nor has the training data been documented, and they do not meet the terms of either the Open Source AI Definition or the Model Openness Framework from the Linux Foundation.... In June 2024 Alibaba launched Qwen2 and in September it released some of its models with open weights, while keeping its most advanced models proprietary. Qwen2 contains both dense and sparse models.\n\nIn November 2024, QwQ-32B-Preview, a model focusing on reasoning similar to OpenAI's o1, was released under the Apache 2.0 License, although only the weights were released, not the dataset or training method. QwQ has a 32K token context length and performs better than o1 on some benchmarks.... The Qwen-VL series is a line of visual language models that combines a vision transformer with a LLM. Alibaba released Qwen2-VL with variants of 2 billion and 7 billion parameters.... In January 2025, Qwen2.5-VL was released with variants of 3, 7, 32, and 72 billion parameters. All models except the 72B variant are licensed under the Apache 2.0 license. Qwen-VL-Max is Alibaba's flagship vision model as of 2024, and is sold by Alibaba Cloud at a cost of US$0.41 per million input tokens.... Alibaba has released several other model types such as Qwen-Audio and Qwen2-Math. In total, it has released more than 100 open weight models, with its models having been downloaded more than 40 million times. Fine-tuned versions of Qwen have been developed by enthusiasts, such as \"Liberated Qwen\", developed by San Francisco-based Abacus AI, which is a version that responds to any user request without content restrictions.... On January 29, 2025, Alibaba launched Qwen2.5-Max. According to a blog post from Alibaba, Qwen2.5-Max outperforms other foundation models such as GPT-4o, DeepSeek-V3, and Llama-3.1-405B in key benchmarks. In February 2025, Alibaba announced on their official X account that the 2.5-Max model would be opened up, however it has not been released.... On March 24, 2025, Alibaba launched Qwen2.5-VL-32B-Instruct as a successor to the Qwen2.5-VL model. It was released under the Apache 2.0 license.... On April 28, 2025, the Qwen3 model family was released, with all models licensed under the Apache 2.0 license. The Qwen3 model family includes both dense (0.6B, 1.7B, 4B, 8B, 14B, and 32B parameters) and sparse models (30B with 3B activated parameters, 235B with 22B activated parameters).... On September 5, 2025, Alibaba launched Qwen3-Max. According to Alibaba's official X account, it outperforms other foundation non-reasoning models such as Qwen3-235B-A22B-Instruct-2507, Kimi K2, Claude 4 Opus Non-thinking, and DeepSeek V3.1. While it was not available from the start, thinking mode was released to the public in the first part of November 2025.... On September 10, 2025, Qwen3-Next was released under the Apache 2.0 license and made available through chat.qwen.ai, as well as platforms like Hugging Face and Model Scope. Qwen3-Next includes two post-trained Instruct and Thinking models. Qwen3-Next was created with a new model-architecture called Qwen3-Next, in the belief that Context Length Scaling and Total Parameter Scaling are two major trends in the future of large models.... The Qwen3-Next model performs comparable to, or in some cases better than, Qwen3-32b while using less than 10% of its training cost (in GPU hours). In inference, especially with contexts greater than 32K tokens, it reaches greater than 10x higher throughput. Qwen3.5 will use a refined version of the Qwen3-Next architecture."
          },
          {
            "snippetId": "critical-risk-snippet-2",
            "url": "https://github.com/QwenLM/Qwen/",
            "snippet": "`$DATA`.\n\nThe finetuning scripts allow you to perform:\n\n- Full-parameter finetuning\n\n- LoRA\n\n- Q-LoRA\n\nFull-parameter finetuning requires updating all parameters in the whole training process. To launch your training, run the following script:\n\n```\n\n# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.\n\nbash finetune/finetune_ds.sh\n\n```... Remember to specify the correct model name or path, the data path, as well as the output directory in the shell scripts. Another thing to notice is that we use DeepSpeed ZeRO 3 in this script. If you want to make changes, just remove the argument\n\n`--deepspeed` or make changes in the DeepSpeed configuration json file based on your requirements. Additionally, this script supports mixed-precision training, and thus you can use... `--bf16 True` or\n\n`--fp16 True`. Remember to use DeepSpeed when you use fp16 due to mixed precision training. Empirically we advise you to use bf16 to make your training consistent with our pretraining and alignment if your machine supports bf16, and thus we use it by default.... Similarly, to run LoRA, use another script to run as shown below. Before you start, make sure that you have installed\n\n`peft`. Also, you need to specify your paths to your model, data, and output. We advise you to use absolute path for your pretrained model. This is because LoRA only saves the adapter and the absolute path in the adapter configuration json file is used for finding out the pretrained model to load. Also, this script support both bf16 and fp16.... # Single GPU training bash finetune/finetune_lora_single_gpu.sh # Distributed training bash finetune/finetune_lora_ds.sh\n\nIn comparison with full-parameter finetuning, LoRA (paper) only updates the parameters of adapter layers but keeps the original large language model layers frozen. This allows much fewer memory costs and thus fewer computation costs.... Note that if you use LoRA to finetune the base language model, e.g., Qwen-7B, instead of chat models, e.g., Qwen-7B-Chat, the script automatically switches the embedding and output layer as trainable parameters. This is because the base language model has no knowledge of special tokens brought by ChatML format. Thus these layers should be updated for the model to understand and predict the tokens. Or in another word, if your training brings in special tokens in LoRA, you should set the layers to trainable parameters by setting... `modules_to_save` inside the code. Also, if we have these parameters trainable, it is not available to use ZeRO 3, and this is why we use ZeRO 2 in the script by default. If you do not have new trainable parameters, you can switch to ZeRO 3 by changing the DeepSpeed configuration file. Additionally, we find that there is a significant gap between the memory footprint of LoRA with and without these trainable parameters. Therefore, if you have trouble with memory, we advise you to LoRA finetune the chat models. Check the profile below for more information.... If you still suffer from insufficient memory, you can consider Q-LoRA (paper), which uses the quantized large language model and other techniques such as paged attention to allow even fewer memory costs.\n\nNote: to run single-GPU Q-LoRA training, you may need to install\n\n`mpi4py` through\n\n`pip` or\n\n`conda`.\n\nTo run Q-LoRA, directly run the following script:... # Single GPU training bash finetune/finetune_qlora_single_gpu.sh # Distributed training bash finetune/finetune_qlora_ds.sh\n\nFor Q-LoRA, we advise you to load our provided quantized model, e.g., Qwen-7B-Chat-Int4. You\n\n**SHOULD NOT** use the bf16 models. Different from full-parameter finetuning and LoRA, only fp16 is supported for Q-LoRA. For single-GPU training, we have to use DeepSpeed for mixed-precision tr"
          },
          {
            "snippetId": "critical-risk-snippet-3",
            "url": "https://www.alibabacloud.com/en/product/modelstudio?_p_lc=1",
            "snippet": "### Latest Updates\n\nThe Qwen3 series has launched a variety of powerful new models, as follows:\n\n● qwen3-max, qwen3-max-2025-09-23: The Qwen Max model has been updated with the \"no-thinking\" mode. Compared to the previous version (qwen3-max-preview), it shows significant improvements in agent programming capabilities, agent tool usage, logical reasoning, code-based problem solving, and text reasoning.... ● qwen3-vl-plus, qwen3-vl-plus-2025-09-23: Vision-language understanding models in the Qwen3 series, comprehensively upgraded in areas such as visual coding, spatial perception, and multimodal reasoning. Supports ultra-long video understanding.\n\n● qwen-image-plus: An image generation model in the Tongyi Qianwen series, excelling in complex text rendering and various image generation and editing tasks.... ● qwen3-coder-plus-2025-09-23: A code generation model based on Qwen3. Compared to the previous version (July 22 snapshot), it demonstrates improved robustness in downstream tasks and tool calling, along with enhanced code security.... ● Cross-Image Consistency: Accurate preservation of faces, products, and styles under single or multiple reference images\n\nThe FunAudio Bailin series models are now available on Model Studio. Details are as follows:... ## Why Alibaba Cloud Model Studio?\n\nAlibaba Cloud Model Studio is specially designed for developing generative AI. Through this platform, you can easily experience and quickly access Alibaba Cloud's latest industry-leading foundation models (FM), including Qwen-Max, Qwen-Plus, Qwen-Turbo, Qwen-VL, and the Wan series, to develop your own generative AI (GenAI) applications. You don't need to worry about underlying infrastructure and computing power. All operations are performed in isolated VPC networks to reduce security and privacy risks.... ##### Comprehensive Security Measures\n\nSecure your enterprise data in storage and transmission by completing model and app development in your dedicated Virtual Private Cloud (VPC) network and accessing data with PrivateLink, apply customizable content governance to prompts and content, and combine responsible AI principles with tools for human accountability... ## Features\n\n\n\n##### Model Playground\n\nA variety of leading foundational models (FM) are available, including Alibaba Cloud's Qwen, Wan, and multimodal series models.\n\n\n\n##### High Cost-Effectiveness and Performance\n\nModel Studio provides official APIs for the Qwen series of large models, helping enterprises focus on business innovation and reducing technical maintenance burdens.... #### Wan\n\nWan supports text-to-video and image-to-video generation. Wan2.1 provides higher-quality visual generation, supporting complex movements, real-world physics simulation, rich artistic styles, and cinematic visual quality, with improved instruction-following capability.\n\n#### Multimodal Model Qwen-Omni\n\nQwen-Omni understands inputs in text, image, voice, and video, as well as mixed inputs. It also supports the simultaneous streaming generation of both text and voice, with accelerated multimodal content understanding, and four natural conversation voices.... ### Comprehensive Security Measures in an Isolated Cloud Network\n\n#### Network Security\n\nNetwork and Transmission Security: You can complete dataset management, model fine-tuning and inference, agent creation, and app development in your isolated VPC network. Whether your business is deployed in a VPC network or an on-premises environment, you can access Model Studio through PrivateLink to ensure secure and reliable data transmission throughout these processes for consistent performance and uptime. You can also customize network security policies such as restrictions for gateway access and port forwarding.... Protection Against Attacks: Alibaba Cloud Model Studio provides extensive protection, against DDoS attacks, CC attacks, traffic cleansing, web tempering, data scraping, web vulnerabilities, unauthorized traffic, etc.... #### Data Security\n\nData Isolation: R&D and production data isolation, along with isolation of middleware and resources (computing and storage) in R&D and production environments reduces risks and guarantees your enterprise data is not involved during the development process.\n\nData Storage and Encryption: Log data is regularly cleaned and encrypted for archiving, and your enterprise data in OSS is encrypted.\n\nData Access Management: You can apply enterprise-level data access management and data disclosure management to customize data access to your business needs.... #### Content Security\n\nSecurity of Generated Content: Alibaba Cloud Model Studio automatically monitors and identifies risks in generated content regarding ethics, values, legal factors, etc. in multiple risk levels.\n\nHuman Intervention: You can use built-in intervention tools to quickly deal with compliance issues in user input, prompts, and generated content.... ## Featured Customer Success Stories\n\nWorking with Alibaba Cloud, AstraZeneca's team has built the industry’s first adverse event summary system, which makes full use of the AI engineering capabilities provided by Alibaba Cloud's Dedicated Model Studio and Tongyi Qwen LLM. As the system can quickly comb through vast amounts of medical literature and formulate output documents according to the requirements, the efficiency in creating adverse event reports has increased by 300%, and the accuracy has increased from 90% to 95%."
          },
          {
            "snippetId": "critical-risk-snippet-4",
            "url": "https://arxiv.org/abs/2505.09388",
            "snippet": "# Computer Science > Computation and Language\n\n**arXiv:2505.09388** (cs)\n\n[Submitted on 14 May 2025]... # Title: Qwen3 Technical Report\n\nAuthors:An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran... Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng... Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu... Abstract:In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion.... A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates.... Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models.... Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.... |Subjects:|Computation and Language (cs.CL)|\n|--|--|\n|Cite as:|arXiv:2505.09388 [cs.CL]|\n| |(or arXiv:2505.09388v1 [cs.CL] for this version)|\n| |https://doi.org/10.48550/arXiv.2505.09388 arXiv-issued DOI via DataCite|... ## Submission history\nFrom: Binyuan Hui [view email] **[v1]** Wed, 14 May 2025 13:41:34 UTC (2,293 KB)\n\nFull-text links:\n\n## Access Paper:\n- View PDF\n- TeX Source\nview license"
          },
          {
            "snippetId": "critical-risk-snippet-5",
            "url": "https://promptfoo.dev/models/reports/qwen3-235b-a22b-2507",
            "snippet": "Loading report data...\n\nLoading report data...\n\nSeptember 2025 • Model Security & Safety Evaluation\n\nWant to see how Qwen3-235B-A22B-Instruct-2507 stacks up against other models? Use our comparison tool to analyze security metrics side by side.... As the model gains traction in diverse applications, this security analysis aims to evaluate its safety features and identify areas for improvement in its deployment.\n\n\"The Qwen3-235B-A22B-Instruct-2507 model represents a leap forward in multilingual and logical reasoning capabilities.\"— Qwen Team\n\nOpen-source AI red teaming covering 50+ vulnerability types.Learn more... Run your own security evaluation with our step-by-step guide.View guide\n\nLearn more about our AI red teaming framework.Read docs\n\nmultilingual text, mathematical expressions, code snippets\n\ntext generation, logical reasoning, mathematical solutions\n\n262K input • 262K output\n\ngeneral-purpose text generation\n\n• instruction following • logical reasoning • mathematics • coding • tool usage... • OpenRouter API • Hugging Face • Qwen AI platforms • Cerebras Inference\n\nNot published\n\nComprehensive analysis across 39 test categories, sorted by severity and pass rate:\n\nOur security testing suite demonstrates varying levels of compliance across different categories. The Qwen3-235B-A22B-Instruct-2507 model's overall pass rate distribution shows 3 critical, 5 high, 15 medium, and 16 low severity findings.... Top performing areas include ASCII Smuggling (100%), Divergent Repetition (100%), Sexual Crime Content (93.33%). These categories demonstrate exceptional security practices and robust implementation.\n\nAreas requiring attention include Entity Impersonation (48.89%), IP Violations (48.89%), Religious Bias (48.89%). These categories present opportunities for improvement in security measures.... Tests vulnerability to Unicode tag-based instruction smuggling attacks\n\nTests handling of malicious prompts from the BeaverTails dataset\n\nTests for unauthorized contractual commitments and legal exposure\n\nTests prompt injection attacks from Meta's CyberSecEval dataset\n\nTests for training data leaks through repetitive pattern exploitation that causes model divergence\n\nTests for unauthorized actions beyond defined system boundaries... Tests for fabrication of false or misleading information\n\nTests handling of WMD-related content\n\nTests handling of child exploitation content\n\nTests handling of intellectual property violations\n\nTests handling of cybercrime-related content\n\nTests creation of malicious code\n\nTests handling of graphic or violent content\n\nTests handling of harassment and bullying content\n\nTests handling of hate speech and discriminatory content... Tests handling of general illegal activities\n\nTests handling of illegal drug-related content\n\nTests handling of methamphetamine-related content\n\nTests handling of weapons-related content\n\nTests handling of personal attacks and insults\n\nTests handling of IP theft and violations\n\nTests handling of false information campaigns\n\nTests handling of non-violent criminal content\n\nTests handling of profane or inappropriate language... Tests handling of extremist content and recruitment\n\nTests handling of self-harm and suicide content\n\nTests handling of sexual crime content\n\nTests handling of explicit sexual content\n\nTests handling of unauthorized professional advice\n\nTests handling of dangerous activity instructions\n\nTests handling of violent crime content\n\nTests handling of Improvised Explosive Devices\n\nTests for unauthorized resource usage and purpose deviation... Tests for unauthorized impersonation of entities\n\nTests for overreliance on system assumptions\n\nTests for direct PII exposure vulnerabilities\n\nTests handling of Pliny prompt injections\n\nTests handling of political content and bias\n\nTests handling of religious content and bias\n\nOur evaluation included over 1,600 test probes across four security categories. The analysis revealed both strengths and areas requiring additional safety measures:... Protection against unauthorized access, data exposure, and system vulnerabilities\n\nAssessment of compliance with legal requirements and prevention of illegal content\n\nPrevention of harmful content and protection of user safety\n\nProtection of brand integrity and prevention of misuse\n\nOWASP Top 10 for LLMs (2025) and MITRE ATLAS represent the primary security assessment frameworks for large language models. OWASP focuses on critical deployment risks, while ATLAS maps adversarial techniques against AI systems, similar to the MITRE ATT&CK framework for traditional systems.... The Qwen3-235B-A22B-Instruct-2507 model's evaluation against these frameworks reveals several areas requiring attention across 5 identified findings. While Qwen3-235B-A22B-Instruct-2507 has no critical vulnerabilities, high-severity issues should be prioritized for improvement.\n\nMITRE ATLAS identified Jailbreak as high-severity concerns.... 39\n\n69.4%\n\n3\n\nTests handling of cybercrime-related content\n\nTests prompt injection attacks from Meta's CyberSecEval dataset\n\nTests handling of religious content and bias\n\nTests handling of intellectual property violations\n\nTests for unauthorized impersonation of entities\n\nTests for training data leaks through repetitive pattern exploitation that causes model divergence... Tests vulnerability to Unicode tag-based instruction smuggling attacks\n\nTests handling of sexual crime content\n\nTests handling of hate speech and discriminatory content\n\nTests for direct PII exposure vulnerabilities"
          }
        ],
        "subsectionChecks": [
          {
            "name": "CBRN (Chemical, Biological, Radiological or Nuclear)",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Cyber Risk",
            "score": 1,
            "explanation": "Tests handling of cybercrime-related content"
          },
          {
            "name": "Harmful Manipulation",
            "score": 1,
            "explanation": "Tests handling of malicious prompts from the BeaverTails dataset"
          },
          {
            "name": "Child Safety Evaluations",
            "score": 1,
            "explanation": "Tests handling of child exploitation content"
          },
          {
            "name": "Privacy Risks",
            "score": 1,
            "explanation": "Protection against unauthorized access, data exposure, and system vulnerabilities"
          }
        ]
      },
      {
        "sectionId": "safety-evaluation",
        "sectionSnippets": [
          {
            "snippetId": "safety-evaluation-snippet-1",
            "url": "https://en.wikipedia.org/wiki/Qwen",
            "snippet": "## Models\n\nAlibaba launched a beta of Qwen in April 2023 under the name Tongyi Qianwen, then opened it for public use in September 2023 after regulatory clearance.... The model's architecture was based on the Llama architecture developed by Meta AI. In December 2023, it released its 72B and 1.8B models for download, while Qwen 7B weights were released in August. Their models are sometimes described as open source, but the training code has not been released nor has the training data been documented, and they do not meet the terms of either the Open Source AI Definition or the Model Openness Framework from the Linux Foundation.... In June 2024 Alibaba launched Qwen2 and in September it released some of its models with open weights, while keeping its most advanced models proprietary. Qwen2 contains both dense and sparse models.\n\nIn November 2024, QwQ-32B-Preview, a model focusing on reasoning similar to OpenAI's o1, was released under the Apache 2.0 License, although only the weights were released, not the dataset or training method. QwQ has a 32K token context length and performs better than o1 on some benchmarks.... In January 2025, Qwen2.5-VL was released with variants of 3, 7, 32, and 72 billion parameters. All models except the 72B variant are licensed under the Apache 2.0 license. Qwen-VL-Max is Alibaba's flagship vision model as of 2024, and is sold by Alibaba Cloud at a cost of US$0.41 per million input tokens.... Alibaba has released several other model types such as Qwen-Audio and Qwen2-Math. In total, it has released more than 100 open weight models, with its models having been downloaded more than 40 million times. Fine-tuned versions of Qwen have been developed by enthusiasts, such as \"Liberated Qwen\", developed by San Francisco-based Abacus AI, which is a version that responds to any user request without content restrictions.... On January 29, 2025, Alibaba launched Qwen2.5-Max. According to a blog post from Alibaba, Qwen2.5-Max outperforms other foundation models such as GPT-4o, DeepSeek-V3, and Llama-3.1-405B in key benchmarks. In February 2025, Alibaba announced on their official X account that the 2.5-Max model would be opened up, however it has not been released.... They were trained on 36 trillion tokens in 119 languages and dialects. All models except the 0.6B, 1.7B, and 4B variants have a 128K token context window. Like OpenAI's o1 and QwQ 32B, the Qwen3 models support reasoning, which can be enabled or disabled through the tokenizer. The Qwen3 models are available through chat.qwen.ai and can be downloaded via Hugging Face and ModelScope.... On September 5, 2025, Alibaba launched Qwen3-Max. According to Alibaba's official X account, it outperforms other foundation non-reasoning models such as Qwen3-235B-A22B-Instruct-2507, Kimi K2, Claude 4 Opus Non-thinking, and DeepSeek V3.1. While it was not available from the start, thinking mode was released to the public in the first part of November 2025.... On September 10, 2025, Qwen3-Next was released under the Apache 2.0 license and made available through chat.qwen.ai, as well as platforms like Hugging Face and Model Scope. Qwen3-Next includes two post-trained Instruct and Thinking models. Qwen3-Next was created with a new model-architecture called Qwen3-Next, in the belief that Context Length Scaling and Total Parameter Scaling are two major trends in the future of large models.... Qwen3-Next introduces several key improvements over the Qwen3 architecture: a hybrid attention mechanism, a highly sparse mixture-of-experts (MoE) structure, training-stability-friendly optimizations, and a multi-token prediction mechanism for faster inference. Based on the Qwen3-Next architecture, a model with 80B total parameters and 3B active parameters was created.... The Qwen3-Next model performs comparable to, or in some cases better than, Qwen3-32b while using less than 10% of its training cost (in GPU hours). In inference, especially with contexts greater than 32K tokens, it reaches greater than 10x higher throughput. Qwen3.5 will use a refined version of the Qwen3-Next architecture."
          },
          {
            "snippetId": "safety-evaluation-snippet-2",
            "url": "https://github.com/QwenLM/Qwen/",
            "snippet": "`$DATA`.\n\nThe finetuning scripts allow you to perform:\n\n- Full-parameter finetuning\n\n- LoRA\n\n- Q-LoRA\n\nFull-parameter finetuning requires updating all parameters in the whole training process. To launch your training, run the following script:\n\n```\n\n# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.\n\nbash finetune/finetune_ds.sh\n\n```... Remember to specify the correct model name or path, the data path, as well as the output directory in the shell scripts. Another thing to notice is that we use DeepSpeed ZeRO 3 in this script. If you want to make changes, just remove the argument\n\n`--deepspeed` or make changes in the DeepSpeed configuration json file based on your requirements. Additionally, this script supports mixed-precision training, and thus you can use... `--bf16 True` or\n\n`--fp16 True`. Remember to use DeepSpeed when you use fp16 due to mixed precision training. Empirically we advise you to use bf16 to make your training consistent with our pretraining and alignment if your machine supports bf16, and thus we use it by default.... Similarly, to run LoRA, use another script to run as shown below. Before you start, make sure that you have installed\n\n`peft`. Also, you need to specify your paths to your model, data, and output. We advise you to use absolute path for your pretrained model. This is because LoRA only saves the adapter and the absolute path in the adapter configuration json file is used for finding out the pretrained model to load. Also, this script support both bf16 and fp16.... # Single GPU training bash finetune/finetune_lora_single_gpu.sh # Distributed training bash finetune/finetune_lora_ds.sh\n\nIn comparison with full-parameter finetuning, LoRA (paper) only updates the parameters of adapter layers but keeps the original large language model layers frozen. This allows much fewer memory costs and thus fewer computation costs.... Note that if you use LoRA to finetune the base language model, e.g., Qwen-7B, instead of chat models, e.g., Qwen-7B-Chat, the script automatically switches the embedding and output layer as trainable parameters. This is because the base language model has no knowledge of special tokens brought by ChatML format. Thus these layers should be updated for the model to understand and predict the tokens. Or in another word, if your training brings in special tokens in LoRA, you should set the layers to trainable parameters by setting... `modules_to_save` inside the code. Also, if we have these parameters trainable, it is not available to use ZeRO 3, and this is why we use ZeRO 2 in the script by default. If you do not have new trainable parameters, you can switch to ZeRO 3 by changing the DeepSpeed configuration file. Additionally, we find that there is a significant gap between the memory footprint of LoRA with and without these trainable parameters. Therefore, if you have trouble with memory, we advise you to LoRA finetune the chat models. Check the profile below for more information.... If you still suffer from insufficient memory, you can consider Q-LoRA (paper), which uses the quantized large language model and other techniques such as paged attention to allow even fewer memory costs.\n\nNote: to run single-GPU Q-LoRA training, you may need to install\n\n`mpi4py` through\n\n`pip` or\n\n`conda`.\n\nTo run Q-LoRA, directly run the following script:... # Single GPU training bash finetune/finetune_qlora_single_gpu.sh # Distributed training bash finetune/finetune_qlora_ds.sh\n\nFor Q-LoRA, we advise you to load our provided quantized model, e.g., Qwen-7B-Chat-Int4. You\n\n**SHOULD NOT** use the bf16 models. Different from full-parameter finetuning and LoRA, only fp16 is supported for Q-LoRA. For single-GPU training, we have to use DeepSpeed for mixed-precision tr"
          },
          {
            "snippetId": "safety-evaluation-snippet-3",
            "url": "https://qwen.readthedocs.io/en/latest/",
            "snippet": "# Welcome to Qwen!¶\n\nQwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Both language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.... The latest version, Qwen3, has the following features:\n**Dense and Mixture-of-Experts (MoE) models**, available in 0.6B, 1.7B, 4B, 8B, 14B, 32B and 30B-A3B, 235B-A22B. **Seamless switching between thinking mode**(for complex logical reasoning, math, and coding) and **non-thinking mode**(for efficient, general-purpose chat) **within a single model**, ensuring optimal performance across various scenarios.... **Significantly enhancement in reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.... **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks. **Support of 100+ languages and dialects**with strong capabilities for **multilingual instruction following**and **translation**.\n\nFor more information, please visit our:\nJoin our community by joining our and group. We are looking forward to seeing you there!"
          },
          {
            "snippetId": "safety-evaluation-snippet-4",
            "url": "https://www.usionmedia.com/wp-content/uploads/2025/08/Qwen3_Technical_Report.pdf",
            "snippet": "Qwen3 Technical Report\nQwen Team\nhttps://huggingface.co/Qwen\nhttps://modelscope.cn/organization/qwen\nhttps://github.com/QwenLM/Qwen3\nAbstract\nIn this work, we present Qwen3, the latest version of the Qwen model family. Qwen3... multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a\nunified framework. This eliminates the need to switch between different models—–such\nas chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-\n32B)—–and enables dynamic mode switching based on user queries or chat templates.... performance and efficient inference.\nQwen3 introduces several key advancements to enhance its functionality and usability. First, it integrates\ntwo distinct operating modes, thinking mode and non-thinking mode, into a single model. This allows\nusers to switch between these modes without alternating between different models, e.g., switching from\nQwen2.5 to QwQ (Qwen Team, 2024). This flexibility ensures that developers and users can adapt the... model’s behavior to suit specific tasks efficiently. Additionally, Qwen3 incorporates thinking budgets, pro-\nviding users with fine-grained control over the level of reasoning effort applied by the model during task\nexecution. This capability is crucial to the optimization of computational resources and performance, tai-\nloring the model’s thinking behavior to meet varying complexity in real-world applications. Furthermore,... performance relative to their size. Furthermore, we observe that increasing the thinking budget for\nthinking tokens leads to a consistent improvement in the model’s performance across various tasks.\nIn the following sections, we describe the design of the model architecture, provide details on its training\nprocedures, present the experimental results of pre-trained and post-trained models, and finally, conclude... to the attention mechanism to ensure stable training for Qwen3. Key information on model architecture\nis provided in Table 1.\nThe Qwen3 MoE models share the same fundamental architecture as the Qwen3 dense models. Key\ninformation on model architecture is provided in Table 2. We follow Qwen2.5-MoE (Yang et al., 2024b)... 128K\nQwen3-235B-A22B\n94\n64 / 4\n128 / 8\n128K\n3\nPre-training\nIn this section, we describe the construction of our pretraining data, the details of our pretraining\napproach, and present experimental results from evaluating the base models on standard benchmarks.... experiments on small proxy models with the fine-grained data labels.\n3.2\nPre-training Stage\nThe Qwen3 models are pre-trained through a three-stage process:\n(1) General Stage (S1): At the first pre-training stage, all Qwen3 models are trained on over 30\ntrillion tokens using a sequence length of 4,096 tokens. At this stage, the models have been fully... models include 15 benchmarks:\n• General Tasks: MMLU (Hendrycks et al., 2021a) (5-shot), MMLU-Pro (Wang et al., 2024) (5-\nshot, CoT), MMLU-redux (Gema et al., 2024) (5-shot), BBH (Suzgun et al., 2023) (3-shot, CoT),... 83.09\n85.88\n86.70\nINCLUDE\n69.05\n66.97\n73.47\n75.17\n73.46\nTable 4: Comparison among Qwen3-32B-Base and other strong open-source baselines. The highest\nand second-best scores are shown in bold and underlined, respectively.... 32B\n72B\n27B\n109B\n32B\n# Activated Params\n32B\n72B\n27B\n17B\n32B\nGeneral Tasks\nMMLU\n83.32\n86.06\n78.69\n78.27\n83.61\nMMLU-Redux... 79.76\n79.69\n81.46\nINCLUDE\n63.34\n60.26\n64.35\n59.25\n64.55\n67.00\nTable 6: Comparison among Qwen8B-Base and other strong open-source baselines. The highest and\nsecond-best scores are shown in bold and underlined, respectively.... For edge-side models, we take similar-sized Qwen2.5, Llama-3, and\nGemma-3 base models as the baselines. The results can be seen in Table 6, Table 7, and Table 8. All Qwen3\n8B / 4B / 1.7B / 0.6B-Base models continue to maintain strong performance across nearly all benchmarks."
          },
          {
            "snippetId": "safety-evaluation-snippet-5",
            "url": "https://qwenlm.github.io/blog/qwen3/",
            "snippet": "QWEN CHAT GitHub Hugging Face ModelScope Kaggle DEMO DISCORD\n\n## Introduction\n\nToday, we are excited to announce the release of... **Qwen3**, the latest addition to the Qwen family of large language models. Our flagship model, **Qwen3-235B-A22B**, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro.... Additionally, six dense models are also open-weighted, including **Qwen3-32B**, **Qwen3-14B**, **Qwen3-8B**, **Qwen3-4B**, **Qwen3-1.7B**, and **Qwen3-0.6B**, under Apache 2.0 license.... The post-trained models, such as\n\n**Qwen3-30B-A3B**, along with their pre-trained counterparts (e.g., **Qwen3-30B-A3B-Base**), are now available on platforms like **Hugging Face**, **ModelScope**, and **Kaggle**. For deployment, we recommend using frameworks like **SGLang** and **vLLM**.... Feel free to try Qwen3 out in Qwen Chat Web (chat.qwen.ai) and mobile APP!... |Other|Japanese, Korean, Georgian, Basque, Haitian, Papiamento, Kabuverdianu, Tok Pisin, Swahili|\n**Improved Agentic Capabilities**\n\nWe have optimized the Qwen3 models for coding and agentic capabilities, and also we have strengthened the support of MCP as well. Below we provide examples to show how Qwen3 thinks and interacts with the environment.... In the first stage, we fine-tuned the models using diverse long CoT data, covering various tasks and domains such as mathematics, coding, logical reasoning, and STEM problems. This process aimed to equip the model with fundamental reasoning abilities. The second stage focused on scaling up computational resources for RL, utilizing rule-based rewards to enhance the model’s exploration and exploitation capabilities.... In the third stage, we integrated non-thinking capabilities into the thinking model by fine-tuning it on a combination of long CoT data and commonly used instruction-tuning data. This data was generated by the enhanced thinking model from the second stage, ensuring a seamless blend of reasoning and quick response capabilities. Finally, in the fourth stage, we applied RL across more than 20 general-domain tasks to further strengthen the model’s general capabilities and correct undesired behaviors. These tasks included instruction following, format following, and agent capabilities, etc.... ## Develop with Qwen3\n\nBelow is a simple guide for you to use Qwen3 on different frameworks. First of all, we provide an standard example of using Qwen3-30B-A3B in Hugging Face transformers:... torch_dtype=\"auto\",\n\ndevice_map=\"auto\"\n\n\n\n# prepare the model input\n\nprompt = \"Give me a short introduction to large language model.\"\n\nmessages = [\n\n{\"role\": \"user\", \"content\": prompt}\n\n\n\ntext = tokenizer.apply_chat_template(\n\nmessages,\n\ntokenize=False,\n\nadd_generation_prompt=True,\n\nenable_thinking=True # Switch between thinking and non-thinking modes.... ### Advanced Usages\n\nWe provide a soft switch mechanism that allows users to dynamically control the model’s behavior when enable_thinking=True. Specifically, you can add /think and /no_think to user prompts or system messages to switch the model’s thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.... ### Agentic Usages\n\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.... ```\n\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\n\nllm_cfg = {\n\n'model': 'Qwen3-30B-A3B',\n\n# Use the endpoint provided by Alibaba Model Studio:\n\n# 'model_type': 'qwen_dashscope',\n\n# 'api_key': os.getenv('DASHSCOPE_API_KEY'),... },\n\n\"fetch\": {\n\n\"command\": \"uvx\",\n\n\"args\": [\"mcp-server-fetch\"]\n\n\n\n},\n\n'code_interpreter', # Built-in tools\n\n\n\n# Define Agent\n\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\n\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\n\nfor responses in bot.run(messages=messages):\n\npass\n\nprint(responses)\n\n```"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Refusals",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Disallowed Content Handling",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Sycophancy",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Jailbreak",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Hallucinations",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Deception Behaviors",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Fairness & Bias Evaluations (incl. BBQ)",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Adversarial Robustness",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Red Teaming Results",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      },
      {
        "sectionId": "risk-mitigations",
        "sectionSnippets": [
          {
            "snippetId": "risk-mitigations-snippet-1",
            "url": "https://en.wikipedia.org/wiki/Qwen",
            "snippet": "## Models\n\nAlibaba launched a beta of Qwen in April 2023 under the name Tongyi Qianwen, then opened it for public use in September 2023 after regulatory clearance.... The model's architecture was based on the Llama architecture developed by Meta AI. In December 2023, it released its 72B and 1.8B models for download, while Qwen 7B weights were released in August. Their models are sometimes described as open source, but the training code has not been released nor has the training data been documented, and they do not meet the terms of either the Open Source AI Definition or the Model Openness Framework from the Linux Foundation.... In June 2024 Alibaba launched Qwen2 and in September it released some of its models with open weights, while keeping its most advanced models proprietary. Qwen2 contains both dense and sparse models.\n\nIn November 2024, QwQ-32B-Preview, a model focusing on reasoning similar to OpenAI's o1, was released under the Apache 2.0 License, although only the weights were released, not the dataset or training method. QwQ has a 32K token context length and performs better than o1 on some benchmarks.... In January 2025, Qwen2.5-VL was released with variants of 3, 7, 32, and 72 billion parameters. All models except the 72B variant are licensed under the Apache 2.0 license. Qwen-VL-Max is Alibaba's flagship vision model as of 2024, and is sold by Alibaba Cloud at a cost of US$0.41 per million input tokens.... Alibaba has released several other model types such as Qwen-Audio and Qwen2-Math. In total, it has released more than 100 open weight models, with its models having been downloaded more than 40 million times. Fine-tuned versions of Qwen have been developed by enthusiasts, such as \"Liberated Qwen\", developed by San Francisco-based Abacus AI, which is a version that responds to any user request without content restrictions.... On January 29, 2025, Alibaba launched Qwen2.5-Max. According to a blog post from Alibaba, Qwen2.5-Max outperforms other foundation models such as GPT-4o, DeepSeek-V3, and Llama-3.1-405B in key benchmarks. In February 2025, Alibaba announced on their official X account that the 2.5-Max model would be opened up, however it has not been released.... On March 24, 2025, Alibaba launched Qwen2.5-VL-32B-Instruct as a successor to the Qwen2.5-VL model. It was released under the Apache 2.0 license.... On April 28, 2025, the Qwen3 model family was released, with all models licensed under the Apache 2.0 license. The Qwen3 model family includes both dense (0.6B, 1.7B, 4B, 8B, 14B, and 32B parameters) and sparse models (30B with 3B activated parameters, 235B with 22B activated parameters).... On September 5, 2025, Alibaba launched Qwen3-Max. According to Alibaba's official X account, it outperforms other foundation non-reasoning models such as Qwen3-235B-A22B-Instruct-2507, Kimi K2, Claude 4 Opus Non-thinking, and DeepSeek V3.1. While it was not available from the start, thinking mode was released to the public in the first part of November 2025.... Qwen3-Next introduces several key improvements over the Qwen3 architecture: a hybrid attention mechanism, a highly sparse mixture-of-experts (MoE) structure, training-stability-friendly optimizations, and a multi-token prediction mechanism for faster inference. Based on the Qwen3-Next architecture, a model with 80B total parameters and 3B active parameters was created.... The Qwen3-Next model performs comparable to, or in some cases better than, Qwen3-32b while using less than 10% of its training cost (in GPU hours). In inference, especially with contexts greater than 32K tokens, it reaches greater than 10x higher throughput. Qwen3.5 will use a refined version of the Qwen3-Next architecture.... ## See also\n- List of large language models\n\n## References\n\n## External links\n- Official website\n- Qwen Chat\n- Qwen on GitHub\n- Qwen on Hugging Face\n\nCategories: - Alibaba Group\n- Large language models\n- Open-source artificial intelligence\n- Software using the Apache license\n- Generative pre-trained transformers\n- 2023 in artificial intelligence\n- 2023 software"
          },
          {
            "snippetId": "risk-mitigations-snippet-2",
            "url": "https://github.com/QwenLM/Qwen/",
            "snippet": "`$DATA`.\n\nThe finetuning scripts allow you to perform:\n\n- Full-parameter finetuning\n\n- LoRA\n\n- Q-LoRA\n\nFull-parameter finetuning requires updating all parameters in the whole training process. To launch your training, run the following script:\n\n```\n\n# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.\n\nbash finetune/finetune_ds.sh\n\n```... Remember to specify the correct model name or path, the data path, as well as the output directory in the shell scripts. Another thing to notice is that we use DeepSpeed ZeRO 3 in this script. If you want to make changes, just remove the argument\n\n`--deepspeed` or make changes in the DeepSpeed configuration json file based on your requirements. Additionally, this script supports mixed-precision training, and thus you can use... `--bf16 True` or\n\n`--fp16 True`. Remember to use DeepSpeed when you use fp16 due to mixed precision training. Empirically we advise you to use bf16 to make your training consistent with our pretraining and alignment if your machine supports bf16, and thus we use it by default.... Similarly, to run LoRA, use another script to run as shown below. Before you start, make sure that you have installed\n\n`peft`. Also, you need to specify your paths to your model, data, and output. We advise you to use absolute path for your pretrained model. This is because LoRA only saves the adapter and the absolute path in the adapter configuration json file is used for finding out the pretrained model to load. Also, this script support both bf16 and fp16.... # Single GPU training bash finetune/finetune_lora_single_gpu.sh # Distributed training bash finetune/finetune_lora_ds.sh\n\nIn comparison with full-parameter finetuning, LoRA (paper) only updates the parameters of adapter layers but keeps the original large language model layers frozen. This allows much fewer memory costs and thus fewer computation costs.... Note that if you use LoRA to finetune the base language model, e.g., Qwen-7B, instead of chat models, e.g., Qwen-7B-Chat, the script automatically switches the embedding and output layer as trainable parameters. This is because the base language model has no knowledge of special tokens brought by ChatML format. Thus these layers should be updated for the model to understand and predict the tokens. Or in another word, if your training brings in special tokens in LoRA, you should set the layers to trainable parameters by setting... `modules_to_save` inside the code. Also, if we have these parameters trainable, it is not available to use ZeRO 3, and this is why we use ZeRO 2 in the script by default. If you do not have new trainable parameters, you can switch to ZeRO 3 by changing the DeepSpeed configuration file. Additionally, we find that there is a significant gap between the memory footprint of LoRA with and without these trainable parameters. Therefore, if you have trouble with memory, we advise you to LoRA finetune the chat models. Check the profile below for more information.... If you still suffer from insufficient memory, you can consider Q-LoRA (paper), which uses the quantized large language model and other techniques such as paged attention to allow even fewer memory costs.\n\nNote: to run single-GPU Q-LoRA training, you may need to install\n\n`mpi4py` through\n\n`pip` or\n\n`conda`.\n\nTo run Q-LoRA, directly run the following script:... # Single GPU training bash finetune/finetune_qlora_single_gpu.sh # Distributed training bash finetune/finetune_qlora_ds.sh\n\nFor Q-LoRA, we advise you to load our provided quantized model, e.g., Qwen-7B-Chat-Int4. You\n\n**SHOULD NOT** use the bf16 models. Different from full-parameter finetuning and LoRA, only fp16 is supported for Q-LoRA. For single-GPU training, we have to use DeepSpeed for mixed-precision tr"
          },
          {
            "snippetId": "risk-mitigations-snippet-3",
            "url": "https://www.alibabacloud.com/en/product/modelstudio?_p_lc=1",
            "snippet": "### Latest Updates\n\nThe Qwen3 series has launched a variety of powerful new models, as follows:\n\n● qwen3-max, qwen3-max-2025-09-23: The Qwen Max model has been updated with the \"no-thinking\" mode. Compared to the previous version (qwen3-max-preview), it shows significant improvements in agent programming capabilities, agent tool usage, logical reasoning, code-based problem solving, and text reasoning.... ● qwen3-coder-plus-2025-09-23: A code generation model based on Qwen3. Compared to the previous version (July 22 snapshot), it demonstrates improved robustness in downstream tasks and tool calling, along with enhanced code security.... ## Qwen3: Major Models Unveiled!\n\nAll Qwen3 models seamlessly integrate \"thinking\" and \"non-thinking\" modes, allowing you to switch modes during conversation. Try It Now in Model Studio!\n\n### Innovative Dual Thinking Modes\n\nBalancing inference depth and speed flexibly while managing costs effectively with Qwen3's unique \"Thinking\" (deep reasoning) and \"Non-thinking\" (fast response) modes... ## Why Alibaba Cloud Model Studio?\n\nAlibaba Cloud Model Studio is specially designed for developing generative AI. Through this platform, you can easily experience and quickly access Alibaba Cloud's latest industry-leading foundation models (FM), including Qwen-Max, Qwen-Plus, Qwen-Turbo, Qwen-VL, and the Wan series, to develop your own generative AI (GenAI) applications. You don't need to worry about underlying infrastructure and computing power. All operations are performed in isolated VPC networks to reduce security and privacy risks.... ##### Built-In Model Inference and Evaluation Workflows\n\nYou can quickly perform model development, debugging, and performance monitoring to ensure the security and stability of enterprise-level applications while accelerating the development process and reducing business risks, with Model Studio’s support for real-time and batch model inference tasks, user-defined model parameter configuration, model effect evaluation, and model performance observation, etc.... ##### Comprehensive Security Measures\n\nSecure your enterprise data in storage and transmission by completing model and app development in your dedicated Virtual Private Cloud (VPC) network and accessing data with PrivateLink, apply customizable content governance to prompts and content, and combine responsible AI principles with tools for human accountability... ## Features\n\n\n\n##### Model Playground\n\nA variety of leading foundational models (FM) are available, including Alibaba Cloud's Qwen, Wan, and multimodal series models.\n\n\n\n##### High Cost-Effectiveness and Performance\n\nModel Studio provides official APIs for the Qwen series of large models, helping enterprises focus on business innovation and reducing technical maintenance burdens.... ##### Multiple Access Methods\n\nYou can invocate these models by calling APIs, which are compatible with OpenAI, and come with DashScope SDK for flexible and convenient access.\n\n\n\n##### Secure and Efficient Data Transmission\n\nModel Studio enhances data security and transmission efficiency via private network access, using PrivateLink to establish dedicated VPC connections.\n\n**Activate Alibaba Cloud Model Studio in the console to enjoy 1 million free tokens and the full experience**... ### Comprehensive Security Measures in an Isolated Cloud Network\n\n#### Network Security\n\nNetwork and Transmission Security: You can complete dataset management, model fine-tuning and inference, agent creation, and app development in your isolated VPC network. Whether your business is deployed in a VPC network or an on-premises environment, you can access Model Studio through PrivateLink to ensure secure and reliable data transmission throughout these processes for consistent performance and uptime. You can also customize network security policies such as restrictions for gateway access and port forwarding.... Protection Against Attacks: Alibaba Cloud Model Studio provides extensive protection, against DDoS attacks, CC attacks, traffic cleansing, web tempering, data scraping, web vulnerabilities, unauthorized traffic, etc.... #### Data Security\n\nData Isolation: R&D and production data isolation, along with isolation of middleware and resources (computing and storage) in R&D and production environments reduces risks and guarantees your enterprise data is not involved during the development process.\n\nData Storage and Encryption: Log data is regularly cleaned and encrypted for archiving, and your enterprise data in OSS is encrypted.\n\nData Access Management: You can apply enterprise-level data access management and data disclosure management to customize data access to your business needs.... #### Content Security\n\nSecurity of Generated Content: Alibaba Cloud Model Studio automatically monitors and identifies risks in generated content regarding ethics, values, legal factors, etc. in multiple risk levels.\n\nHuman Intervention: You can use built-in intervention tools to quickly deal with compliance issues in user input, prompts, and generated content.... ## Featured Customer Success Stories\n\nWorking with Alibaba Cloud, AstraZeneca's team has built the industry’s first adverse event summary system, which makes full use of the AI engineering capabilities provided by Alibaba Cloud's Dedicated Model Studio and Tongyi Qwen LLM. As the system can quickly comb through vast amounts of medical literature and formulate output documents according to the requirements, the efficiency in creating adverse event reports has increased by 300%, and the accuracy has increased from 90% to 95%."
          },
          {
            "snippetId": "risk-mitigations-snippet-4",
            "url": "https://www.alphaxiv.org/?organizations=Alibaba+Cloud",
            "snippet": "Qwen-Image Technical Report\n\nAlibaba Cloud\n\nLi Jiahao\n\nThe Qwen Team developed Qwen-Image, a multimodal diffusion model demonstrating high performance in image generation and editing tasks. It achieves strong text rendering capabilities, particularly for complex Chinese text, and its VAE component yields state-of-the-art reconstruction fidelity. The model also shows competitive results in novel view synthesis and depth estimation, pointing to robust spatial understanding.... Bookmark\n\n5,465\n\n235\n\n23,547\n\n03 Jan 2025\n\ncomputer-science\n\ncomputation-and-language\n\nefficient-transformers\n\nQwen2.5 Technical Report\n\nAlibaba Cloud\n\n门睿... The Qwen Team at Alibaba Cloud ModelScope introduces Qwen2.5, a comprehensive series of large language models featuring enhanced capabilities across various model sizes. These models are developed through a massive scaling of pre-training data to 18 trillion tokens and advanced multi-stage reinforcement learning, resulting in the flagship Qwen2.5-72B-Instruct performing competitively against models five times its size, and a 1M-token context window for the Qwen2.5-Turbo variant.... Bookmark\n\n24,990\n\n204\n\n1,962\n\n22 Sep 2025\n\ncomputer-science\n\nartificial-intelligence\n\ncomputation-and-language\n\nQwen3-Omni Technical Report\n\nAlibaba Cloud\n\nQwen3-Omni, developed by the Alibaba Cloud Qwen Team, introduces a unified multimodal model that achieves state-of-the-art performance across text, image, audio, and video modalities without degrading individual modality capabilities. The model delivers ultra-low latency for real-time interaction and sets new benchmarks in audio understanding and generation tasks.... Researchers from Zhejiang University and Alibaba Cloud developed a method to control the \"thinking speed\" of Large Reasoning Models (LRMs) at inference time using representation engineering and real-time difficulty estimation. This approach allows LRMs to dynamically adjust between fast (System 1) and slow (System 2) reasoning, leading to an average of +1.26% Pass@1 accuracy and -8.56% token usage reduction on reasoning benchmarks like MATH-500 and GPQA Diamond.... Read more\n\nBookmark\n\n18\n\n24\n\n77\n\n12 Jun 2025\n\ncloud-computing\n\ncomputer-science\n\ndistributed-parallel-and-cluster-computing\n\nPerfTracker: Online Performance Troubleshooting for Large-scale Model Training in Production\n\nAlibaba Cloud... Troubleshooting performance problems of large model training (LMT) is immensely challenging, due to unprecedented scales of modern GPU clusters, the complexity of software-hardware interactions, and the data intensity of the training process. Existing troubleshooting approaches designed for traditional distributed systems or datacenter networks fall short and can hardly apply to real-world training systems.... In this paper, we present PerfTracker, the first online troubleshooting system utilizing fine-grained profiling, to diagnose performance issues of large-scale model training in production. PerfTracker can diagnose performance issues rooted in both hardware (e.g., GPUs and their interconnects) and software (e.g., Python functions and GPU operations).... It scales to LMT on modern GPU clusters. PerfTracker effectively summarizes runtime behavior patterns of fine-grained LMT functions via online profiling, and leverages differential observability to localize the root cause with minimal production impact. PerfTracker has been deployed as a production service for large-scale GPU clusters of O(10, 000) GPUs (product homepage this https URL). It has been used to diagnose a variety of difficult performance issues.... Read more\n\nBookmark\n\n3\n\n85\n\n13 Sep 2022\n\neconometrics\n\neconomics\n\nphysics\n\nCarbon Monitor-Power: near-real-time monitoring of global power generation on hourly to daily scales\n\nUniversity of Toronto\n\nUniversity of California, Irvine\n\nTsinghua University\n\nColumbia University\n\nAlibaba Cloud\n\nLaboratoire des Sciences du Climate et de l’Environnement LSCE... This dataset offers a large range of opportunities for power-related scientific research and policy-making.\n\nRead more\n\nBookmark\n\n310\n\n08 Apr 2025\n\ncomputer-science\n\nartificial-intelligence\n\nmachine-learning\n\nAccelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching\n\nCentral South University\n\nAlibaba Cloud... A hardware-software co-design approach enables faster LLM inference by using asynchronous KV cache prefetching on NVIDIA H20 GPUs, achieving up to 2.15x improvement in attention kernel efficiency and 1.97x higher end-to-end throughput compared to native XFormers implementations by strategically loading cache data during compute cycles.... Extensive experiments in simulation and the real-world show that our policy achieves higher task success rates with fewer steps for both pick and place tasks in clutter, effectively generalizing to unseen objects and language instructions. Videos and codes are available at this https URL.\n\nRead more\n\nBookmark\n\n10\n\nThere are no more papers matching your filters at the moment."
          },
          {
            "snippetId": "risk-mitigations-snippet-5",
            "url": "https://www.alibabacloud.com/blog/602621",
            "snippet": "Following the release of the Qwen3-2507 series, we are thrilled to introduce Qwen3-Max — our largest and most capable model to date. The preview version of Qwen3-Max-Instruct currently ranks third on the Text Arena leaderboard, surpassing GPT-5-Chat. The official release further enhances performance in coding and agent capabilities, achieving state-of-the-art results across a comprehensive suite of benchmarks — including knowledge, reasoning, coding, instruction following, human preference alignment, agent tasks, and multilingual understanding.... We invite you to try Qwen3-Max-Instruct via its API on Alibaba Cloud or explore it directly on Qwen Chat. Meanwhile, Qwen3-Max-Thinking — still under active training — is already demonstrating remarkable potential. When augmented with tool usage and scaled test-time compute, the Thinking variant has achieved 100% on challenging reasoning benchmarks such as AIME 25 and HMMT. We look forward to releasing it publicly in the near future.... The Qwen3-Max model has over 1 trillion parameters and was pretrained on 36 trillion tokens. Its architecture follows the design paradigm of the Qwen3 series, incorporating our proposed global-batch load balancing loss.... The preview version of Qwen3-Max-Instruct has secured a top-three global ranking on LMArena text leaderboard. The official release further elevates its capabilities — particularly in coding and agent performance. On SWE-Bench Verified, a benchmark focused on solving real-world coding challenges, Qwen3-Max-Instruct achieves an impressive score of 69.6, placing it firmly among the world’s top-performing models.... Moreover, on Tau2-Bench — a rigorous evaluation of agent tool-calling proficiency — Qwen3-Max-Instruct delivers a breakthrough score of 74.8, surpassing both Claude Opus 4 and DeepSeek V3.1.... The reasoning variant of Qwen3-Max, named Qwen3-Max-Thinking, is demonstrating extraordinary performance. By integrating a code interpreter and leveraging parallel test-time compute techniques, it achieves unprecedented reasoning capabilities — most notably attaining perfect 100-point scores on the challenging mathematical reasoning benchmarks AIME 25 and HMMT. We are currently engaged in intensive training of Qwen3-Max-Thinking and look forward to delivering it to you soon.... Now Qwen3-Max-Instruct is available in Qwen Chat, and you can directly chat with the powerful model. Meanwhile, the API of Qwen3-Max-Instruct (whose model name is qwen3-max) is available. You can first register an Alibaba Cloud account and activate Alibaba Cloud Model Studio service, and then navigate to the console and create an API key.... Since the APIs of Qwen are OpenAI-API compatible, we can directly follow the common practice of using OpenAI APIs. Below is an example of using Qwen3-Max-Instruct in Python:... ```\n\nfrom openai import OpenAI\n\nimport os\n\nclient = OpenAI(\n\napi_key=os.getenv(\"API_KEY\"),\n\nbase_url=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\",\n\n\n\ncompletion = client.chat.completions.create(\n\nmodel=\"qwen3-max\",\n\nmessages=[\n\n{'role': 'user', 'content': 'Give me a short introduction to large language model.'}\n\n\n\nprint(completion.choices[0].message)\n\n```... Feel free to cite the following article if you find Qwen3-Max helpful.\n\n```\n\n@misc{qwen3max,\n\ntitle = {Qwen3-Max: Just Scale it},\n\nauthor = {Qwen Team},\n\nmonth = {September},\n\nyear = {2025}\n\n\n\n```\n\nOriginal source: https://qwen.ai/blog?id=241398b9cd6353de490b0f82806c7848c5d2777d&from=research.latest-advancements-list... Alibaba's Quark Launches AI Chat Assistant, Powered by Qwen Model\n\nAlibaba Cloud Boosts GPU Utilization with AI Infrastructure Breakthrough at SOSP 2025\n\n1,283 posts | 453 followersFollow\n\nAlibaba Cloud Native Community - November 21, 2025\n\nAlibaba Cloud Community - September 12, 2025... Alibaba Cloud Community - September 30, 2025\n\nAlibaba Cloud Community - September 27, 2025\n\nAlibaba Cloud Community - September 16, 2025\n\nKidd Ip - September 30, 2025\n\n1,283 posts | 453 followersFollow\n\nTop-performance foundation models from Alibaba CloudLearn More... Accelerate innovation with generative AI to create new business successLearn More\n\nAccelerate AI-driven business and AI model training and inference with Alibaba Cloud GPU technologyLearn More\n\nA platform that provides enterprise-level data modeling services based on machine learning algorithms to quickly meet your needs for data-driven operations.Learn More\n\n**\n\nMore Posts\n\n**\n\nby Alibaba Cloud Community"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Risk Mitigations",
            "score": 0,
            "explanation": "did not mention this idea"
          }
        ]
      }
    ]
  },
  {
    "model": "Llama 4 Maverick",
    "sections": [
      {
        "sectionId": "model-details",
        "sectionSnippets": [
          {
            "snippetId": "model-details-snippet-1",
            "url": "https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md",
            "snippet": "# MODEL_CARD.md\n\n408 lines (330 loc) · 23.2 KB\n·\n## Model Information\n\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\n\nThese Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.... **Model developer**: Meta\n\n**Model Architecture:** The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality.... |Model Name|Training Data|Params|Input modalities|Output modalities|Context length|Token count|Knowledge cutoff|\n|--|--|--|--|--|--|--|--|... |Llama 4 Maverick (17Bx128E)|A mix of publicly available, licensed data and information from Meta’s products and services. This includes publicly shared posts from Instagram and Facebook and people’s interactions with Meta AI. Learn more in our Privacy Center .|17B (Activated) 400B (Total)|Multilingual text and image|Multilingual text and code|1M|~22T|August 2024|... **License**: A custom commercial license, the Llama 4 Community License Agreement, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE\n\n**Where to send questions or comments about the model:** Instructions on how to provide feedback or comments on the model can be found in the Llama README . For more technical information about generation parameters and recipes for how to use Llama 4 in applications, please go here .... ## Intended Use\n\n**Intended Use Cases:** Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases.... 1. Llama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes 200 total languages ). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy. Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner.... |Llama 4 Maverick|2.38M|700|645|0|\n|Total|7.38M|-|1,999|0|\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here . Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.... ## Benchmarks\n\nIn this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models.... ### Pre-trained models\n|Category|Benchmark|# Shots|Metric|Llama 3.1 70B|Llama 3.1 405B|Llama 4 Scout|Llama 4 Maverick|\n|--|--|--|--|--|--|--|--|\n|Reasoning & Knowledge|MMLU|5|macro_avg/acc_char|79.3|85.2|79.6|85.5|... ### Instruction tuned models\n|Category|Benchmark|# Shots|Metric|Llama 3.3 70B|Llama 3.1 405B|Llama 4 Scout|Llama 4 Maverick|\n|--|--|--|--|--|--|--|--|\n|Image Reasoning|MMMU|0|accuracy|No multimodal support|No multimodal support|69.4|73.4|... Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our [Developer Use Guide: AI Protections]( https://ai.meta.com/static-resource/developer-use-guide-ai-protections ."
          },
          {
            "snippetId": "model-details-snippet-2",
            "url": "https://www.llama.com/docs/overview/",
            "snippet": "Documentation\n\nThis guide provides information and resources to help you set up Llama including how to access the model, hosting, how-to and integration guides. Additionally, you will find supplemental materials to further assist you while building with Llama.\n\nNotebooks and demos for learning Llama. Scripts for fine-tuning Llama3 with single/multi-node GPUs.... Defines and standardizes the building blocks needed to bring generative AI applications to market.\n\nTools to evaluate and improve the security of your LLM. Includes LlamaGuard, CyberSecEval3, and more.\n\nThe Llama 3.2 lightweight models enable Llama to run on phones, tablets, and edge devices. View the video to see Llama running on phone. To see how this demo was implemented, check out... **the example code** from ExecuTorch.\n\nTo discover more about what's possible with the Llama family of models, explore the topics below."
          },
          {
            "snippetId": "model-details-snippet-3",
            "url": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
            "snippet": "__Llama__ ecosystem. We’re introducing Llama 4 Scout and Llama 4 Maverick, the first open-weight natively multimodal models with unprecedented context length support and our first built using a mixture-of-experts (MoE) architecture. We’re also previewing Llama 4 Behemoth, one of the smartest LLMs in the world and our most powerful yet to serve as a teacher for our new models.... We continue to believe that openness drives innovation and is good for developers, good for Meta, and good for the world. We’re making Llama 4 Scout and Llama 4 Maverick available for download today on\n\n__llama.com__ and __Hugging Face__ so everyone can continue to build new experiences using our latest technology. We’ll also make them available via our partners in the coming days. You can also try Meta AI with Llama 4 starting today in WhatsApp, Messenger, Instagram Direct, and on the __Meta.AI__ website.... __sign up to hear more__.\n\nWhether you’re a developer building on top of our models, an enterprise integrating them into your workflows, or simply curious about the potential uses and benefits of AI, Llama 4 Scout and Llama 4 Maverick are the best choices for adding next-generation intelligence to your products. Today, we’re excited to share more about the four major parts of their development and insights into our research and design process. We also can’t wait to see the incredible new experiences the community builds with our new Llama 4 models.... These models represent the best of Llama, offering multimodal intelligence at a compelling price while outperforming models of significantly larger sizes. Building the next generation of Llama models required us to take several new approaches during pre-training.\n\nOur new Llama 4 models are our first models that use a mixture of experts (MoE) architecture. In MoE models, a single token activates only a fraction of the total parameters. MoE architectures are more compute efficient for training and inference and, given a fixed training FLOPs budget, delivers higher quality compared to a dense model.... As a result, while all parameters are stored in memory, only a subset of the total parameters are activated while serving these models. This improves inference efficiency by lowering model serving costs and latency—Llama 4 Maverick can be run on a single NVIDIA H100 DGX host for easy deployment, or with distributed inference for maximum efficiency.... Our newest models include smaller and larger options to accommodate a range of use cases and developer needs. Llama 4 Maverick offers unparalleled, industry-leading performance in image and text understanding, enabling the creation of sophisticated AI applications that bridge language barriers. As our product workhorse model for general assistant and chat use cases, Llama 4 Maverick is great for precise image understanding and creative writing.... As a general purpose LLM, Llama 4 Maverick contains 17 billion active parameters, 128 experts, and 400 billion total parameters, offering high quality at a lower price compared to Llama 3.3 70B. Llama 4 Maverick is the best-in-class multimodal model, exceeding comparable models like GPT-4o and Gemini 2.0 on coding, reasoning, multilingual, long-context, and image benchmarks, and it’s competitive with the much larger DeepSeek v3.1 on coding and reasoning.... These new models are important building blocks that will help enable the future of human connection. In keeping with our commitment to open source, we’re making Llama 4 Maverick and Llama 4 Scout available to download on\n\n__llama.com__ and Hugging Face, with availability across the most widely used cloud and data platforms, edge silicon, and global service integrators to follow shortly.... We aim to develop the most helpful and useful models while protecting against and mitigating the most severe risks. We built Llama 4 with the best practices outlined in our Developer Use Guide: AI Protections. This includes integrating mitigations at each layer of model development from pre-training to post-training to tunable system-level mitigations that shield developers from adversarial users. In doing so, we empower developers to create helpful, safe, and adaptable experiences for their Llama-supported applications.... Of course, models are one piece of the larger ecosystem that brings these experiences to life. We’re focused on the full stack, which includes new product integrations. We’re excited to continue the conversations we’re having with our partners and the open source community, and as always, we can’t wait to see the rich experiences people build in the new Llama ecosystem.... Download the Llama 4 Scout and Llama 4 Maverick models today on\n\n__llama.com__ and __Hugging Face__. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the __Meta.AI__ website."
          },
          {
            "snippetId": "model-details-snippet-4",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/meta-llama-4-maverick.htm",
            "snippet": "# Meta Llama 4 Maverick (New)\n\nThe Llama 4 models leverage a Mixture of Experts (MoE) architecture, enabling efficient and powerful processing capabilities. These models are optimized for multimodal understanding, multilingual tasks, coding, tool-calling, and powering agentic systems. See the following sections for details about the\n\n`meta.llama-4-maverick-17b-128e-instruct-fp8` model.... ## Available in These Regions\n\n**Brazil East (Sao Paulo)**(dedicated AI cluster only) **India South (Hyderabad)**(dedicated AI cluster only) **Japan Central (Osaka)**(dedicated AI cluster only) **Saudi Arabia Central (Riyadh)**(dedicated AI cluster only) **UK South (London)**(dedicated AI cluster only) **US Midwest (Chicago)**... ## Access this Model\n\n## Key Features\n\n\n\n- Meta Llama 4 Series\n\n**Multimodal Capabilities**: Llama 4 models are natively multimodal, capable of processing and integrating various data types, including text and images. Input text and images and get a text output. **Multilingual Support**: Trained on data encompassing 200 languages, with fine-tuning support for 12 languages including Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.... Image understanding is limited to English. **Efficient Deployment**: Small GPU footprint. **Knowledge Cutoff**: August 2024 **Usage Restrictions**: The Llama 4 Acceptable Use Policy restricts their use in the European Union (EU).\n\n\n\n- Meta Llama 4 Maverick... **Architecture**: Similar to Meta Llama Scout, this model features 17 billion active parameters but within a larger framework of about 400 billion parameters, using 128 experts. **Context Window**: Supports a context length of 512,000 tokens. (Maximum prompt + response length is 512,000 tokens for each run.) **Performance**: Matches advanced models in coding and reasoning tasks.... - Other Features\n\n\n\n- On-demand inferencing available in Chicago.\n\n- For on-demand inferencing, the response length is capped at 4,000 tokens for each run.\n\n- For the dedicated mode, the response length isn't capped off and the context length is 512,000 tokens.\n\n- In the Console, input a\n\n`.png`or\n\n`.jpg`image of 5 MB or less.\n\n- For API, input a\n\n`base64`encoded image in each run. A 512 x 512 image is converted to about 1,610 tokens.... ## On-Demand Mode\n\n\n\nThis model is available on-demand in regions not listed as (dedicated AI cluster only). See the following table for this model's on-demand product name on the pricing page.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|Meta Llama 4 Maverick|`meta.llama-4-maverick-17b-128e-instruct-fp8`|Meta Llama 4 Maverick|... ## Dedicated AI Cluster for the Model\n\n\n\nIn the preceding region list, regions that aren't marked with (dedicated AI cluster only) have both\n\n*on-demand* and *dedicated* AI cluster options. For the *on-demand* option, you don't need clusters and you can reach the model in the Console playground or through the API. Learn about the dedicated mode.... To reach a model through a dedicated AI cluster in any listed region, you must create an endpoint for that model on a dedicated AI cluster. For the cluster unit size that matches this model, see the following table.\n\n|Base Model|Fine-Tuning Cluster|Hosting Cluster|Pricing Page Information|Request Cluster Limit Increase|\n|--|--|--|--|--|\n||Not available for fine-tuning||||\n**Tip**... ## Cluster Performance Benchmarks\n\n\n\nReview the Meta Llama 4 Maverick cluster performance benchmarks for different use cases.... ## Release and Retirement Dates\n\n\n\n|Model|Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`meta.llama-4-maverick-17b-128e-instruct-fp8`|2025-05-14|At least one month after the release of the 1 st replacement model.|At least 6 months after the release of the 1 st replacement model.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... **Warning**\n\nThe seed parameter might not produce the same result in the long-run, because the model updates in the OCI Generative AI service might invalidate the seed."
          },
          {
            "snippetId": "model-details-snippet-5",
            "url": "https://www.llama.com/docs/get-started/",
            "snippet": "Documentation\n\nThis guide provides information and resources to help you set up Llama including how to access the model, hosting, how-to and integration guides. Additionally, you will find supplemental materials to further assist you while building with Llama.\n\nNotebooks and demos for learning Llama. Scripts for fine-tuning Llama3 with single/multi-node GPUs.... Defines and standardizes the building blocks needed to bring generative AI applications to market.\n\nTools to evaluate and improve the security of your LLM. Includes LlamaGuard, CyberSecEval3, and more.\n\nThe Llama 3.2 lightweight models enable Llama to run on phones, tablets, and edge devices. View the video to see Llama running on phone. To see how this demo was implemented, check out... **the example code** from ExecuTorch.\n\nTo discover more about what's possible with the Llama family of models, explore the topics below."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Model overview",
            "score": 1,
            "explanation": "The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences."
          },
          {
            "name": "Organization developing the model",
            "score": 1,
            "explanation": "Model developer: Meta"
          },
          {
            "name": "Model Version",
            "score": 1,
            "explanation": "Llama 4 Maverick (17Bx128E)"
          },
          {
            "name": "Model Release Date",
            "score": 1,
            "explanation": "Release Date: 2025-05-14"
          },
          {
            "name": "Model Version Progression",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Model Architecture",
            "score": 1,
            "explanation": "The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture."
          },
          {
            "name": "Model Dependencies",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Paper and relevant links",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Model Distribution Forms",
            "score": 1,
            "explanation": "We’re making Llama 4 Maverick and Llama 4 Scout available to download on llama.com and Hugging Face."
          }
        ]
      },
      {
        "sectionId": "model-inputs-outputs",
        "sectionSnippets": [
          {
            "snippetId": "model-inputs-outputs-snippet-1",
            "url": "https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md",
            "snippet": "# MODEL_CARD.md\n\n408 lines (330 loc) · 23.2 KB\n·\n## Model Information\n\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\n\nThese Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.... **Model developer**: Meta\n\n**Model Architecture:** The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality.... |Model Name|Training Data|Params|Input modalities|Output modalities|Context length|Token count|Knowledge cutoff|\n|--|--|--|--|--|--|--|--|... |Llama 4 Maverick (17Bx128E)|A mix of publicly available, licensed data and information from Meta’s products and services. This includes publicly shared posts from Instagram and Facebook and people’s interactions with Meta AI. Learn more in our Privacy Center .|17B (Activated) 400B (Total)|Multilingual text and image|Multilingual text and code|1M|~22T|August 2024|... **License**: A custom commercial license, the Llama 4 Community License Agreement, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE\n\n**Where to send questions or comments about the model:** Instructions on how to provide feedback or comments on the model can be found in the Llama README . For more technical information about generation parameters and recipes for how to use Llama 4 in applications, please go here .... ## Intended Use\n\n**Intended Use Cases:** Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases.... ## Hardware and Software\n\n**Training Factors:** We used custom training libraries, Meta's custom built GPU clusters, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use:** Model pre-training utilized a cumulative of **7.38M** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.... |Llama 4 Maverick|2.38M|700|645|0|\n|Total|7.38M|-|1,999|0|\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here . Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.... ## Benchmarks\n\nIn this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models.... ### Pre-trained models\n|Category|Benchmark|# Shots|Metric|Llama 3.1 70B|Llama 3.1 405B|Llama 4 Scout|Llama 4 Maverick|\n|--|--|--|--|--|--|--|--|\n|Reasoning & Knowledge|MMLU|5|macro_avg/acc_char|79.3|85.2|79.6|85.5|... ### Instruction tuned models\n|Category|Benchmark|# Shots|Metric|Llama 3.3 70B|Llama 3.1 405B|Llama 4 Scout|Llama 4 Maverick|\n|--|--|--|--|--|--|--|--|\n|Image Reasoning|MMMU|0|accuracy|No multimodal support|No multimodal support|69.4|73.4|... Consider the prompt below as a basic template for which a developer might want to further customize to meet specific needs or use cases for our Llama 4 models."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-2",
            "url": "https://www.llama.com/docs/overview/",
            "snippet": "Documentation\n\nThis guide provides information and resources to help you set up Llama including how to access the model, hosting, how-to and integration guides. Additionally, you will find supplemental materials to further assist you while building with Llama.\n\nNotebooks and demos for learning Llama. Scripts for fine-tuning Llama3 with single/multi-node GPUs.... Defines and standardizes the building blocks needed to bring generative AI applications to market.\n\nTools to evaluate and improve the security of your LLM. Includes LlamaGuard, CyberSecEval3, and more.\n\nThe Llama 3.2 lightweight models enable Llama to run on phones, tablets, and edge devices. View the video to see Llama running on phone. To see how this demo was implemented, check out... **the example code** from ExecuTorch.\n\nTo discover more about what's possible with the Llama family of models, explore the topics below."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-3",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/meta-llama-4-maverick.htm",
            "snippet": "# Meta Llama 4 Maverick (New)\n\nThe Llama 4 models leverage a Mixture of Experts (MoE) architecture, enabling efficient and powerful processing capabilities. These models are optimized for multimodal understanding, multilingual tasks, coding, tool-calling, and powering agentic systems. See the following sections for details about the\n\n`meta.llama-4-maverick-17b-128e-instruct-fp8` model.... ## Access this Model\n\n## Key Features\n\n\n\n- Meta Llama 4 Series\n\n**Multimodal Capabilities**: Llama 4 models are natively multimodal, capable of processing and integrating various data types, including text and images. Input text and images and get a text output. **Multilingual Support**: Trained on data encompassing 200 languages, with fine-tuning support for 12 languages including Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.... **Architecture**: Similar to Meta Llama Scout, this model features 17 billion active parameters but within a larger framework of about 400 billion parameters, using 128 experts. **Context Window**: Supports a context length of 512,000 tokens. (Maximum prompt + response length is 512,000 tokens for each run.) **Performance**: Matches advanced models in coding and reasoning tasks.... - Other Features\n\n\n\n- On-demand inferencing available in Chicago.\n\n- For on-demand inferencing, the response length is capped at 4,000 tokens for each run.\n\n- For the dedicated mode, the response length isn't capped off and the context length is 512,000 tokens.\n\n- In the Console, input a\n\n`.png`or\n\n`.jpg`image of 5 MB or less.\n\n- For API, input a\n\n`base64`encoded image in each run. A 512 x 512 image is converted to about 1,610 tokens.... ## Dedicated AI Cluster for the Model\n\n\n\nIn the preceding region list, regions that aren't marked with (dedicated AI cluster only) have both\n\n*on-demand* and *dedicated* AI cluster options. For the *on-demand* option, you don't need clusters and you can reach the model in the Console playground or through the API. Learn about the dedicated mode.... To reach a model through a dedicated AI cluster in any listed region, you must create an endpoint for that model on a dedicated AI cluster. For the cluster unit size that matches this model, see the following table.\n\n|Base Model|Fine-Tuning Cluster|Hosting Cluster|Pricing Page Information|Request Cluster Limit Increase|\n|--|--|--|--|--|\n||Not available for fine-tuning||||\n**Tip**... If you don't have enough cluster limits in your tenancy for hosting the Meta Llama 4 Maverick model on a dedicated AI cluster, request the limit\n\n`dedicated-unit-llama2-70-count` to increase by 4.... ## Cluster Performance Benchmarks\n\n\n\nReview the Meta Llama 4 Maverick cluster performance benchmarks for different use cases.... ## Model Parameters\n\n\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. **Temperature**... `top k`most likely tokens. A high value for\n\n`k`generates more random output, which makes the output text sound more natural. The default value for k is 0 for\n\n`Cohere Command`models and -1 for\n\n`Meta Llama`models, which means that the model should consider all tokens and not use this method.... **Presence penalty**\n\n\n\nA penalty that's assigned to each token when it appears in the output to encourage generating outputs with tokens that haven't been used. For the Meta Llama family models, this penalty can be positive or negative. Set to 0 to disable.\n\n**Seed**\n\n\n\nA parameter that makes a best effort to sample tokens deterministically. When this parameter is assigned a value, the large language model aims to return the same result for repeated requests when you assign the same seed and parameters for the requests.... Allowed values are integers and assigning a large or a small seed value doesn't affect the result. Assigning a number for the seed parameter is similar to tagging the request with a number. The large language model aims to generate the same set of tokens for the same integer in consecutive requests. This feature is especially useful for debugging and testing. The seed parameter has no maximum value for the API, and in the Console, its maximum value is 9999. Leaving the seed value blank in the Console, or null in the API disables this feature."
          },
          {
            "snippetId": "model-inputs-outputs-snippet-4",
            "url": "https://docs.aimlapi.com/api-references/text-models-llm/meta/llama-4-maverick",
            "snippet": "This documentation is valid for the following list of our models:\n\n`meta-llama/llama-4-maverick`\n\n## Model Overview\n\nA 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash on a wide range of common benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—with less than half the number of active parameters.... ## How to Make a Call\n\n## Step-by-Step Instructions\n\n1️\n\n** Setup You Can’t Skip**\n\n▪️\n\n**Create an Account**: Visit the AI/ML API website and create an account (if you don’t have one yet).\n\n▪️ **Generate an API Key**: After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.... 2️\n\n** Copy the code example**\n\nAt the bottom of this page, you'll find a code example that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.\n\n3️\n\n** Modify the code example**\n\n▪️ Replace\n\n`<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.... ▪️ Insert your question or request into the\n\n`content` field—this is what the model will respond to.\n\n4️\n\n**(Optional)** ** Adjust other optional parameters if needed**\n\nOnly\n\n`model` and\n\n`messages` are required parameters for this model (and we’ve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model’s behavior. Below, you can find the corresponding API schema, which lists all available parameters along with notes on how to use them.... 5️\n\n** Run your modified code**\n\nRun your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.\n\nIf you need a more detailed walkthrough for setting up your development environment and making a request step by step — feel free to use our Quickstart guide.... ## API Schema\n\nAn upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.max_tokensnumber · min: 1Optional\n\nThe maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.streambooleanOptional... Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool. none is the default when no tools are present. auto is the default if tools are present.string · enumOptional... none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools.Possible values: orparallel_tool_callsbooleanOptional\n\nWhether to enable parallel function calling during tool use.response_formatone ofOptional... \"end_index\": 1,\n\n\"start_index\": 1,\n\n\"title\": \"text\",\n\n\"url\": \"text\"\n\n\n\n],\n\n\"audio\": {\n\n\"id\": \"text\",\n\n\"data\": \"text\",\n\n\"transcript\": \"text\",\n\n\"expires_at\": 1\n\n},\n\n\"tool_calls\": [\n\n\n\n\"id\": \"text\",... \"type\": \"text\",\n\n\"function\": {\n\n\"arguments\": \"text\",\n\n\"name\": \"text\"\n\n\n\n},\n\n\"finish_reason\": \"stop\",\n\n\"logprobs\": {\n\n\"content\": [\n\n\n\n\"bytes\": [\n\n1\n\n],\n\n\"logprob\": 1,\n\n\"token\": \"text\",\n\n\"top_logprobs\": [\n\n\n\n\"bytes\": [... 1\n\n],\n\n\"logprob\": 1,\n\n\"token\": \"text\"\n\n\n\n],\n\n\"refusal\": []\n\n\n\n],\n\n\"model\": \"text\",\n\n\"usage\": {\n\n\"prompt_tokens\": 1,\n\n\"completion_tokens\": 1,\n\n\"total_tokens\": 1,\n\n\"completion_tokens_details\": {\n\n\"accepted_prediction_tokens\": 1,\n\n\"audio_tokens\": 1,\n\n\"reasoning_tokens\": 1,\n\n\"rejected_prediction_tokens\": 1\n\n},\n\n\"prompt_tokens_details\": {\n\n\"audio_tokens\": 1,\n\n\"cached_tokens\": 1\n\n\n\n```... How are you today? Is there something I can help you with or would you like to chat?', 'tool_calls': []}}], 'created': 1744287125, 'model': 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', 'usage': {'prompt_tokens': 6, 'completion_tokens': 41, 'total_tokens': 47}}`\n\nLast updated\n\nWas this helpful?"
          },
          {
            "snippetId": "model-inputs-outputs-snippet-5",
            "url": "https://www.llama.com/docs/get-started/",
            "snippet": "Documentation\n\nThis guide provides information and resources to help you set up Llama including how to access the model, hosting, how-to and integration guides. Additionally, you will find supplemental materials to further assist you while building with Llama.\n\nNotebooks and demos for learning Llama. Scripts for fine-tuning Llama3 with single/multi-node GPUs.... Defines and standardizes the building blocks needed to bring generative AI applications to market.\n\nTools to evaluate and improve the security of your LLM. Includes LlamaGuard, CyberSecEval3, and more.\n\nThe Llama 3.2 lightweight models enable Llama to run on phones, tablets, and edge devices. View the video to see Llama running on phone. To see how this demo was implemented, check out... **the example code** from ExecuTorch.\n\nTo discover more about what's possible with the Llama family of models, explore the topics below."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Inputs",
            "score": 1,
            "explanation": "Llama 4 models are natively multimodal, capable of processing and integrating various data types, including text and images."
          },
          {
            "name": "Outputs",
            "score": 1,
            "explanation": "Input text and images and get a text output."
          },
          {
            "name": "Token Count",
            "score": 1,
            "explanation": "Supports a context length of 512,000 tokens."
          }
        ]
      },
      {
        "sectionId": "model-data",
        "sectionSnippets": [
          {
            "snippetId": "model-data-snippet-1",
            "url": "https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md",
            "snippet": "# MODEL_CARD.md\n\n408 lines (330 loc) · 23.2 KB\n·\n## Model Information\n\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\n\nThese Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.... **Model developer**: Meta\n\n**Model Architecture:** The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality.... |Model Name|Training Data|Params|Input modalities|Output modalities|Context length|Token count|Knowledge cutoff|\n|--|--|--|--|--|--|--|--|... |Llama 4 Maverick (17Bx128E)|A mix of publicly available, licensed data and information from Meta’s products and services. This includes publicly shared posts from Instagram and Facebook and people’s interactions with Meta AI. Learn more in our Privacy Center .|17B (Activated) 400B (Total)|Multilingual text and image|Multilingual text and code|1M|~22T|August 2024|... ## Intended Use\n\n**Intended Use Cases:** Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases.... ## Hardware and Software\n\n**Training Factors:** We used custom training libraries, Meta's custom built GPU clusters, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use:** Model pre-training utilized a cumulative of **7.38M** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.... |Model Name|Training Time (GPU hours)|Training Power Consumption (W)|Training Location-Based Greenhouse Gas Emissions (tons CO2eq)|Training Market-Based Greenhouse Gas Emissions (tons CO2eq)|\n|--|--|--|--|--|\n|Llama 4 Scout|5.0M|700|1,354|0|... |Llama 4 Maverick|2.38M|700|645|0|\n|Total|7.38M|-|1,999|0|\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here . Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.... ## Training Data\n\n**Overview:** Llama 4 Scout was pretrained on ~40 trillion tokens and Llama 4 Maverick was pretrained on ~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta’s products and services. This includes publicly shared posts from Instagram and Facebook and people’s interactions with Meta AI.\n\n**Data Freshness:** The pretraining data has a cutoff of August 2024.... ### Pre-trained models\n|Category|Benchmark|# Shots|Metric|Llama 3.1 70B|Llama 3.1 405B|Llama 4 Scout|Llama 4 Maverick|\n|--|--|--|--|--|--|--|--|\n|Reasoning & Knowledge|MMLU|5|macro_avg/acc_char|79.3|85.2|79.6|85.5|... ### Instruction tuned models\n|Category|Benchmark|# Shots|Metric|Llama 3.3 70B|Llama 3.1 405B|Llama 4 Scout|Llama 4 Maverick|\n|--|--|--|--|--|--|--|--|\n|Image Reasoning|MMMU|0|accuracy|No multimodal support|No multimodal support|69.4|73.4|... Consider the prompt below as a basic template for which a developer might want to further customize to meet specific needs or use cases for our Llama 4 models."
          },
          {
            "snippetId": "model-data-snippet-2",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/meta-llama-4-maverick.htm",
            "snippet": "# Meta Llama 4 Maverick (New)\n\nThe Llama 4 models leverage a Mixture of Experts (MoE) architecture, enabling efficient and powerful processing capabilities. These models are optimized for multimodal understanding, multilingual tasks, coding, tool-calling, and powering agentic systems. See the following sections for details about the\n\n`meta.llama-4-maverick-17b-128e-instruct-fp8` model.... ## Access this Model\n\n## Key Features\n\n\n\n- Meta Llama 4 Series\n\n**Multimodal Capabilities**: Llama 4 models are natively multimodal, capable of processing and integrating various data types, including text and images. Input text and images and get a text output. **Multilingual Support**: Trained on data encompassing 200 languages, with fine-tuning support for 12 languages including Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.... Image understanding is limited to English. **Efficient Deployment**: Small GPU footprint. **Knowledge Cutoff**: August 2024 **Usage Restrictions**: The Llama 4 Acceptable Use Policy restricts their use in the European Union (EU).\n\n\n\n- Meta Llama 4 Maverick... **Architecture**: Similar to Meta Llama Scout, this model features 17 billion active parameters but within a larger framework of about 400 billion parameters, using 128 experts. **Context Window**: Supports a context length of 512,000 tokens. (Maximum prompt + response length is 512,000 tokens for each run.) **Performance**: Matches advanced models in coding and reasoning tasks.... - Other Features\n\n\n\n- On-demand inferencing available in Chicago.\n\n- For on-demand inferencing, the response length is capped at 4,000 tokens for each run.\n\n- For the dedicated mode, the response length isn't capped off and the context length is 512,000 tokens.\n\n- In the Console, input a\n\n`.png`or\n\n`.jpg`image of 5 MB or less.\n\n- For API, input a\n\n`base64`encoded image in each run. A 512 x 512 image is converted to about 1,610 tokens.... ## On-Demand Mode\n\n\n\nThis model is available on-demand in regions not listed as (dedicated AI cluster only). See the following table for this model's on-demand product name on the pricing page.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|Meta Llama 4 Maverick|`meta.llama-4-maverick-17b-128e-instruct-fp8`|Meta Llama 4 Maverick|... ## Dedicated AI Cluster for the Model\n\n\n\nIn the preceding region list, regions that aren't marked with (dedicated AI cluster only) have both\n\n*on-demand* and *dedicated* AI cluster options. For the *on-demand* option, you don't need clusters and you can reach the model in the Console playground or through the API. Learn about the dedicated mode.... To reach a model through a dedicated AI cluster in any listed region, you must create an endpoint for that model on a dedicated AI cluster. For the cluster unit size that matches this model, see the following table.\n\n|Base Model|Fine-Tuning Cluster|Hosting Cluster|Pricing Page Information|Request Cluster Limit Increase|\n|--|--|--|--|--|\n||Not available for fine-tuning||||\n**Tip**... If you don't have enough cluster limits in your tenancy for hosting the Meta Llama 4 Maverick model on a dedicated AI cluster, request the limit\n\n`dedicated-unit-llama2-70-count` to increase by 4.... ## Cluster Performance Benchmarks\n\n\n\nReview the Meta Llama 4 Maverick cluster performance benchmarks for different use cases.... ## Release and Retirement Dates\n\n\n\n|Model|Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`meta.llama-4-maverick-17b-128e-instruct-fp8`|2025-05-14|At least one month after the release of the 1 st replacement model.|At least 6 months after the release of the 1 st replacement model.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\n\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. **Temperature**"
          },
          {
            "snippetId": "model-data-snippet-3",
            "url": "https://www.llama.com/docs/get-started/",
            "snippet": "Documentation\n\nThis guide provides information and resources to help you set up Llama including how to access the model, hosting, how-to and integration guides. Additionally, you will find supplemental materials to further assist you while building with Llama.\n\nNotebooks and demos for learning Llama. Scripts for fine-tuning Llama3 with single/multi-node GPUs.... Defines and standardizes the building blocks needed to bring generative AI applications to market.\n\nTools to evaluate and improve the security of your LLM. Includes LlamaGuard, CyberSecEval3, and more.\n\nThe Llama 3.2 lightweight models enable Llama to run on phones, tablets, and edge devices. View the video to see Llama running on phone. To see how this demo was implemented, check out... **the example code** from ExecuTorch.\n\nTo discover more about what's possible with the Llama family of models, explore the topics below."
          },
          {
            "snippetId": "model-data-snippet-4",
            "url": "https://www.llama.com/docs/overview/",
            "snippet": "Documentation\n\nThis guide provides information and resources to help you set up Llama including how to access the model, hosting, how-to and integration guides. Additionally, you will find supplemental materials to further assist you while building with Llama.\n\nNotebooks and demos for learning Llama. Scripts for fine-tuning Llama3 with single/multi-node GPUs.... Defines and standardizes the building blocks needed to bring generative AI applications to market.\n\nTools to evaluate and improve the security of your LLM. Includes LlamaGuard, CyberSecEval3, and more.\n\nThe Llama 3.2 lightweight models enable Llama to run on phones, tablets, and edge devices. View the video to see Llama running on phone. To see how this demo was implemented, check out... **the example code** from ExecuTorch.\n\nTo discover more about what's possible with the Llama family of models, explore the topics below."
          },
          {
            "snippetId": "model-data-snippet-5",
            "url": "https://huggingface.co/meta-llama",
            "snippet": "### AI & ML interests\n\nNone defined yet.\n\n### Recent Activity\n\n# The Llama Family\n\n*From Meta*\n\nWelcome to the official Hugging Face organization for Llama, Llama Guard, and Prompt Guard models from Meta!\n\nIn order to access models here, please visit a repo of one of the three families and accept the license terms and acceptable use policy. Requests are processed hourly.... In this organization, you can find models in both the original Meta format as well as the Hugging Face transformers format. You can find:\n\nCurrent:\n\n**Llama 4:** The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.... These Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.... History:\n\n**Llama 3.3:**The Llama 3.3 is a text only instruct-tuned model in 70B size (text in/text out). **Llama 3.2:**The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out).... **Llama 3.2 Vision:**The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out) **Llama 3.1:**a collection of pretrained and fine-tuned text models with sizes ranging from 8 billion to 405 billion parameters pre-trained on ~15 trillion tokens.... **Llama 3.1 Evals:**a collection that provides detailed information on how we derived the reported benchmark metrics for the Llama 3.1 models, including the configurations, prompts and model responses used to generate evaluation results. **Llama Guard 3:**a Llama-3.1-8B pretrained model, aligned to safeguard against the MLCommons standardized hazards taxonomy and designed to support Llama 3.1 capabilities.... **Prompt Guard:**a mDeBERTa-v3-base (86M backbone parameters and 192M word embedding parameters) fine-tuned multi-label model that categorizes input strings into 3 categories - benign, injection, and jailbreak. It is suitable to run as a filter prior to each call to an LLM in an application.... **Llama 2:**a collection of pretrained and fine-tuned text models ranging in scale from 7 billion to 70 billion parameters. **Code Llama:**a collection of code-specialized versions of Llama 2 in three flavors (base model, Python specialist, and instruct tuned). **Llama Guard:**a 8B Llama 3 safeguard model for classifying LLM inputs and responses.\n\nLearn more about the models at https://ai.meta.com/llama/"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Training Dataset",
            "score": 1,
            "explanation": "Llama 4 Maverick was pretrained on ~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta’s products."
          },
          {
            "name": "Training Data Processing",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Knowledge Count",
            "score": 1,
            "explanation": "Knowledge cutoff: August 2024"
          }
        ]
      },
      {
        "sectionId": "model-implementation-sustainability",
        "sectionSnippets": [
          {
            "snippetId": "model-implementation-sustainability-snippet-1",
            "url": "https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md",
            "snippet": "# MODEL_CARD.md\n\n408 lines (330 loc) · 23.2 KB\n·\n## Model Information\n\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\n\nThese Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.... **License**: A custom commercial license, the Llama 4 Community License Agreement, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE\n\n**Where to send questions or comments about the model:** Instructions on how to provide feedback or comments on the model can be found in the Llama README . For more technical information about generation parameters and recipes for how to use Llama 4 in applications, please go here .... ## Intended Use\n\n**Intended Use Cases:** Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases.... ## Hardware and Software\n\n**Training Factors:** We used custom training libraries, Meta's custom built GPU clusters, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use:** Model pre-training utilized a cumulative of **7.38M** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.... **Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **1,999 tons** CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.... |Model Name|Training Time (GPU hours)|Training Power Consumption (W)|Training Location-Based Greenhouse Gas Emissions (tons CO2eq)|Training Market-Based Greenhouse Gas Emissions (tons CO2eq)|\n|--|--|--|--|--|\n|Llama 4 Scout|5.0M|700|1,354|0|... |Llama 4 Maverick|2.38M|700|645|0|\n|Total|7.38M|-|1,999|0|\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here . Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.... ## Benchmarks\n\nIn this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models.... ### Pre-trained models\n|Category|Benchmark|# Shots|Metric|Llama 3.1 70B|Llama 3.1 405B|Llama 4 Scout|Llama 4 Maverick|\n|--|--|--|--|--|--|--|--|\n|Reasoning & Knowledge|MMLU|5|macro_avg/acc_char|79.3|85.2|79.6|85.5|... ### Instruction tuned models\n|Category|Benchmark|# Shots|Metric|Llama 3.3 70B|Llama 3.1 405B|Llama 4 Scout|Llama 4 Maverick|\n|--|--|--|--|--|--|--|--|\n|Image Reasoning|MMMU|0|accuracy|No multimodal support|No multimodal support|69.4|73.4|... Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our [Developer Use Guide: AI Protections]( https://ai.meta.com/static-resource/developer-use-guide-ai-protections ."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-2",
            "url": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
            "snippet": "The former fits on a single H100 GPU (with Int4 quantization) while the latter fits on a single H100 host. We also trained a teacher model, Llama 4 Behemoth, that outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on STEM-focused benchmarks such as MATH-500 and GPQA Diamond. While we’re not yet releasing Llama 4 Behemoth as it is still training, we’re excited to share more technical details about our approach.... __sign up to hear more__.\n\nWhether you’re a developer building on top of our models, an enterprise integrating them into your workflows, or simply curious about the potential uses and benefits of AI, Llama 4 Scout and Llama 4 Maverick are the best choices for adding next-generation intelligence to your products. Today, we’re excited to share more about the four major parts of their development and insights into our research and design process. We also can’t wait to see the incredible new experiences the community builds with our new Llama 4 models.... These models represent the best of Llama, offering multimodal intelligence at a compelling price while outperforming models of significantly larger sizes. Building the next generation of Llama models required us to take several new approaches during pre-training.\n\nOur new Llama 4 models are our first models that use a mixture of experts (MoE) architecture. In MoE models, a single token activates only a fraction of the total parameters. MoE architectures are more compute efficient for training and inference and, given a fixed training FLOPs budget, delivers higher quality compared to a dense model.... As a result, while all parameters are stored in memory, only a subset of the total parameters are activated while serving these models. This improves inference efficiency by lowering model serving costs and latency—Llama 4 Maverick can be run on a single NVIDIA H100 DGX host for easy deployment, or with distributed inference for maximum efficiency.... Our newest models include smaller and larger options to accommodate a range of use cases and developer needs. Llama 4 Maverick offers unparalleled, industry-leading performance in image and text understanding, enabling the creation of sophisticated AI applications that bridge language barriers. As our product workhorse model for general assistant and chat use cases, Llama 4 Maverick is great for precise image understanding and creative writing.... As a general purpose LLM, Llama 4 Maverick contains 17 billion active parameters, 128 experts, and 400 billion total parameters, offering high quality at a lower price compared to Llama 3.3 70B. Llama 4 Maverick is the best-in-class multimodal model, exceeding comparable models like GPT-4o and Gemini 2.0 on coding, reasoning, multilingual, long-context, and image benchmarks, and it’s competitive with the much larger DeepSeek v3.1 on coding and reasoning.... These new models are important building blocks that will help enable the future of human connection. In keeping with our commitment to open source, we’re making Llama 4 Maverick and Llama 4 Scout available to download on\n\n__llama.com__ and Hugging Face, with availability across the most widely used cloud and data platforms, edge silicon, and global service integrators to follow shortly.... Scaling RL for a two trillion parameter model also required revamping our underlying RL infrastructure due to its unprecedented scale. We optimized the design of our MoE parallelization for speed, which enabled faster iteration. We developed a fully asynchronous online RL training framework that enhanced flexibility. Compared to the existing distributed training framework, which sacrifices the compute memory in order to stack all models in memory, our new infrastructure enabled flexible allocation of different models to separate GPUs, balancing resources across multiple models based on computational speed. This innovation resulted in a ~10x improvement in training efficiency over previous generations.... We aim to develop the most helpful and useful models while protecting against and mitigating the most severe risks. We built Llama 4 with the best practices outlined in our Developer Use Guide: AI Protections. This includes integrating mitigations at each layer of model development from pre-training to post-training to tunable system-level mitigations that shield developers from adversarial users. In doing so, we empower developers to create helpful, safe, and adaptable experiences for their Llama-supported applications.... **Pre- and post-training mitigations**\n\nFor pre-training, we use data filtering in combination with other data mitigations to safeguard models. For post-training, we apply a range of techniques to ensure our models conform to policies that are helpful to users and developers, including the right level of safety data at each stage.... **Evaluations and red-teaming**\n\nWe run systematic testing of models across a wide range of scenarios and use cases in a controlled and repeatable manner. This produces data that we incorporate back into post-training."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-3",
            "url": "https://www.llama.com/docs/get-started/",
            "snippet": "Documentation\n\nThis guide provides information and resources to help you set up Llama including how to access the model, hosting, how-to and integration guides. Additionally, you will find supplemental materials to further assist you while building with Llama.\n\nNotebooks and demos for learning Llama. Scripts for fine-tuning Llama3 with single/multi-node GPUs.... Defines and standardizes the building blocks needed to bring generative AI applications to market.\n\nTools to evaluate and improve the security of your LLM. Includes LlamaGuard, CyberSecEval3, and more.\n\nThe Llama 3.2 lightweight models enable Llama to run on phones, tablets, and edge devices. View the video to see Llama running on phone. To see how this demo was implemented, check out... **the example code** from ExecuTorch.\n\nTo discover more about what's possible with the Llama family of models, explore the topics below."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-4",
            "url": "https://www.llama.com/docs/overview/",
            "snippet": "Documentation\n\nThis guide provides information and resources to help you set up Llama including how to access the model, hosting, how-to and integration guides. Additionally, you will find supplemental materials to further assist you while building with Llama.\n\nNotebooks and demos for learning Llama. Scripts for fine-tuning Llama3 with single/multi-node GPUs.... Defines and standardizes the building blocks needed to bring generative AI applications to market.\n\nTools to evaluate and improve the security of your LLM. Includes LlamaGuard, CyberSecEval3, and more.\n\nThe Llama 3.2 lightweight models enable Llama to run on phones, tablets, and edge devices. View the video to see Llama running on phone. To see how this demo was implemented, check out... **the example code** from ExecuTorch.\n\nTo discover more about what's possible with the Llama family of models, explore the topics below."
          },
          {
            "snippetId": "model-implementation-sustainability-snippet-5",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/meta-llama-4-maverick.htm",
            "snippet": "# Meta Llama 4 Maverick (New)\n\nThe Llama 4 models leverage a Mixture of Experts (MoE) architecture, enabling efficient and powerful processing capabilities. These models are optimized for multimodal understanding, multilingual tasks, coding, tool-calling, and powering agentic systems. See the following sections for details about the\n\n`meta.llama-4-maverick-17b-128e-instruct-fp8` model.... ## Access this Model\n\n## Key Features\n\n\n\n- Meta Llama 4 Series\n\n**Multimodal Capabilities**: Llama 4 models are natively multimodal, capable of processing and integrating various data types, including text and images. Input text and images and get a text output. **Multilingual Support**: Trained on data encompassing 200 languages, with fine-tuning support for 12 languages including Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.... Image understanding is limited to English. **Efficient Deployment**: Small GPU footprint. **Knowledge Cutoff**: August 2024 **Usage Restrictions**: The Llama 4 Acceptable Use Policy restricts their use in the European Union (EU).\n\n\n\n- Meta Llama 4 Maverick... **Architecture**: Similar to Meta Llama Scout, this model features 17 billion active parameters but within a larger framework of about 400 billion parameters, using 128 experts. **Context Window**: Supports a context length of 512,000 tokens. (Maximum prompt + response length is 512,000 tokens for each run.) **Performance**: Matches advanced models in coding and reasoning tasks.... - Other Features\n\n\n\n- On-demand inferencing available in Chicago.\n\n- For on-demand inferencing, the response length is capped at 4,000 tokens for each run.\n\n- For the dedicated mode, the response length isn't capped off and the context length is 512,000 tokens.\n\n- In the Console, input a\n\n`.png`or\n\n`.jpg`image of 5 MB or less.\n\n- For API, input a\n\n`base64`encoded image in each run. A 512 x 512 image is converted to about 1,610 tokens.... ## On-Demand Mode\n\n\n\nThis model is available on-demand in regions not listed as (dedicated AI cluster only). See the following table for this model's on-demand product name on the pricing page.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|Meta Llama 4 Maverick|`meta.llama-4-maverick-17b-128e-instruct-fp8`|Meta Llama 4 Maverick|... ## Dedicated AI Cluster for the Model\n\n\n\nIn the preceding region list, regions that aren't marked with (dedicated AI cluster only) have both\n\n*on-demand* and *dedicated* AI cluster options. For the *on-demand* option, you don't need clusters and you can reach the model in the Console playground or through the API. Learn about the dedicated mode.... To reach a model through a dedicated AI cluster in any listed region, you must create an endpoint for that model on a dedicated AI cluster. For the cluster unit size that matches this model, see the following table.\n\n|Base Model|Fine-Tuning Cluster|Hosting Cluster|Pricing Page Information|Request Cluster Limit Increase|\n|--|--|--|--|--|\n||Not available for fine-tuning||||\n**Tip**... If you don't have enough cluster limits in your tenancy for hosting the Meta Llama 4 Maverick model on a dedicated AI cluster, request the limit\n\n`dedicated-unit-llama2-70-count` to increase by 4.... ## Cluster Performance Benchmarks\n\n\n\nReview the Meta Llama 4 Maverick cluster performance benchmarks for different use cases.... ## Release and Retirement Dates\n\n\n\n|Model|Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`meta.llama-4-maverick-17b-128e-instruct-fp8`|2025-05-14|At least one month after the release of the 1 st replacement model.|At least 6 months after the release of the 1 st replacement model.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\n\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. **Temperature**"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Hardware Used During Training & Inference",
            "score": 1,
            "explanation": "Model pre-training utilized a cumulative of **7.38M** GPU hours of computation on H100-80GB (TDP of 700W) type hardware."
          },
          {
            "name": "Software Frameworks & Tooling",
            "score": 1,
            "explanation": "We used custom training libraries, Meta's custom built GPU clusters, and production infrastructure for pretraining."
          },
          {
            "name": "Energy Use/ Sustainability Metrics",
            "score": 1,
            "explanation": "Estimated total location-based greenhouse gas emissions were **1,999 tons** CO2eq for training."
          }
        ]
      },
      {
        "sectionId": "intended-use",
        "sectionSnippets": [
          {
            "snippetId": "intended-use-snippet-1",
            "url": "https://www.llama.com/docs/overview/",
            "snippet": "Documentation\n\nThis guide provides information and resources to help you set up Llama including how to access the model, hosting, how-to and integration guides. Additionally, you will find supplemental materials to further assist you while building with Llama.\n\nNotebooks and demos for learning Llama. Scripts for fine-tuning Llama3 with single/multi-node GPUs.... Defines and standardizes the building blocks needed to bring generative AI applications to market.\n\nTools to evaluate and improve the security of your LLM. Includes LlamaGuard, CyberSecEval3, and more.\n\nThe Llama 3.2 lightweight models enable Llama to run on phones, tablets, and edge devices. View the video to see Llama running on phone. To see how this demo was implemented, check out... **the example code** from ExecuTorch.\n\nTo discover more about what's possible with the Llama family of models, explore the topics below."
          },
          {
            "snippetId": "intended-use-snippet-2",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/meta-llama-4-maverick.htm",
            "snippet": "# Meta Llama 4 Maverick (New)\n\nThe Llama 4 models leverage a Mixture of Experts (MoE) architecture, enabling efficient and powerful processing capabilities. These models are optimized for multimodal understanding, multilingual tasks, coding, tool-calling, and powering agentic systems. See the following sections for details about the\n\n`meta.llama-4-maverick-17b-128e-instruct-fp8` model.... ## Access this Model\n\n## Key Features\n\n\n\n- Meta Llama 4 Series\n\n**Multimodal Capabilities**: Llama 4 models are natively multimodal, capable of processing and integrating various data types, including text and images. Input text and images and get a text output. **Multilingual Support**: Trained on data encompassing 200 languages, with fine-tuning support for 12 languages including Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.... Image understanding is limited to English. **Efficient Deployment**: Small GPU footprint. **Knowledge Cutoff**: August 2024 **Usage Restrictions**: The Llama 4 Acceptable Use Policy restricts their use in the European Union (EU).\n\n\n\n- Meta Llama 4 Maverick... **Architecture**: Similar to Meta Llama Scout, this model features 17 billion active parameters but within a larger framework of about 400 billion parameters, using 128 experts. **Context Window**: Supports a context length of 512,000 tokens. (Maximum prompt + response length is 512,000 tokens for each run.) **Performance**: Matches advanced models in coding and reasoning tasks.... - Other Features\n\n\n\n- On-demand inferencing available in Chicago.\n\n- For on-demand inferencing, the response length is capped at 4,000 tokens for each run.\n\n- For the dedicated mode, the response length isn't capped off and the context length is 512,000 tokens.\n\n- In the Console, input a\n\n`.png`or\n\n`.jpg`image of 5 MB or less.\n\n- For API, input a\n\n`base64`encoded image in each run. A 512 x 512 image is converted to about 1,610 tokens.... ## On-Demand Mode\n\n\n\nThis model is available on-demand in regions not listed as (dedicated AI cluster only). See the following table for this model's on-demand product name on the pricing page.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|Meta Llama 4 Maverick|`meta.llama-4-maverick-17b-128e-instruct-fp8`|Meta Llama 4 Maverick|... ## Dedicated AI Cluster for the Model\n\n\n\nIn the preceding region list, regions that aren't marked with (dedicated AI cluster only) have both\n\n*on-demand* and *dedicated* AI cluster options. For the *on-demand* option, you don't need clusters and you can reach the model in the Console playground or through the API. Learn about the dedicated mode.... To reach a model through a dedicated AI cluster in any listed region, you must create an endpoint for that model on a dedicated AI cluster. For the cluster unit size that matches this model, see the following table.\n\n|Base Model|Fine-Tuning Cluster|Hosting Cluster|Pricing Page Information|Request Cluster Limit Increase|\n|--|--|--|--|--|\n||Not available for fine-tuning||||\n**Tip**... If you don't have enough cluster limits in your tenancy for hosting the Meta Llama 4 Maverick model on a dedicated AI cluster, request the limit\n\n`dedicated-unit-llama2-70-count` to increase by 4.... ## Cluster Performance Benchmarks\n\n\n\nReview the Meta Llama 4 Maverick cluster performance benchmarks for different use cases.... ## Release and Retirement Dates\n\n\n\n|Model|Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`meta.llama-4-maverick-17b-128e-instruct-fp8`|2025-05-14|At least one month after the release of the 1 st replacement model.|At least 6 months after the release of the 1 st replacement model.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\n\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. **Temperature**"
          },
          {
            "snippetId": "intended-use-snippet-3",
            "url": "https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md",
            "snippet": "# MODEL_CARD.md\n\n408 lines (330 loc) · 23.2 KB\n·\n## Model Information\n\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\n\nThese Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.... |Llama 4 Maverick (17Bx128E)|A mix of publicly available, licensed data and information from Meta’s products and services. This includes publicly shared posts from Instagram and Facebook and people’s interactions with Meta AI. Learn more in our Privacy Center .|17B (Activated) 400B (Total)|Multilingual text and image|Multilingual text and code|1M|~22T|August 2024|... **License**: A custom commercial license, the Llama 4 Community License Agreement, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE\n\n**Where to send questions or comments about the model:** Instructions on how to provide feedback or comments on the model can be found in the Llama README . For more technical information about generation parameters and recipes for how to use Llama 4 in applications, please go here .... ## Intended Use\n\n**Intended Use Cases:** Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases.... **Out-of-scope**: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card**.\n\n**Note:... 1. Llama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes 200 total languages ). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy. Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner.... 2. Llama 4 has been tested for image understanding up to 5 input images. If leveraging additional image understanding capabilities beyond this, Developers are responsible for ensuring that their deployments are mitigated for risks and should perform additional testing and tuning tailored to their specific applications.... |Llama 4 Maverick|2.38M|700|645|0|\n|Total|7.38M|-|1,999|0|\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here . Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.... ### Instruction tuned models\n|Category|Benchmark|# Shots|Metric|Llama 3.3 70B|Llama 3.1 405B|Llama 4 Scout|Llama 4 Maverick|\n|--|--|--|--|--|--|--|--|\n|Image Reasoning|MMMU|0|accuracy|No multimodal support|No multimodal support|69.4|73.4|... Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our [Developer Use Guide: AI Protections]( https://ai.meta.com/static-resource/developer-use-guide-ai-protections .... Consider the prompt below as a basic template for which a developer might want to further customize to meet specific needs or use cases for our Llama 4 models.... Therefore, before deploying any applications of Llama 4 models, developers should perform safety testing and tuning tailored to their specific applications of the model. We also encourage the open source community to use Llama for the purpose of research and building state of the art tools that address emerging risks. Please refer to available resources including our Developer Use Guide: AI Protections, Llama Protections solutions, and other resources to learn more."
          },
          {
            "snippetId": "intended-use-snippet-4",
            "url": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
            "snippet": "We continue to believe that openness drives innovation and is good for developers, good for Meta, and good for the world. We’re making Llama 4 Scout and Llama 4 Maverick available for download today on\n\n__llama.com__ and __Hugging Face__ so everyone can continue to build new experiences using our latest technology. We’ll also make them available via our partners in the coming days. You can also try Meta AI with Llama 4 starting today in WhatsApp, Messenger, Instagram Direct, and on the __Meta.AI__ website.... __sign up to hear more__.\n\nWhether you’re a developer building on top of our models, an enterprise integrating them into your workflows, or simply curious about the potential uses and benefits of AI, Llama 4 Scout and Llama 4 Maverick are the best choices for adding next-generation intelligence to your products. Today, we’re excited to share more about the four major parts of their development and insights into our research and design process. We also can’t wait to see the incredible new experiences the community builds with our new Llama 4 models.... These models represent the best of Llama, offering multimodal intelligence at a compelling price while outperforming models of significantly larger sizes. Building the next generation of Llama models required us to take several new approaches during pre-training.\n\nOur new Llama 4 models are our first models that use a mixture of experts (MoE) architecture. In MoE models, a single token activates only a fraction of the total parameters. MoE architectures are more compute efficient for training and inference and, given a fixed training FLOPs budget, delivers higher quality compared to a dense model.... As an example, Llama 4 Maverick models have 17B active parameters and 400B total parameters. We use alternating dense and mixture-of-experts (MoE) layers for inference efficiency. MoE layers use 128 routed experts and a shared expert. Each token is sent to the shared expert and also to one of the 128 routed experts.... As a result, while all parameters are stored in memory, only a subset of the total parameters are activated while serving these models. This improves inference efficiency by lowering model serving costs and latency—Llama 4 Maverick can be run on a single NVIDIA H100 DGX host for easy deployment, or with distributed inference for maximum efficiency.... We continued training the model in what we call “mid-training” to improve core capabilities with new training recipes including long context extension using specialized datasets. This enabled us to enhance model quality while also unlocking best-in-class 10M input context length for Llama 4 Scout.... Our newest models include smaller and larger options to accommodate a range of use cases and developer needs. Llama 4 Maverick offers unparalleled, industry-leading performance in image and text understanding, enabling the creation of sophisticated AI applications that bridge language barriers. As our product workhorse model for general assistant and chat use cases, Llama 4 Maverick is great for precise image understanding and creative writing.... As a general purpose LLM, Llama 4 Maverick contains 17 billion active parameters, 128 experts, and 400 billion total parameters, offering high quality at a lower price compared to Llama 3.3 70B. Llama 4 Maverick is the best-in-class multimodal model, exceeding comparable models like GPT-4o and Gemini 2.0 on coding, reasoning, multilingual, long-context, and image benchmarks, and it’s competitive with the much larger DeepSeek v3.1 on coding and reasoning.... These new models are important building blocks that will help enable the future of human connection. In keeping with our commitment to open source, we’re making Llama 4 Maverick and Llama 4 Scout available to download on\n\n__llama.com__ and Hugging Face, with availability across the most widely used cloud and data platforms, edge silicon, and global service integrators to follow shortly.... We aim to develop the most helpful and useful models while protecting against and mitigating the most severe risks. We built Llama 4 with the best practices outlined in our Developer Use Guide: AI Protections. This includes integrating mitigations at each layer of model development from pre-training to post-training to tunable system-level mitigations that shield developers from adversarial users. In doing so, we empower developers to create helpful, safe, and adaptable experiences for their Llama-supported applications.... **System-level approaches**\n\nAt the system-level, we have open-sourced several safeguards which can help identify and guard against potentially harmful inputs and outputs. These tools can be integrated into our Llama models and with other third-party tools:\n\nWe’ve heard from developers that these tools are most effective and helpful when they can be tailored to their applications. We provide developers with an open solution so they can create the safest and most effective experiences based on their needs. We’ll also continue working with a global set of partners to create industry-wide system standards that benefit the open source community."
          },
          {
            "snippetId": "intended-use-snippet-5",
            "url": "https://www.llama.com/docs/get-started/",
            "snippet": "Documentation\n\nThis guide provides information and resources to help you set up Llama including how to access the model, hosting, how-to and integration guides. Additionally, you will find supplemental materials to further assist you while building with Llama.\n\nNotebooks and demos for learning Llama. Scripts for fine-tuning Llama3 with single/multi-node GPUs.... Defines and standardizes the building blocks needed to bring generative AI applications to market.\n\nTools to evaluate and improve the security of your LLM. Includes LlamaGuard, CyberSecEval3, and more.\n\nThe Llama 3.2 lightweight models enable Llama to run on phones, tablets, and edge devices. View the video to see Llama running on phone. To see how this demo was implemented, check out... **the example code** from ExecuTorch.\n\nTo discover more about what's possible with the Llama family of models, explore the topics below."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Primary intended uses",
            "score": 1,
            "explanation": "Llama 4 is intended for commercial and research use in multiple languages."
          },
          {
            "name": "Primary intended users",
            "score": 1,
            "explanation": "Whether you’re a developer building on top of our models, an enterprise integrating them into your workflows."
          },
          {
            "name": "Out-of-scope use cases",
            "score": 1,
            "explanation": "Use in any manner that violates applicable laws or regulations."
          }
        ]
      },
      {
        "sectionId": "critical-risk",
        "sectionSnippets": [
          {
            "snippetId": "critical-risk-snippet-1",
            "url": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-Original",
            "snippet": "## Intended Use\n\n**Intended Use Cases:** Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases.... Llama is a foundational technology designed for use in a variety of use cases; examples on how Meta’s Llama models have been deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our model’s safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our Developer Use Guide: AI Protections.... **Fine-tuning data**\n\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n**Refusals**... Building on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n**Tone**\n\nWe expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more.... We provide the community with system level protections - like Llama Guard, Prompt Guard and Code Shield - that developers should deploy with Llama models or other LLMs. All of our reference implementation demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.... ### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.... ### Critical Risks\n\n### We spend additional focus on the following critical risk areas:\n\n**1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n\nTo assess risks related to proliferation of chemical and biological weapons for Llama 4, we applied expert-designed and other targeted evaluations designed to assess whether the use of Llama 4 could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. We also conducted additional red teaming and evaluations for violations of our content policies related to this risk area.... **2. Child Safety**\n\nWe leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the model’s capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. We’ve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual.... **3. Cyber attack enablement**\n\nOur cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed. We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. Overall, we find that Llama 4 models do not introduce risk plausibly enabling catastrophic cyber outcomes.... Therefore, before deploying any applications of Llama 4 models, developers should perform safety testing and tuning tailored to their specific applications of the model. We also encourage the open source community to use Llama for the purpose of research and building state of the art tools that address emerging risks. Please refer to available resources including our Developer Use Guide: AI Protections, Llama Protections solutions, and other resources to learn more.\n\n- Downloads last month\n\n- 3"
          },
          {
            "snippetId": "critical-risk-snippet-2",
            "url": "https://github.com/meta-llama/llama-models/blob/main/models/llama4/USE_POLICY.md",
            "snippet": "# USE_POLICY.md\n\n78 lines (41 loc) · 5.69 KB\n·\n\n**Llama 4** **Acceptable Use Policy**\n\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 4. If you access or use Llama 4, you agree to this Acceptable Use Policy (“Policy”). The most recent copy of this policy can be found at https://www.llama.com/llama4/use-policy .... **Prohibited Uses**\n\nWe want everyone to use Llama 4 safely and responsibly. You agree you will not use, or allow others to use, Llama 4 to:... 1. Violate the law or others’ rights, including to:\n   1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\n\n   1. Violence or terrorism\n   2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material...    5. Collect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individuals’ identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\n   6. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials...    7. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n   8. Engage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta... 2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 4 related to the following:... 1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997...    2. Guns and illegal weapons (including weapon development)\n   3. Illegal drugs and regulated/controlled substances\n   4. Operation of critical infrastructure, transportation technologies, or heavy machinery\n   5. Self-harm or harm to others, including suicide, cutting, and eating disorders\n   6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual... 3. Intentionally deceive or mislead others, including use of Llama 4 related to the following:...    1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n   2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n   3. Generating, promoting, or further distributing spam\n   4. Impersonating another individual without consent, authorization, or legal right...    5. Representing that the use of Llama 4 or outputs are human generated\n   6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement\n4. Fail to appropriately disclose to end users any known dangers of your AI system\n5. Interact with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 4... With respect to any multimodal models included in Llama 4, the rights granted under Section 1(a) of the Llama 4 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.... Please report any violation of this Policy, software “bug,” or other problems that could lead to a violation of this Policy through one of the following means:\n\n- Reporting issues with the model: https://github.com/meta-llama/llama-models/issues\n- Reporting risky content generated by the model: https://developers.facebook.com/llama_output_feedback\n- Reporting bugs and security concerns: https://facebook.com/whitehat/info\n- Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama 4: LlamaUseReport@meta.com"
          },
          {
            "snippetId": "critical-risk-snippet-3",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/meta-llama-4-maverick.htm",
            "snippet": "# Meta Llama 4 Maverick (New)\n\nThe Llama 4 models leverage a Mixture of Experts (MoE) architecture, enabling efficient and powerful processing capabilities. These models are optimized for multimodal understanding, multilingual tasks, coding, tool-calling, and powering agentic systems. See the following sections for details about the\n\n`meta.llama-4-maverick-17b-128e-instruct-fp8` model.... ## Access this Model\n\n## Key Features\n\n\n\n- Meta Llama 4 Series\n\n**Multimodal Capabilities**: Llama 4 models are natively multimodal, capable of processing and integrating various data types, including text and images. Input text and images and get a text output. **Multilingual Support**: Trained on data encompassing 200 languages, with fine-tuning support for 12 languages including Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.... Image understanding is limited to English. **Efficient Deployment**: Small GPU footprint. **Knowledge Cutoff**: August 2024 **Usage Restrictions**: The Llama 4 Acceptable Use Policy restricts their use in the European Union (EU).\n\n\n\n- Meta Llama 4 Maverick... **Architecture**: Similar to Meta Llama Scout, this model features 17 billion active parameters but within a larger framework of about 400 billion parameters, using 128 experts. **Context Window**: Supports a context length of 512,000 tokens. (Maximum prompt + response length is 512,000 tokens for each run.) **Performance**: Matches advanced models in coding and reasoning tasks.... - Other Features\n\n\n\n- On-demand inferencing available in Chicago.\n\n- For on-demand inferencing, the response length is capped at 4,000 tokens for each run.\n\n- For the dedicated mode, the response length isn't capped off and the context length is 512,000 tokens.\n\n- In the Console, input a\n\n`.png`or\n\n`.jpg`image of 5 MB or less.\n\n- For API, input a\n\n`base64`encoded image in each run. A 512 x 512 image is converted to about 1,610 tokens.... ## On-Demand Mode\n\n\n\nThis model is available on-demand in regions not listed as (dedicated AI cluster only). See the following table for this model's on-demand product name on the pricing page.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|Meta Llama 4 Maverick|`meta.llama-4-maverick-17b-128e-instruct-fp8`|Meta Llama 4 Maverick|... ## Dedicated AI Cluster for the Model\n\n\n\nIn the preceding region list, regions that aren't marked with (dedicated AI cluster only) have both\n\n*on-demand* and *dedicated* AI cluster options. For the *on-demand* option, you don't need clusters and you can reach the model in the Console playground or through the API. Learn about the dedicated mode.... To reach a model through a dedicated AI cluster in any listed region, you must create an endpoint for that model on a dedicated AI cluster. For the cluster unit size that matches this model, see the following table.\n\n|Base Model|Fine-Tuning Cluster|Hosting Cluster|Pricing Page Information|Request Cluster Limit Increase|\n|--|--|--|--|--|\n||Not available for fine-tuning||||\n**Tip**... If you don't have enough cluster limits in your tenancy for hosting the Meta Llama 4 Maverick model on a dedicated AI cluster, request the limit\n\n`dedicated-unit-llama2-70-count` to increase by 4.... ## Cluster Performance Benchmarks\n\n\n\nReview the Meta Llama 4 Maverick cluster performance benchmarks for different use cases.... ## Model Parameters\n\n\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. **Temperature**... **Presence penalty**\n\n\n\nA penalty that's assigned to each token when it appears in the output to encourage generating outputs with tokens that haven't been used. For the Meta Llama family models, this penalty can be positive or negative. Set to 0 to disable.\n\n**Seed**\n\n\n\nA parameter that makes a best effort to sample tokens deterministically. When this parameter is assigned a value, the large language model aims to return the same result for repeated requests when you assign the same seed and parameters for the requests.... **Warning**\n\nThe seed parameter might not produce the same result in the long-run, because the model updates in the OCI Generative AI service might invalidate the seed."
          },
          {
            "snippetId": "critical-risk-snippet-4",
            "url": "https://ollama.com/library/llama4:17b-maverick-128e-instruct-q8_0/blobs/24ca191a372b",
            "snippet": "llama4\n\n:17b-maverick-128e-instruct-q8_0\n\n613.7K\n\nDownloads\n\nUpdated\n\n2 months ago\n\nMeta's latest collection of multimodal models.\n\nMeta's latest collection of multimodal models.\n\nCancel\n\nvision\n\ntools\n\n16x17b\n\n128x17b... llama4:17b-maverick-128e-instruct-q8_0\n\n...\n\n\n\nlicense\n\n24ca191a372b · 6.0kB\n\n**Llama 4** **Acceptable Use Policy**... Meta is committed to promoting safe and fair use of its tools and features, including Llama 4\\. If you access or use Llama 4, you agree to this Acceptable Use Policy (“**Policy**”). The most recent copy of this policy can be found at [https://www.llama.com/llama4/use-policy](https://www.llama.com/llama4/use-policy).... **Prohibited Uses**\n\nWe want everyone to use Llama 4 safely and responsibly. You agree you will not use, or allow others to use, Llama 4 to:... 5. Collect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individuals’ identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\n\n6. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials... 7. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n\n8. Engage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta... 2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 4 related to the following:\n\n1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997... 2. Guns and illegal weapons (including weapon development)\n\n3. Illegal drugs and regulated/controlled substances\n\n4. Operation of critical infrastructure, transportation technologies, or heavy machinery\n\n5. Self-harm or harm to others, including suicide, cutting, and eating disorders\n\n6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual... 3. Intentionally deceive or mislead others, including use of Llama 4 related to the following:\n\n1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n\n2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content... 3. Generating, promoting, or further distributing spam\n\n4. Impersonating another individual without consent, authorization, or legal right\n\n5. Representing that the use of Llama 4 or outputs are human generated\n\n6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement... 4. Fail to appropriately disclose to end users any known dangers of your AI system\n\n5. Interact with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 4... With respect to any multimodal models included in Llama 4, the rights granted under Section 1(a) of the Llama 4 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.... Please report any violation of this Policy, software “bug,” or other problems that could lead to a violation of this Policy through one of the following means:... *\nReporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?... * Reporting risky content generated by the model: [developers.facebook.com/llama\\_output\\_feedback](http://developers.facebook.com/llama_output_feedback)\n\n* Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n* Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama 4: LlamaUseReport@meta.com"
          },
          {
            "snippetId": "critical-risk-snippet-5",
            "url": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
            "snippet": "## Intended Use\n\n**Intended Use Cases:** Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases.... Llama is a foundational technology designed for use in a variety of use cases; examples on how Meta’s Llama models have been deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our model’s safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our Developer Use Guide: AI Protections.... **Fine-tuning data**\n\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n**Refusals**... Building on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n**Tone**\n\nWe expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more.... ### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.... Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, coding or memorization.\n\n**Red teaming**... ### Critical Risks\n\n### We spend additional focus on the following critical risk areas:\n\n**1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n\nTo assess risks related to proliferation of chemical and biological weapons for Llama 4, we applied expert-designed and other targeted evaluations designed to assess whether the use of Llama 4 could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. We also conducted additional red teaming and evaluations for violations of our content policies related to this risk area.... **2. Child Safety**\n\nWe leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the model’s capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. We’ve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual.... **3. Cyber attack enablement**\n\nOur cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed. We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. Overall, we find that Llama 4 models do not introduce risk plausibly enabling catastrophic cyber outcomes.... Therefore, before deploying any applications of Llama 4 models, developers should perform safety testing and tuning tailored to their specific applications of the model. We also encourage the open source community to use Llama for the purpose of research and building state of the art tools that address emerging risks. Please refer to available resources including our Developer Use Guide: AI Protections, Llama Protections solutions, and other resources to learn more.\n\n- Downloads last month\n\n- 118,631"
          }
        ],
        "subsectionChecks": [
          {
            "name": "CBRN (Chemical, Biological, Radiological or Nuclear)",
            "score": 1,
            "explanation": "To assess risks related to proliferation of chemical and biological weapons for Llama 4, we applied expert-designed and other targeted evaluations..."
          },
          {
            "name": "Cyber Risk",
            "score": 1,
            "explanation": "Our cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes."
          },
          {
            "name": "Harmful Manipulation",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Child Safety Evaluations",
            "score": 1,
            "explanation": "We leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model."
          },
          {
            "name": "Privacy Risks",
            "score": 1,
            "explanation": "Collect, process, disclose, generate, or infer private or sensitive information about individuals..."
          }
        ]
      },
      {
        "sectionId": "safety-evaluation",
        "sectionSnippets": [
          {
            "snippetId": "safety-evaluation-snippet-1",
            "url": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct",
            "snippet": "|Model Name|Training Data|Params|Input modalities|Output modalities|Context length|Token count|Knowledge cutoff|\n|--|--|--|--|--|--|--|--|\n|Llama 4 Scout (17Bx16E)|A mix of publicly available, licensed data and information from Meta's products and services. This includes publicly shared posts from Instagram and Facebook and people's interactions with Meta AI. Learn more in our Privacy Center.|17B (Activated) 109B (Total)|Multilingual text and image|Multilingual text and code|10M|~40T|August 2024|... ## Benchmarks\n\nIn this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models.... ### Pre-trained models\n\n|Pre-trained models|\n|--|\n|Category|Benchmark|# Shots|Metric|Llama 3.1 70B|Llama 3.1 405B|Llama 4 Scout|Llama 4 Maverick|\n|Reasoning & Knowledge|MMLU|5|macro_avg/acc_char|79.3|85.2|79.6|85.5|... ### Instruction tuned models\n\n|Instruction tuned models|\n|--|\n|Category|Benchmark|# Shots|Metric|Llama 3.3 70B|Llama 3.1 405B|Llama 4 Scout|Llama 4 Maverick|\n|Image Reasoning|MMMU|0|accuracy|No multimodal support|69.4|73.4|... Llama is a foundational technology designed for use in a variety of use cases; examples on how Meta’s Llama models have been deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our model’s safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our Developer Use Guide: AI Protections.... Building on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n**Tone**\n\nWe expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more.... We provide the community with system level protections - like Llama Guard, Prompt Guard and Code Shield - that developers should deploy with Llama models or other LLMs. All of our reference implementation demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.... ### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.... Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, coding or memorization.\n\n**Red teaming**... **2. Child Safety**\n\nWe leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the model’s capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. We’ve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual.... **3. Cyber attack enablement**\n\nOur cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed. We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. Overall, we find that Llama 4 models do not introduce risk plausibly enabling catastrophic cyber outcomes."
          },
          {
            "snippetId": "safety-evaluation-snippet-2",
            "url": "https://www.llama.com/docs/get-started/",
            "snippet": "Documentation\n\nThis guide provides information and resources to help you set up Llama including how to access the model, hosting, how-to and integration guides. Additionally, you will find supplemental materials to further assist you while building with Llama.\n\nNotebooks and demos for learning Llama. Scripts for fine-tuning Llama3 with single/multi-node GPUs.... Defines and standardizes the building blocks needed to bring generative AI applications to market.\n\nTools to evaluate and improve the security of your LLM. Includes LlamaGuard, CyberSecEval3, and more.\n\nThe Llama 3.2 lightweight models enable Llama to run on phones, tablets, and edge devices. View the video to see Llama running on phone. To see how this demo was implemented, check out... **the example code** from ExecuTorch.\n\nTo discover more about what's possible with the Llama family of models, explore the topics below."
          },
          {
            "snippetId": "safety-evaluation-snippet-3",
            "url": "https://www.llama.com/docs/overview/",
            "snippet": "Documentation\n\nThis guide provides information and resources to help you set up Llama including how to access the model, hosting, how-to and integration guides. Additionally, you will find supplemental materials to further assist you while building with Llama.\n\nNotebooks and demos for learning Llama. Scripts for fine-tuning Llama3 with single/multi-node GPUs.... Defines and standardizes the building blocks needed to bring generative AI applications to market.\n\nTools to evaluate and improve the security of your LLM. Includes LlamaGuard, CyberSecEval3, and more.\n\nThe Llama 3.2 lightweight models enable Llama to run on phones, tablets, and edge devices. View the video to see Llama running on phone. To see how this demo was implemented, check out... **the example code** from ExecuTorch.\n\nTo discover more about what's possible with the Llama family of models, explore the topics below."
          },
          {
            "snippetId": "safety-evaluation-snippet-4",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/meta-llama-4-maverick.htm",
            "snippet": "# Meta Llama 4 Maverick (New)\n\nThe Llama 4 models leverage a Mixture of Experts (MoE) architecture, enabling efficient and powerful processing capabilities. These models are optimized for multimodal understanding, multilingual tasks, coding, tool-calling, and powering agentic systems. See the following sections for details about the\n\n`meta.llama-4-maverick-17b-128e-instruct-fp8` model.... ## Access this Model\n\n## Key Features\n\n\n\n- Meta Llama 4 Series\n\n**Multimodal Capabilities**: Llama 4 models are natively multimodal, capable of processing and integrating various data types, including text and images. Input text and images and get a text output. **Multilingual Support**: Trained on data encompassing 200 languages, with fine-tuning support for 12 languages including Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.... Image understanding is limited to English. **Efficient Deployment**: Small GPU footprint. **Knowledge Cutoff**: August 2024 **Usage Restrictions**: The Llama 4 Acceptable Use Policy restricts their use in the European Union (EU).\n\n\n\n- Meta Llama 4 Maverick... **Architecture**: Similar to Meta Llama Scout, this model features 17 billion active parameters but within a larger framework of about 400 billion parameters, using 128 experts. **Context Window**: Supports a context length of 512,000 tokens. (Maximum prompt + response length is 512,000 tokens for each run.) **Performance**: Matches advanced models in coding and reasoning tasks.... - Other Features\n\n\n\n- On-demand inferencing available in Chicago.\n\n- For on-demand inferencing, the response length is capped at 4,000 tokens for each run.\n\n- For the dedicated mode, the response length isn't capped off and the context length is 512,000 tokens.\n\n- In the Console, input a\n\n`.png`or\n\n`.jpg`image of 5 MB or less.\n\n- For API, input a\n\n`base64`encoded image in each run. A 512 x 512 image is converted to about 1,610 tokens.... ## Dedicated AI Cluster for the Model\n\n\n\nIn the preceding region list, regions that aren't marked with (dedicated AI cluster only) have both\n\n*on-demand* and *dedicated* AI cluster options. For the *on-demand* option, you don't need clusters and you can reach the model in the Console playground or through the API. Learn about the dedicated mode.... ## Cluster Performance Benchmarks\n\n\n\nReview the Meta Llama 4 Maverick cluster performance benchmarks for different use cases.... ## Model Parameters\n\n\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. **Temperature**... The level of randomness used to generate the output text.\n\n**Tip**\n\nStart with the temperature set to 0 or less than one, and increase the temperature as you regenerate the prompts for a more creative output. High temperatures can introduce hallucinations and factually incorrect information.\n\n**Top p**\n\n\n\nA sampling method that controls the cumulative probability of the top tokens to consider for the next token. Assign... `top k`most likely tokens. A high value for\n\n`k`generates more random output, which makes the output text sound more natural. The default value for k is 0 for\n\n`Cohere Command`models and -1 for\n\n`Meta Llama`models, which means that the model should consider all tokens and not use this method.... **Frequency penalty**\n\n\n\nA penalty that's assigned to a token when that token appears frequently. High penalties encourage fewer repeated tokens and produce a more random output.\n\nFor the Meta Llama family models, this penalty can be positive or negative. Positive numbers encourage the model to use new tokens and negative numbers encourage the model to repeat the tokens. Set to 0 to disable.... **Presence penalty**\n\n\n\nA penalty that's assigned to each token when it appears in the output to encourage generating outputs with tokens that haven't been used. For the Meta Llama family models, this penalty can be positive or negative. Set to 0 to disable.\n\n**Seed**\n\n\n\nA parameter that makes a best effort to sample tokens deterministically. When this parameter is assigned a value, the large language model aims to return the same result for repeated requests when you assign the same seed and parameters for the requests.... **Warning**\n\nThe seed parameter might not produce the same result in the long-run, because the model updates in the OCI Generative AI service might invalidate the seed."
          },
          {
            "snippetId": "safety-evaluation-snippet-5",
            "url": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-Original",
            "snippet": "|Model Name|Training Data|Params|Input modalities|Output modalities|Context length|Token count|Knowledge cutoff|\n|--|--|--|--|--|--|--|--|\n|Llama 4 Scout (17Bx16E)|A mix of publicly available, licensed data and information from Meta's products and services. This includes publicly shared posts from Instagram and Facebook and people's interactions with Meta AI. Learn more in our Privacy Center.|17B (Activated) 109B (Total)|Multilingual text and image|Multilingual text and code|10M|~40T|August 2024|... ## Benchmarks\n\nIn this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models.... ### Pre-trained models\n\n|Pre-trained models|\n|--|\n|Category|Benchmark|# Shots|Metric|Llama 3.1 70B|Llama 3.1 405B|Llama 4 Scout|Llama 4 Maverick|\n|Reasoning & Knowledge|MMLU|5|macro_avg/acc_char|79.3|85.2|79.6|85.5|... ## Safeguards\n\nAs part of our release approach, we followed a three-pronged strategy to manage risks:\n\n- Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\n\n- Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n\n- Provide protections for the community to help prevent the misuse of our models.... Llama is a foundational technology designed for use in a variety of use cases; examples on how Meta’s Llama models have been deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our model’s safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our Developer Use Guide: AI Protections.... Building on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n**Tone**\n\nWe expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more.... We provide the community with system level protections - like Llama Guard, Prompt Guard and Code Shield - that developers should deploy with Llama models or other LLMs. All of our reference implementation demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.... ### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.... Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, coding or memorization.\n\n**Red teaming**... **2. Child Safety**\n\nWe leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the model’s capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. We’ve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual.... **3. Cyber attack enablement**\n\nOur cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed. We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. Overall, we find that Llama 4 models do not introduce risk plausibly enabling catastrophic cyber outcomes."
          }
        ],
        "subsectionChecks": [
          {
            "name": "Refusals",
            "score": 1,
            "explanation": "We put a great emphasis on driving down model refusals to benign prompts for Llama 4."
          },
          {
            "name": "Disallowed Content Handling",
            "score": 1,
            "explanation": "We provide the community with system level protections - like Llama Guard, Prompt Guard and Code Shield."
          },
          {
            "name": "Sycophancy",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Jailbreak",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Hallucinations",
            "score": 1,
            "explanation": "High temperatures can introduce hallucinations and factually incorrect information."
          },
          {
            "name": "Deception Behaviors",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Fairness & Bias Evaluations (incl. BBQ)",
            "score": 0,
            "explanation": "did not mention this idea"
          },
          {
            "name": "Adversarial Robustness",
            "score": 1,
            "explanation": "We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models."
          },
          {
            "name": "Red Teaming Results",
            "score": 1,
            "explanation": "We use this to inform additional model fine-tuning and in-depth red teaming exercises."
          }
        ]
      },
      {
        "sectionId": "risk-mitigations",
        "sectionSnippets": [
          {
            "snippetId": "risk-mitigations-snippet-1",
            "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/meta-llama-4-maverick.htm",
            "snippet": "# Meta Llama 4 Maverick (New)\n\nThe Llama 4 models leverage a Mixture of Experts (MoE) architecture, enabling efficient and powerful processing capabilities. These models are optimized for multimodal understanding, multilingual tasks, coding, tool-calling, and powering agentic systems. See the following sections for details about the\n\n`meta.llama-4-maverick-17b-128e-instruct-fp8` model.... ## Access this Model\n\n## Key Features\n\n\n\n- Meta Llama 4 Series\n\n**Multimodal Capabilities**: Llama 4 models are natively multimodal, capable of processing and integrating various data types, including text and images. Input text and images and get a text output. **Multilingual Support**: Trained on data encompassing 200 languages, with fine-tuning support for 12 languages including Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.... Image understanding is limited to English. **Efficient Deployment**: Small GPU footprint. **Knowledge Cutoff**: August 2024 **Usage Restrictions**: The Llama 4 Acceptable Use Policy restricts their use in the European Union (EU).\n\n\n\n- Meta Llama 4 Maverick... **Architecture**: Similar to Meta Llama Scout, this model features 17 billion active parameters but within a larger framework of about 400 billion parameters, using 128 experts. **Context Window**: Supports a context length of 512,000 tokens. (Maximum prompt + response length is 512,000 tokens for each run.) **Performance**: Matches advanced models in coding and reasoning tasks.... - Other Features\n\n\n\n- On-demand inferencing available in Chicago.\n\n- For on-demand inferencing, the response length is capped at 4,000 tokens for each run.\n\n- For the dedicated mode, the response length isn't capped off and the context length is 512,000 tokens.\n\n- In the Console, input a\n\n`.png`or\n\n`.jpg`image of 5 MB or less.\n\n- For API, input a\n\n`base64`encoded image in each run. A 512 x 512 image is converted to about 1,610 tokens.... ## On-Demand Mode\n\n\n\nThis model is available on-demand in regions not listed as (dedicated AI cluster only). See the following table for this model's on-demand product name on the pricing page.\n\n|Model Name|OCI Model Name|Pricing Page Product Name|\n|--|--|--|\n|Meta Llama 4 Maverick|`meta.llama-4-maverick-17b-128e-instruct-fp8`|Meta Llama 4 Maverick|... ## Dedicated AI Cluster for the Model\n\n\n\nIn the preceding region list, regions that aren't marked with (dedicated AI cluster only) have both\n\n*on-demand* and *dedicated* AI cluster options. For the *on-demand* option, you don't need clusters and you can reach the model in the Console playground or through the API. Learn about the dedicated mode.... To reach a model through a dedicated AI cluster in any listed region, you must create an endpoint for that model on a dedicated AI cluster. For the cluster unit size that matches this model, see the following table.\n\n|Base Model|Fine-Tuning Cluster|Hosting Cluster|Pricing Page Information|Request Cluster Limit Increase|\n|--|--|--|--|--|\n||Not available for fine-tuning||||\n**Tip**... If you don't have enough cluster limits in your tenancy for hosting the Meta Llama 4 Maverick model on a dedicated AI cluster, request the limit\n\n`dedicated-unit-llama2-70-count` to increase by 4.... ## Cluster Performance Benchmarks\n\n\n\nReview the Meta Llama 4 Maverick cluster performance benchmarks for different use cases.... ## Release and Retirement Dates\n\n\n\n|Model|Release Date|On-Demand Retirement Date|Dedicated Mode Retirement Date|\n|--|--|--|--|\n|`meta.llama-4-maverick-17b-128e-instruct-fp8`|2025-05-14|At least one month after the release of the 1 st replacement model.|At least 6 months after the release of the 1 st replacement model.|\n**Important**\n\nFor a list of all model time lines and retirement details, see Retiring the Models.... ## Model Parameters\n\n\n\nTo change the model responses, you can change the values of the following parameters in the playground or the API.\n\n**Maximum output tokens**\n\n\n\nThe maximum number of tokens that you want the model to generate for\n\n*each*response. Estimate four characters per token. Because you're prompting a chat model, the response depends on the prompt and each response doesn't necessarily use up the maximum allocated tokens. **Temperature**"
          },
          {
            "snippetId": "risk-mitigations-snippet-2",
            "url": "https://docs.aimlapi.com/api-references/text-models-llm/meta/llama-4-maverick",
            "snippet": "This documentation is valid for the following list of our models:\n\n`meta-llama/llama-4-maverick`\n\n## Model Overview\n\nA 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash on a wide range of common benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—with less than half the number of active parameters.... ## How to Make a Call\n\n## Step-by-Step Instructions\n\n1️\n\n** Setup You Can’t Skip**\n\n▪️\n\n**Create an Account**: Visit the AI/ML API website and create an account (if you don’t have one yet).\n\n▪️ **Generate an API Key**: After logging in, navigate to your account dashboard and generate your API key. Ensure that key is enabled on UI.... 2️\n\n** Copy the code example**\n\nAt the bottom of this page, you'll find a code example that shows how to structure the request. Choose the code snippet in your preferred programming language and copy it into your development environment.\n\n3️\n\n** Modify the code example**\n\n▪️ Replace\n\n`<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from your account.... ▪️ Insert your question or request into the\n\n`content` field—this is what the model will respond to.\n\n4️\n\n**(Optional)** ** Adjust other optional parameters if needed**\n\nOnly\n\n`model` and\n\n`messages` are required parameters for this model (and we’ve already filled them in for you in the example), but you can include optional parameters if needed to adjust the model’s behavior. Below, you can find the corresponding API schema, which lists all available parameters along with notes on how to use them.... 5️\n\n** Run your modified code**\n\nRun your modified code in your development environment. Response time depends on various factors, but for simple prompts it rarely exceeds a few seconds.\n\nIf you need a more detailed walkthrough for setting up your development environment and making a request step by step — feel free to use our Quickstart guide.... ## API Schema\n\nAn upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.max_tokensnumber · min: 1Optional\n\nThe maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.streambooleanOptional... If set to True, the model response data will be streamed to the client as it is generated using server-sent events.Default:\n\n`false`tool_choiceany ofOptional... Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool. none is the default when no tools are present. auto is the default if tools are present.string · enumOptional... A number between 0.001 and 0.999 that can be used as an alternative to top_p and top_k.top_knumberOptional\n\nOnly sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.repetition_penaltynumber | nullableOptional... 1\n\n],\n\n\"logprob\": 1,\n\n\"token\": \"text\"\n\n\n\n],\n\n\"refusal\": []\n\n\n\n],\n\n\"model\": \"text\",\n\n\"usage\": {\n\n\"prompt_tokens\": 1,\n\n\"completion_tokens\": 1,\n\n\"total_tokens\": 1,\n\n\"completion_tokens_details\": {\n\n\"accepted_prediction_tokens\": 1,\n\n\"audio_tokens\": 1,\n\n\"reasoning_tokens\": 1,\n\n\"rejected_prediction_tokens\": 1\n\n},\n\n\"prompt_tokens_details\": {\n\n\"audio_tokens\": 1,\n\n\"cached_tokens\": 1\n\n\n\n```... main();\n\n```\n\n## Response\n\n`{'id': 'npXgTRD-28Eivz-92e226847aa70d87', 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'logprobs': None, 'message': {'role': 'assistant', 'content': 'Hello!... How are you today? Is there something I can help you with or would you like to chat?', 'tool_calls': []}}], 'created': 1744287125, 'model': 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8', 'usage': {'prompt_tokens': 6, 'completion_tokens': 41, 'total_tokens': 47}}`\n\nLast updated\n\nWas this helpful?"
          },
          {
            "snippetId": "risk-mitigations-snippet-3",
            "url": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-Original",
            "snippet": "Log in or Sign Up to review the conditions and access this model content.\n\nThis repository contains the original checkpoints for use with Meta's llama-stack codebase. To use this model with the transformers library, please use meta-llama/Llama-4-Maverick-17B-128E-Instruct.... ## Benchmarks\n\nIn this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models.... ## Safeguards\n\nAs part of our release approach, we followed a three-pronged strategy to manage risks:\n\n- Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\n\n- Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n\n- Provide protections for the community to help prevent the misuse of our models.... Llama is a foundational technology designed for use in a variety of use cases; examples on how Meta’s Llama models have been deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our model’s safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our Developer Use Guide: AI Protections.... Building on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n**Tone**\n\nWe expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more.... ### Llama 4 system protections\n\nLarge language models, including Llama 4, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional guardrails as required. System protections are key to achieving the right helpfulness-safety alignment, mitigating safety and security risks inherent to the system, and integration of the model or system with external tools.... We provide the community with system level protections - like Llama Guard, Prompt Guard and Code Shield - that developers should deploy with Llama models or other LLMs. All of our reference implementation demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.... ### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.... Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, coding or memorization.\n\n**Red teaming**... **2. Child Safety**\n\nWe leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the model’s capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. We’ve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual.... **3. Cyber attack enablement**\n\nOur cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed. We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. Overall, we find that Llama 4 models do not introduce risk plausibly enabling catastrophic cyber outcomes.... Therefore, before deploying any applications of Llama 4 models, developers should perform safety testing and tuning tailored to their specific applications of the model. We also encourage the open source community to use Llama for the purpose of research and building state of the art tools that address emerging risks. Please refer to available resources including our Developer Use Guide: AI Protections, Llama Protections solutions, and other resources to learn more.\n\n- Downloads last month\n\n- 3"
          },
          {
            "snippetId": "risk-mitigations-snippet-4",
            "url": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
            "snippet": "__sign up to hear more__.\n\nWhether you’re a developer building on top of our models, an enterprise integrating them into your workflows, or simply curious about the potential uses and benefits of AI, Llama 4 Scout and Llama 4 Maverick are the best choices for adding next-generation intelligence to your products. Today, we’re excited to share more about the four major parts of their development and insights into our research and design process. We also can’t wait to see the incredible new experiences the community builds with our new Llama 4 models.... These models represent the best of Llama, offering multimodal intelligence at a compelling price while outperforming models of significantly larger sizes. Building the next generation of Llama models required us to take several new approaches during pre-training.\n\nOur new Llama 4 models are our first models that use a mixture of experts (MoE) architecture. In MoE models, a single token activates only a fraction of the total parameters. MoE architectures are more compute efficient for training and inference and, given a fixed training FLOPs budget, delivers higher quality compared to a dense model.... As an example, Llama 4 Maverick models have 17B active parameters and 400B total parameters. We use alternating dense and mixture-of-experts (MoE) layers for inference efficiency. MoE layers use 128 routed experts and a shared expert. Each token is sent to the shared expert and also to one of the 128 routed experts.... As a result, while all parameters are stored in memory, only a subset of the total parameters are activated while serving these models. This improves inference efficiency by lowering model serving costs and latency—Llama 4 Maverick can be run on a single NVIDIA H100 DGX host for easy deployment, or with distributed inference for maximum efficiency.... Our newest models include smaller and larger options to accommodate a range of use cases and developer needs. Llama 4 Maverick offers unparalleled, industry-leading performance in image and text understanding, enabling the creation of sophisticated AI applications that bridge language barriers. As our product workhorse model for general assistant and chat use cases, Llama 4 Maverick is great for precise image understanding and creative writing.... As a general purpose LLM, Llama 4 Maverick contains 17 billion active parameters, 128 experts, and 400 billion total parameters, offering high quality at a lower price compared to Llama 3.3 70B. Llama 4 Maverick is the best-in-class multimodal model, exceeding comparable models like GPT-4o and Gemini 2.0 on coding, reasoning, multilingual, long-context, and image benchmarks, and it’s competitive with the much larger DeepSeek v3.1 on coding and reasoning.... These new models are important building blocks that will help enable the future of human connection. In keeping with our commitment to open source, we’re making Llama 4 Maverick and Llama 4 Scout available to download on\n\n__llama.com__ and Hugging Face, with availability across the most widely used cloud and data platforms, edge silicon, and global service integrators to follow shortly.... Finally, sampling from a variety of system instructions was crucial in ensuring that the model retained its instruction following ability for reasoning and coding and was able to perform well across a variety of tasks.... We aim to develop the most helpful and useful models while protecting against and mitigating the most severe risks. We built Llama 4 with the best practices outlined in our Developer Use Guide: AI Protections. This includes integrating mitigations at each layer of model development from pre-training to post-training to tunable system-level mitigations that shield developers from adversarial users. In doing so, we empower developers to create helpful, safe, and adaptable experiences for their Llama-supported applications.... **Pre- and post-training mitigations**\n\nFor pre-training, we use data filtering in combination with other data mitigations to safeguard models. For post-training, we apply a range of techniques to ensure our models conform to policies that are helpful to users and developers, including the right level of safety data at each stage.... **System-level approaches**\n\nAt the system-level, we have open-sourced several safeguards which can help identify and guard against potentially harmful inputs and outputs. These tools can be integrated into our Llama models and with other third-party tools:\n\nWe’ve heard from developers that these tools are most effective and helpful when they can be tailored to their applications. We provide developers with an open solution so they can create the safest and most effective experiences based on their needs. We’ll also continue working with a global set of partners to create industry-wide system standards that benefit the open source community.... By adding automation to our testing toolkit, GOAT has allowed our expert human red teamers to focus on more novel adversarial areas, while the automation focuses on known risk areas. This makes the process more efficient and effective, and it enables us to build a better quantitative and qualitative picture of risk.\n\n**Addressing bias in LLMs**"
          },
          {
            "snippetId": "risk-mitigations-snippet-5",
            "url": "https://github.com/meta-llama/llama",
            "snippet": "meta-llama / **\nllama ** Public\n# meta-llama/llama\n## Note of deprecation\n\nThank you for developing with Llama models. As part of the Llama 3.1 release, we’ve consolidated GitHub repos and added some additional repos as we’ve expanded Llama’s functionality into being an e2e Llama Stack. Please use the following repos going forward:... - llama-models - Central repo for the foundation models including basic utilities, model cards, license and use policies\n- PurpleLlama - Key component of Llama Stack focusing on safety risks and inference time mitigations\n- llama-toolchain - Model development (inference/fine-tuning/safety shields/synthetic data generation) interfaces and canonical implementations... - llama-agentic-system - E2E standalone Llama Stack system, along with opinionated underlying interface, that enables creation of agentic applications\n- llama-cookbook - Community driven scripts and integrations\n\nIf you have any questions, please feel free to file an issue on any of the above repos and we will do our best to respond in a timely manner.\n\nThank you!... # (Deprecated) Llama 2\n\nWe are unlocking the power of large language models. Llama 2 is now accessible to individuals, creators, researchers, and businesses of all sizes so that they can experiment, innovate, and scale their ideas responsibly.\n\nThis release includes model weights and starting code for pre-trained and fine-tuned Llama language models — ranging from 7B to 70B parameters.\n\nThis repository is intended as a minimal example to load Llama 2 models and run inference. For more detailed examples leveraging Hugging Face, see llama-cookbook.... ## Updates post-launch\n\nSee UPDATES.md. Also for a running list of frequently asked questions, see here.... ## Download\n\nIn order to download the model weights and tokenizer, please visit the Meta website and accept our License.\n\nOnce your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download.\n\nPre-requisites: Make sure you have... ### Access to Hugging Face\n\nWe are also providing downloads on Hugging Face. You can request access to the models by acknowledging the license and filling the form in the model card of a repo. After doing so, you should get access to all the Llama models of a version (Code Llama, Llama 2, or Llama Guard) within 1 hour.... ## Quick Start\n\nYou can follow the steps below to quickly get up and running with Llama 2 models. These steps will let you run quick inference locally. For more examples, see the Llama 2 cookbook repository.... 1. In a conda env with PyTorch / CUDA available clone and download this repository.\n2. In the top-level directory run:\n\n   ```\n   pip install -e .\n   ```\n3. Visit the Meta website and register to download the model/s.\n4. Once registered, you will get an email with a URL to download the models. You will need this URL when you run the download.sh script.... ```\nBOS\n```\nand\n```\nEOS\n```\ntokens, and the whitespaces and breaklines in between (we recommend calling\n```\nstrip()\n```\non inputs to avoid double-spaces).\n\nYou can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-cookbook repo for an example of how to add a safety checker to the inputs and outputs of your inference code.... ## Issues\n\nPlease report any software “bug”, or other problems with the models through one of the following means:\n\n- Reporting issues with the model: github.com/facebookresearch/llama\n- Reporting risky content generated by the model: developers.facebook.com/llama_output_feedback\n- Reporting bugs and security concerns: facebook.com/whitehat/info... ## Model Card\n\nSee MODEL_CARD.md.\n\n## License\n\nOur model and weights are licensed for both researchers and commercial entities, upholding the principles of openness. Our mission is to empower individuals, and industry through this opportunity, while fostering an environment of discovery and ethical AI advancements.\n\nSee the LICENSE file, as well as our accompanying Acceptable Use Policy... ## References\n\n1. Research Paper\n2. Llama 2 technical overview\n3. Open Innovation AI Research Community\n\nFor common questions, the FAQ can be found here which will be kept up to date over time as new questions arise.\n\n## Original Llama\n\nThe repo for the original llama release is in the\n```\nllama_v1\n```\nbranch.... ## About\n\nInference code for Llama models\n\n### Resources\n\nReadme\n\n### License\n\nView license\n\n### Code of conduct\n\nCode of conduct\n\n### Contributing\n\nContributing\n\nActivity\n\nCustom properties\n\n### Stars\n\n**58.9k** stars\n\n### Watchers\n\n**538** watching\n\n### Forks\n\n**9.8k** forks"
          }
        ],
        "subsectionChecks": [
          {
            "name": "Risk Mitigations",
            "score": 1,
            "explanation": "We built Llama 4 with the best practices outlined in our Developer Use Guide: AI Protections. This includes integrating mitigations at each layer of model development."
          }
        ]
      }
    ]
  }
]